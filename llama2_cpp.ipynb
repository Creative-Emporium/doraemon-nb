{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzk/lEODRk47czGja6gKxK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/llama2_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "6DbVIOZUqWCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPlA4C6wqR3x",
        "outputId": "73b4c00b-d75d-4018-9641-b9d89efc2e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 14558, done.\u001b[K\n",
            "remote: Counting objects: 100% (14558/14558), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4378/4378), done.\u001b[K\n",
            "remote: Total 14558 (delta 10200), reused 14343 (delta 10078), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (14558/14558), 16.57 MiB | 25.17 MiB/s, done.\n",
            "Resolving deltas: 100% (10200/10200), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv7hh9s0uB8S",
        "outputId": "54f04fc2-4f8c-4875-b807-5944e79bbecf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ_RqEzFvI-Q",
        "outputId": "6ef0bbea-c4da-4d2f-8d29-575a73f91cec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  8\n",
            "  On-line CPU(s) list:   0-7\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:          6\n",
            "    Model:               79\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  4\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4399.99\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsa\n",
            "                         veopt arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   128 KiB (4 instances)\n",
            "  L1i:                   128 KiB (4 instances)\n",
            "  L2:                    1 MiB (4 instances)\n",
            "  L3:                    55 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0-7\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq6EU0b8uJid",
        "outputId": "2bb5e114-94fd-4b7e-8fb0-bc37428d696f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "\u001b[01m\u001b[Kggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_compute_forward_scale_f32\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kggml.c:10338:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstrict-aliasing\u0007-Wstrict-aliasing\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "10338 |     const float v = *\u001b[01;35m\u001b[K(float *) dst->op_params\u001b[m\u001b[K;\n",
            "      |                      \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_compute_backward\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kggml.c:15155:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstrict-aliasing\u0007-Wstrict-aliasing\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "15155 |                     const float s = ((float *) tensor->op_params)\u001b[01;35m\u001b[K[\u001b[m\u001b[K0];\n",
            "      |                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server    -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Python dependencies\n",
        "!python3 -m pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w1osdb0zJ_e",
        "outputId": "e2542ec7-95b5-4389-9a29-f9a1ae15f44a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.35.2)\n",
            "Collecting gguf>=0.1.0 (from -r requirements.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf>=4.21.0 (from -r requirements.txt (line 5))\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.34.0->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.11.17)\n",
            "Installing collected packages: sentencepiece, protobuf, numpy, gguf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 protobuf-4.25.1 sentencepiece-0.1.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub>=0.17.1\n",
        "\n"
      ],
      "metadata": {
        "id": "XvjBfHYE2FqQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3turm7Gs2JQr",
        "outputId": "06ae99a0-07d5-4b11-9c51-877f933229d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF#provided-files\n",
        "!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir ./models/ --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU18A-8D2ieo",
        "outputId": "16b2d356-5c3b-43ee-ed32-f5e8e8754298"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpkj52npx1\n",
            "llama-2-7b-chat.Q4_K_M.gguf: 100% 4.08G/4.08G [00:20<00:00, 200MB/s]\n",
            "./models/llama-2-7b-chat.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF --include='*Q4_K*gguf' --local-dir ./models/ --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rd-uhVr73Fd",
        "outputId": "a5f2610f-4a22-408e-a710-a52845b22b76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]downloading https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp7p3fjhy4\n",
            "downloading https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q4_K_S.gguf to /root/.cache/huggingface/hub/tmpysepjj44\n",
            "\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   0% 0.00/4.08G [00:00<?, ?B/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   0% 10.5M/4.08G [00:00<00:44, 91.9MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   1% 31.5M/4.08G [00:00<00:27, 145MB/s] \u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   1% 52.4M/4.08G [00:00<00:23, 171MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   2% 73.4M/4.08G [00:00<00:21, 185MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   0% 10.5M/3.86G [00:00<00:49, 77.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   2% 94.4M/4.08G [00:00<00:20, 194MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   1% 21.0M/3.86G [00:00<00:45, 85.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   3% 115M/4.08G [00:00<00:20, 197MB/s] \u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   4% 147M/4.08G [00:00<00:19, 202MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   1% 31.5M/3.86G [00:00<01:03, 59.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   4% 168M/4.08G [00:00<00:19, 203MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   1% 41.9M/3.86G [00:00<01:01, 61.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   5% 199M/4.08G [00:01<00:18, 207MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   5% 220M/4.08G [00:01<00:18, 208MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   1% 52.4M/3.86G [00:00<01:19, 48.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   6% 252M/4.08G [00:01<00:18, 208MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   2% 62.9M/3.86G [00:01<01:08, 55.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   7% 283M/4.08G [00:01<00:18, 209MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   2% 73.4M/3.86G [00:01<01:03, 59.6MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   8% 315M/4.08G [00:01<00:17, 209MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   8% 336M/4.08G [00:01<00:17, 209MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   2% 83.9M/3.86G [00:01<01:05, 57.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   9% 357M/4.08G [00:01<00:17, 209MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:   9% 377M/4.08G [00:01<00:17, 208MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  10% 409M/4.08G [00:02<00:17, 210MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   3% 105M/3.86G [00:01<01:05, 57.1MB/s] \u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  11% 430M/4.08G [00:02<00:17, 210MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  11% 451M/4.08G [00:02<00:17, 208MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  12% 472M/4.08G [00:02<00:17, 204MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  12% 493M/4.08G [00:02<00:17, 204MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   3% 115M/3.86G [00:02<01:27, 42.7MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  13% 514M/4.08G [00:02<00:17, 202MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  13% 535M/4.08G [00:02<00:17, 204MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  14% 556M/4.08G [00:02<00:17, 204MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  14% 577M/4.08G [00:02<00:17, 196MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   4% 136M/3.86G [00:02<01:16, 48.7MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  15% 598M/4.08G [00:02<00:18, 191MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   4% 147M/3.86G [00:02<01:16, 48.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  15% 619M/4.08G [00:03<00:18, 188MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  16% 640M/4.08G [00:03<00:18, 188MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  16% 661M/4.08G [00:03<00:18, 185MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   4% 168M/3.86G [00:03<01:08, 53.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  17% 682M/4.08G [00:03<00:17, 190MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  17% 703M/4.08G [00:03<00:17, 191MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   5% 178M/3.86G [00:03<01:08, 54.0MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  18% 724M/4.08G [00:03<00:17, 192MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  18% 744M/4.08G [00:03<00:17, 194MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  19% 765M/4.08G [00:03<00:16, 197MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   5% 199M/3.86G [00:03<01:03, 57.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  19% 786M/4.08G [00:03<00:16, 198MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  20% 807M/4.08G [00:04<00:16, 201MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  20% 828M/4.08G [00:04<00:16, 202MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   5% 210M/3.86G [00:03<01:08, 53.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  21% 849M/4.08G [00:04<00:16, 202MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  21% 870M/4.08G [00:04<00:15, 202MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  22% 891M/4.08G [00:04<00:17, 185MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   6% 220M/3.86G [00:04<01:23, 43.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  22% 912M/4.08G [00:04<00:17, 185MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   6% 231M/3.86G [00:04<01:14, 48.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  23% 933M/4.08G [00:04<00:16, 191MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   6% 241M/3.86G [00:04<01:15, 48.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  24% 965M/4.08G [00:05<00:21, 148MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  24% 986M/4.08G [00:05<00:19, 160MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  25% 1.01G/4.08G [00:05<00:17, 171MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  25% 1.04G/4.08G [00:05<00:16, 184MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   7% 262M/3.86G [00:05<01:16, 47.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  26% 1.06G/4.08G [00:05<00:15, 189MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  26% 1.08G/4.08G [00:05<00:15, 194MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   7% 273M/3.86G [00:05<01:12, 49.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  27% 1.10G/4.08G [00:05<00:15, 194MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  27% 1.12G/4.08G [00:05<00:15, 195MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  28% 1.14G/4.08G [00:05<00:15, 195MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  29% 1.16G/4.08G [00:06<00:14, 197MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  29% 1.18G/4.08G [00:06<00:14, 197MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   8% 294M/3.86G [00:05<01:20, 44.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  30% 1.21G/4.08G [00:06<00:14, 198MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  30% 1.23G/4.08G [00:06<00:14, 198MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  31% 1.25G/4.08G [00:06<00:14, 200MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  31% 1.27G/4.08G [00:06<00:13, 201MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   8% 304M/3.86G [00:06<01:33, 37.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  32% 1.29G/4.08G [00:06<00:13, 201MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  32% 1.31G/4.08G [00:06<00:13, 200MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  33% 1.33G/4.08G [00:06<00:13, 200MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   8% 325M/3.86G [00:06<01:21, 43.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  33% 1.35G/4.08G [00:06<00:13, 200MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  34% 1.37G/4.08G [00:07<00:13, 200MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  34% 1.39G/4.08G [00:07<00:13, 200MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   9% 346M/3.86G [00:06<01:09, 50.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  35% 1.42G/4.08G [00:07<00:13, 200MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  35% 1.44G/4.08G [00:07<00:13, 200MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  36% 1.46G/4.08G [00:07<00:13, 192MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:   9% 357M/3.86G [00:07<01:15, 46.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  36% 1.48G/4.08G [00:07<00:13, 190MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  37% 1.50G/4.08G [00:07<00:13, 188MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  10% 377M/3.86G [00:07<01:01, 56.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  37% 1.52G/4.08G [00:07<00:13, 186MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  38% 1.54G/4.08G [00:07<00:14, 177MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  38% 1.56G/4.08G [00:08<00:14, 178MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  39% 1.58G/4.08G [00:08<00:13, 179MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  10% 388M/3.86G [00:07<01:22, 42.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  39% 1.60G/4.08G [00:08<00:13, 179MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  10% 398M/3.86G [00:08<01:11, 48.7MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  40% 1.63G/4.08G [00:08<00:13, 179MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  40% 1.65G/4.08G [00:08<00:13, 179MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  41% 1.67G/4.08G [00:08<00:13, 179MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  11% 409M/3.86G [00:08<01:24, 41.0MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  41% 1.69G/4.08G [00:08<00:13, 177MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  42% 1.71G/4.08G [00:08<00:13, 176MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  11% 419M/3.86G [00:08<01:24, 40.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  42% 1.73G/4.08G [00:09<00:13, 179MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  43% 1.75G/4.08G [00:09<00:13, 178MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  43% 1.77G/4.08G [00:09<00:12, 182MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  11% 440M/3.86G [00:08<01:08, 49.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  44% 1.79G/4.08G [00:09<00:12, 184MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  44% 1.81G/4.08G [00:09<00:12, 186MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  45% 1.84G/4.08G [00:09<00:11, 189MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  45% 1.86G/4.08G [00:09<00:11, 192MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  12% 451M/3.86G [00:09<01:23, 40.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  46% 1.88G/4.08G [00:09<00:12, 179MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  12% 461M/3.86G [00:09<01:14, 45.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  47% 1.90G/4.08G [00:09<00:11, 183MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  12% 472M/3.86G [00:09<01:10, 47.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  47% 1.92G/4.08G [00:10<00:11, 187MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  48% 1.94G/4.08G [00:10<00:11, 189MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  13% 482M/3.86G [00:09<01:08, 49.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  48% 1.96G/4.08G [00:10<00:11, 189MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  49% 1.98G/4.08G [00:10<00:11, 190MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  49% 2.00G/4.08G [00:10<00:10, 192MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  50% 2.02G/4.08G [00:10<00:10, 194MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  13% 503M/3.86G [00:10<01:03, 52.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  50% 2.04G/4.08G [00:10<00:10, 195MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  51% 2.07G/4.08G [00:10<00:10, 196MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  51% 2.09G/4.08G [00:10<00:10, 198MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  52% 2.11G/4.08G [00:10<00:09, 198MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  13% 514M/3.86G [00:10<01:20, 41.7MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  52% 2.13G/4.08G [00:11<00:09, 196MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  53% 2.15G/4.08G [00:11<00:09, 196MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  53% 2.17G/4.08G [00:11<00:09, 193MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  54% 2.19G/4.08G [00:11<00:09, 189MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  14% 535M/3.86G [00:11<01:18, 42.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  54% 2.21G/4.08G [00:11<00:10, 186MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  14% 545M/3.86G [00:11<01:11, 46.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  55% 2.23G/4.08G [00:11<00:09, 186MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  15% 566M/3.86G [00:11<00:50, 65.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  55% 2.25G/4.08G [00:11<00:09, 183MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  15% 587M/3.86G [00:11<00:38, 84.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  56% 2.28G/4.08G [00:11<00:09, 183MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  56% 2.30G/4.08G [00:12<00:09, 182MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  57% 2.32G/4.08G [00:12<00:09, 179MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  57% 2.34G/4.08G [00:12<00:09, 177MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  58% 2.36G/4.08G [00:12<00:09, 178MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  16% 608M/3.86G [00:12<00:54, 59.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  58% 2.38G/4.08G [00:12<00:09, 177MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  59% 2.40G/4.08G [00:12<00:09, 177MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  59% 2.42G/4.08G [00:12<00:09, 173MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  16% 629M/3.86G [00:12<00:55, 58.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  60% 2.44G/4.08G [00:12<00:09, 172MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  60% 2.46G/4.08G [00:12<00:09, 173MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  17% 640M/3.86G [00:12<00:55, 57.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  61% 2.49G/4.08G [00:13<00:09, 172MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  17% 650M/3.86G [00:12<00:59, 54.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  61% 2.51G/4.08G [00:13<00:09, 170MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  17% 661M/3.86G [00:13<00:54, 59.0MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  62% 2.53G/4.08G [00:13<00:09, 167MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  62% 2.55G/4.08G [00:13<00:09, 167MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  63% 2.57G/4.08G [00:13<00:08, 169MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  63% 2.59G/4.08G [00:13<00:08, 170MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  18% 682M/3.86G [00:13<00:55, 57.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  64% 2.61G/4.08G [00:13<00:08, 172MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  64% 2.63G/4.08G [00:13<00:08, 177MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  18% 692M/3.86G [00:13<01:00, 51.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  65% 2.65G/4.08G [00:14<00:07, 179MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  18% 703M/3.86G [00:13<00:53, 58.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  66% 2.67G/4.08G [00:14<00:07, 185MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  18% 713M/3.86G [00:13<00:48, 64.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  66% 2.69G/4.08G [00:14<00:07, 189MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  67% 2.72G/4.08G [00:14<00:07, 191MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  19% 724M/3.86G [00:14<00:52, 59.6MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  67% 2.74G/4.08G [00:14<00:06, 193MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  68% 2.76G/4.08G [00:14<00:06, 194MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  68% 2.78G/4.08G [00:14<00:06, 192MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  19% 744M/3.86G [00:14<00:49, 62.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  69% 2.80G/4.08G [00:14<00:06, 192MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  69% 2.82G/4.08G [00:14<00:06, 195MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  70% 2.84G/4.08G [00:15<00:06, 195MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  20% 755M/3.86G [00:14<01:02, 50.0MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  70% 2.86G/4.08G [00:15<00:06, 193MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  71% 2.88G/4.08G [00:15<00:06, 192MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  71% 2.90G/4.08G [00:15<00:06, 194MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  20% 776M/3.86G [00:15<00:58, 52.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  72% 2.93G/4.08G [00:15<00:05, 194MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  72% 2.95G/4.08G [00:15<00:05, 194MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  73% 2.97G/4.08G [00:15<00:05, 196MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  20% 786M/3.86G [00:15<01:06, 46.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  73% 2.99G/4.08G [00:15<00:05, 197MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  74% 3.01G/4.08G [00:15<00:05, 197MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  21% 797M/3.86G [00:15<01:04, 47.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  74% 3.03G/4.08G [00:16<00:05, 189MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  75% 3.05G/4.08G [00:16<00:05, 188MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  21% 807M/3.86G [00:15<00:59, 51.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  75% 3.07G/4.08G [00:16<00:05, 190MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  76% 3.09G/4.08G [00:16<00:05, 193MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  21% 818M/3.86G [00:16<01:03, 47.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  76% 3.11G/4.08G [00:16<00:04, 195MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  77% 3.14G/4.08G [00:16<00:04, 197MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  77% 3.16G/4.08G [00:16<00:04, 195MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  78% 3.18G/4.08G [00:16<00:04, 189MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  78% 3.20G/4.08G [00:16<00:04, 188MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  22% 839M/3.86G [00:16<01:07, 44.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  79% 3.22G/4.08G [00:17<00:04, 187MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  22% 849M/3.86G [00:16<01:01, 48.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  79% 3.24G/4.08G [00:17<00:04, 183MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  80% 3.26G/4.08G [00:17<00:04, 180MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  23% 870M/3.86G [00:17<00:50, 58.9MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  80% 3.28G/4.08G [00:17<00:04, 176MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  81% 3.30G/4.08G [00:17<00:04, 177MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  23% 881M/3.86G [00:17<00:54, 54.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  81% 3.32G/4.08G [00:17<00:04, 176MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  82% 3.34G/4.08G [00:17<00:04, 156MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  23% 902M/3.86G [00:17<00:48, 60.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  82% 3.37G/4.08G [00:17<00:04, 147MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  83% 3.39G/4.08G [00:18<00:05, 138MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  24% 912M/3.86G [00:17<00:55, 53.5MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  84% 3.41G/4.08G [00:18<00:05, 133MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  84% 3.43G/4.08G [00:18<00:04, 134MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  85% 3.45G/4.08G [00:18<00:04, 133MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  24% 933M/3.86G [00:18<00:59, 48.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  24% 944M/3.86G [00:18<00:54, 53.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  85% 3.47G/4.08G [00:18<00:04, 131MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  25% 954M/3.86G [00:18<00:50, 57.6MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  86% 3.49G/4.08G [00:18<00:04, 129MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  86% 3.51G/4.08G [00:19<00:04, 132MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  25% 965M/3.86G [00:18<00:55, 52.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  87% 3.53G/4.08G [00:19<00:04, 130MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  26% 986M/3.86G [00:19<00:45, 63.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  87% 3.55G/4.08G [00:19<00:04, 129MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  88% 3.58G/4.08G [00:19<00:03, 129MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  26% 996M/3.86G [00:19<00:51, 55.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  88% 3.60G/4.08G [00:19<00:03, 127MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  89% 3.62G/4.08G [00:19<00:03, 129MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  26% 1.02G/3.86G [00:19<00:51, 54.6MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  89% 3.64G/4.08G [00:20<00:03, 131MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  90% 3.66G/4.08G [00:20<00:03, 129MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  27% 1.03G/3.86G [00:20<01:01, 45.7MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  90% 3.68G/4.08G [00:20<00:03, 129MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  27% 1.04G/3.86G [00:20<00:53, 52.3MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  91% 3.70G/4.08G [00:20<00:02, 129MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  91% 3.72G/4.08G [00:20<00:02, 131MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  27% 1.05G/3.86G [00:20<00:58, 48.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  92% 3.74G/4.08G [00:20<00:02, 130MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  27% 1.06G/3.86G [00:20<01:01, 45.6MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  92% 3.76G/4.08G [00:21<00:02, 130MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  28% 1.07G/3.86G [00:20<00:51, 54.0MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  93% 3.79G/4.08G [00:21<00:02, 128MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  28% 1.08G/3.86G [00:20<00:45, 61.6MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  93% 3.81G/4.08G [00:21<00:02, 126MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  28% 1.09G/3.86G [00:21<00:50, 55.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  94% 3.83G/4.08G [00:21<00:01, 127MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  94% 3.85G/4.08G [00:21<00:01, 131MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  95% 3.87G/4.08G [00:21<00:01, 133MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  29% 1.11G/3.86G [00:21<00:49, 55.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  95% 3.89G/4.08G [00:21<00:01, 133MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  29% 1.12G/3.86G [00:21<00:47, 58.1MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  96% 3.91G/4.08G [00:22<00:01, 133MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  96% 3.93G/4.08G [00:22<00:01, 129MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  30% 1.14G/3.86G [00:22<00:47, 57.6MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  97% 3.95G/4.08G [00:22<00:01, 124MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  30% 1.15G/3.86G [00:22<00:48, 55.2MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  97% 3.97G/4.08G [00:22<00:00, 123MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  30% 1.17G/3.86G [00:22<00:39, 67.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  98% 4.00G/4.08G [00:22<00:00, 126MB/s]\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  98% 4.02G/4.08G [00:23<00:00, 125MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  31% 1.18G/3.86G [00:22<00:43, 61.4MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  99% 4.04G/4.08G [00:23<00:00, 121MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  31% 1.21G/3.86G [00:22<00:36, 72.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  32% 1.22G/3.86G [00:23<00:34, 75.8MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf:  99% 4.06G/4.08G [00:23<00:00, 117MB/s]\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  32% 1.23G/3.86G [00:23<00:34, 75.7MB/s]\u001b[A\u001b[A\n",
            "llama-2-7b-chat.Q4_K_M.gguf: 100% 4.08G/4.08G [00:23<00:00, 173MB/s]\n",
            "\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  32% 1.24G/3.86G [00:23<00:38, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  32% 1.25G/3.86G [00:23<00:52, 49.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  33% 1.26G/3.86G [00:24<00:58, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  33% 1.27G/3.86G [00:24<00:50, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  33% 1.29G/3.86G [00:24<00:44, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  34% 1.30G/3.86G [00:24<00:49, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  34% 1.31G/3.86G [00:24<00:42, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  34% 1.32G/3.86G [00:25<00:44, 56.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  35% 1.33G/3.86G [00:25<00:44, 57.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  35% 1.35G/3.86G [00:25<00:38, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching 2 files:  50% 1/2 [00:26<00:26, 26.85s/it]\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  36% 1.38G/3.86G [00:26<00:43, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  36% 1.39G/3.86G [00:26<00:46, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  36% 1.41G/3.86G [00:26<00:47, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  37% 1.42G/3.86G [00:27<00:58, 41.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  37% 1.43G/3.86G [00:27<01:02, 38.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  38% 1.45G/3.86G [00:27<00:53, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  38% 1.46G/3.86G [00:27<00:48, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  38% 1.48G/3.86G [00:28<00:38, 61.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  39% 1.49G/3.86G [00:28<00:50, 46.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  39% 1.51G/3.86G [00:28<00:45, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  39% 1.52G/3.86G [00:29<00:46, 50.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  40% 1.54G/3.86G [00:29<00:45, 50.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  40% 1.55G/3.86G [00:29<00:43, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  41% 1.56G/3.86G [00:29<00:42, 53.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  41% 1.57G/3.86G [00:29<00:40, 56.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  41% 1.59G/3.86G [00:30<00:39, 56.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  42% 1.60G/3.86G [00:30<00:36, 61.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  42% 1.63G/3.86G [00:30<00:39, 57.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  42% 1.64G/3.86G [00:31<00:45, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  43% 1.66G/3.86G [00:31<00:43, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  43% 1.67G/3.86G [00:31<00:48, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  44% 1.69G/3.86G [00:32<00:41, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  44% 1.70G/3.86G [00:32<00:37, 57.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  45% 1.72G/3.86G [00:32<00:38, 55.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  45% 1.73G/3.86G [00:32<00:42, 50.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  45% 1.75G/3.86G [00:33<00:39, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  46% 1.76G/3.86G [00:33<00:44, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  46% 1.78G/3.86G [00:34<00:44, 46.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  46% 1.79G/3.86G [00:34<00:40, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  47% 1.81G/3.86G [00:34<00:34, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  47% 1.82G/3.86G [00:34<00:36, 55.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  48% 1.85G/3.86G [00:35<00:32, 62.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  48% 1.86G/3.86G [00:35<00:31, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  48% 1.87G/3.86G [00:35<00:31, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  49% 1.88G/3.86G [00:35<00:35, 55.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  49% 1.89G/3.86G [00:35<00:41, 47.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  49% 1.90G/3.86G [00:36<00:39, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  49% 1.91G/3.86G [00:36<00:44, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  50% 1.93G/3.86G [00:36<00:37, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  50% 1.94G/3.86G [00:36<00:38, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  51% 1.95G/3.86G [00:37<00:35, 54.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  51% 1.96G/3.86G [00:37<00:38, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  51% 1.97G/3.86G [00:37<00:37, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  52% 1.99G/3.86G [00:37<00:31, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  52% 2.00G/3.86G [00:38<00:45, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  52% 2.02G/3.86G [00:38<00:43, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  53% 2.03G/3.86G [00:39<00:48, 37.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  53% 2.04G/3.86G [00:39<00:42, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  53% 2.06G/3.86G [00:39<00:41, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  54% 2.07G/3.86G [00:39<00:42, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  54% 2.08G/3.86G [00:39<00:35, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  54% 2.09G/3.86G [00:40<00:36, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  54% 2.10G/3.86G [00:40<00:38, 46.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  55% 2.12G/3.86G [00:40<00:30, 57.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  55% 2.13G/3.86G [00:41<00:36, 47.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  56% 2.15G/3.86G [00:41<00:29, 57.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  56% 2.16G/3.86G [00:41<00:31, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  57% 2.18G/3.86G [00:41<00:25, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  57% 2.19G/3.86G [00:41<00:28, 58.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  57% 2.20G/3.86G [00:42<00:26, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  57% 2.21G/3.86G [00:42<00:32, 50.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  58% 2.23G/3.86G [00:43<00:42, 38.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  58% 2.24G/3.86G [00:43<00:45, 35.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  59% 2.26G/3.86G [00:43<00:36, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  59% 2.28G/3.86G [00:44<00:40, 39.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  60% 2.30G/3.86G [00:44<00:31, 49.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  60% 2.31G/3.86G [00:44<00:35, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  60% 2.33G/3.86G [00:45<00:35, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  61% 2.34G/3.86G [00:45<00:37, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  61% 2.35G/3.86G [00:45<00:32, 45.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  61% 2.36G/3.86G [00:45<00:31, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  61% 2.37G/3.86G [00:46<00:28, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  62% 2.38G/3.86G [00:46<00:25, 59.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  62% 2.39G/3.86G [00:46<00:26, 56.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  62% 2.40G/3.86G [00:46<00:36, 39.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  63% 2.41G/3.86G [00:46<00:30, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  63% 2.43G/3.86G [00:47<00:26, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  64% 2.45G/3.86G [00:47<00:22, 61.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  64% 2.46G/3.86G [00:47<00:23, 58.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  64% 2.49G/3.86G [00:48<00:24, 57.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  65% 2.50G/3.86G [00:48<00:25, 53.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  65% 2.51G/3.86G [00:48<00:23, 58.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  65% 2.52G/3.86G [00:48<00:26, 51.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  66% 2.54G/3.86G [00:49<00:22, 57.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  66% 2.55G/3.86G [00:49<00:26, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  66% 2.56G/3.86G [00:49<00:30, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  67% 2.57G/3.86G [00:49<00:28, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  67% 2.58G/3.86G [00:50<00:26, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  67% 2.60G/3.86G [00:50<00:23, 54.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  68% 2.61G/3.86G [00:50<00:23, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  68% 2.62G/3.86G [00:50<00:23, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  68% 2.63G/3.86G [00:51<00:23, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  69% 2.64G/3.86G [00:51<00:30, 39.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  69% 2.65G/3.86G [00:51<00:27, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  69% 2.66G/3.86G [00:52<00:30, 38.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  69% 2.67G/3.86G [00:52<00:29, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  70% 2.68G/3.86G [00:52<00:23, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  70% 2.69G/3.86G [00:52<00:24, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  70% 2.71G/3.86G [00:52<00:24, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  71% 2.73G/3.86G [00:53<00:19, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  71% 2.74G/3.86G [00:53<00:21, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  72% 2.76G/3.86G [00:53<00:16, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  72% 2.77G/3.86G [00:53<00:17, 60.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  72% 2.79G/3.86G [00:54<00:15, 67.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  73% 2.80G/3.86G [00:54<00:15, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  73% 2.81G/3.86G [00:54<00:16, 62.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  73% 2.82G/3.86G [00:54<00:19, 54.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  74% 2.84G/3.86G [00:54<00:16, 61.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  74% 2.85G/3.86G [00:55<00:23, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  74% 2.87G/3.86G [00:55<00:17, 55.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  75% 2.88G/3.86G [00:55<00:19, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  75% 2.90G/3.86G [00:56<00:16, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  76% 2.92G/3.86G [00:56<00:20, 46.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  76% 2.94G/3.86G [00:56<00:17, 54.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  76% 2.95G/3.86G [00:57<00:17, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  77% 2.97G/3.86G [00:57<00:14, 60.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  77% 2.98G/3.86G [00:57<00:16, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  77% 2.99G/3.86G [00:57<00:15, 56.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  78% 3.00G/3.86G [00:58<00:16, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  78% 3.01G/3.86G [00:58<00:17, 47.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  79% 3.03G/3.86G [00:58<00:15, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  79% 3.04G/3.86G [00:59<00:22, 36.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  79% 3.06G/3.86G [00:59<00:18, 43.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  80% 3.07G/3.86G [01:00<00:21, 36.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  80% 3.09G/3.86G [01:00<00:16, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  80% 3.10G/3.86G [01:00<00:17, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  81% 3.11G/3.86G [01:00<00:16, 45.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  81% 3.12G/3.86G [01:01<00:16, 44.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  82% 3.15G/3.86G [01:01<00:12, 57.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  82% 3.16G/3.86G [01:01<00:11, 59.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  82% 3.18G/3.86G [01:01<00:09, 71.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  83% 3.19G/3.86G [01:01<00:11, 57.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  83% 3.20G/3.86G [01:02<00:10, 61.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  83% 3.21G/3.86G [01:02<00:10, 62.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  83% 3.22G/3.86G [01:02<00:13, 46.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  84% 3.24G/3.86G [01:02<00:12, 50.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  84% 3.25G/3.86G [01:03<00:14, 41.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  85% 3.26G/3.86G [01:03<00:12, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  85% 3.27G/3.86G [01:03<00:14, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  85% 3.28G/3.86G [01:04<00:15, 37.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  86% 3.30G/3.86G [01:04<00:11, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  86% 3.31G/3.86G [01:04<00:11, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  86% 3.32G/3.86G [01:04<00:09, 56.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  86% 3.33G/3.86G [01:05<00:11, 45.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  87% 3.34G/3.86G [01:05<00:11, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  87% 3.37G/3.86G [01:05<00:09, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  88% 3.38G/3.86G [01:05<00:10, 46.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  88% 3.40G/3.86G [01:06<00:09, 47.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  88% 3.41G/3.86G [01:06<00:09, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  89% 3.42G/3.86G [01:06<00:08, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  89% 3.43G/3.86G [01:07<00:09, 46.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  89% 3.45G/3.86G [01:07<00:09, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  90% 3.46G/3.86G [01:07<00:09, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  90% 3.48G/3.86G [01:08<00:08, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  91% 3.49G/3.86G [01:08<00:08, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  91% 3.50G/3.86G [01:08<00:08, 40.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  91% 3.51G/3.86G [01:09<00:07, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  91% 3.52G/3.86G [01:09<00:08, 38.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  92% 3.54G/3.86G [01:09<00:06, 51.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  92% 3.55G/3.86G [01:09<00:06, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  93% 3.58G/3.86G [01:10<00:04, 62.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  93% 3.59G/3.86G [01:10<00:05, 53.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  94% 3.61G/3.86G [01:10<00:04, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  94% 3.62G/3.86G [01:11<00:05, 47.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  94% 3.64G/3.86G [01:11<00:04, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  95% 3.65G/3.86G [01:11<00:04, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  95% 3.67G/3.86G [01:12<00:03, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  95% 3.68G/3.86G [01:12<00:03, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  96% 3.70G/3.86G [01:12<00:03, 51.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  97% 3.72G/3.86G [01:12<00:02, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  97% 3.73G/3.86G [01:13<00:02, 54.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  97% 3.75G/3.86G [01:13<00:01, 67.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  98% 3.76G/3.86G [01:13<00:01, 57.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  98% 3.77G/3.86G [01:13<00:01, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  98% 3.79G/3.86G [01:14<00:01, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  98% 3.80G/3.86G [01:14<00:01, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  99% 3.82G/3.86G [01:15<00:00, 43.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf:  99% 3.83G/3.86G [01:15<00:00, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf: 100% 3.85G/3.86G [01:15<00:00, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-2-7b-chat.Q4_K_S.gguf: 100% 3.86G/3.86G [01:15<00:00, 50.8MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:16<00:00, 38.37s/it]\n",
            "/content/llama.cpp/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the original LLaMA model weights and place them in ./models\n",
        "!ls -hlg ./models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0x0qmPPy9-N",
        "outputId": "034f8fe9-982a-493b-b316-bbf3dae1fbbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 7.5G\n",
            "-rw-r--r-- 1 root 4.7M Dec 22 08:50 ggml-vocab-aquila.gguf\n",
            "-rw-r--r-- 1 root 1.3M Dec 22 08:50 ggml-vocab-baichuan.gguf\n",
            "-rw-r--r-- 1 root 2.5M Dec 22 08:50 ggml-vocab-falcon.gguf\n",
            "-rw-r--r-- 1 root 1.7M Dec 22 08:50 ggml-vocab-gpt-neox.gguf\n",
            "-rw-r--r-- 1 root 707K Dec 22 08:50 ggml-vocab-llama.gguf\n",
            "-rw-r--r-- 1 root 1.7M Dec 22 08:50 ggml-vocab-mpt.gguf\n",
            "-rw-r--r-- 1 root 1.7M Dec 22 08:50 ggml-vocab-refact.gguf\n",
            "-rw-r--r-- 1 root 1.7M Dec 22 08:50 ggml-vocab-stablelm-3b-4e1t.gguf\n",
            "-rw-r--r-- 1 root 1.7M Dec 22 08:50 ggml-vocab-starcoder.gguf\n",
            "-rw-r--r-- 1 root 3.9G Dec 22 09:52 llama-2-7b-chat.Q4_K_M.gguf\n",
            "-rw-r--r-- 1 root 3.6G Dec 22 09:53 llama-2-7b-chat.Q4_K_S.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTVm4-L85FlK",
        "outputId": "5d9d012d-5991-4b13-b05c-038cdfe54e4b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "usage: ./main [options]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "      --version         show version and build info\n",
            "  -i, --interactive     run in interactive mode\n",
            "  --interactive-first   run in interactive mode and wait for input right away\n",
            "  -ins, --instruct      run in instruction mode (use with Alpaca models)\n",
            "  -cml, --chatml        run in chatml mode (use with ChatML-compatible models)\n",
            "  --multiline-input     allows you to write or paste multiple lines without ending each in '\\'\n",
            "  -r PROMPT, --reverse-prompt PROMPT\n",
            "                        halt generation at PROMPT, return control in interactive mode\n",
            "                        (can be specified more than once for multiple prompts).\n",
            "  --color               colorise output to distinguish prompt and user input from generations\n",
            "  -s SEED, --seed SEED  RNG seed (default: -1, use random seed for < 0)\n",
            "  -t N, --threads N     number of threads to use during generation (default: 4)\n",
            "  -tb N, --threads-batch N\n",
            "                        number of threads to use during batch and prompt processing (default: same as --threads)\n",
            "  -p PROMPT, --prompt PROMPT\n",
            "                        prompt to start generation with (default: empty)\n",
            "  -e, --escape          process prompt escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n",
            "  --prompt-cache FNAME  file to cache prompt state for faster startup (default: none)\n",
            "  --prompt-cache-all    if specified, saves user input and generations to cache as well.\n",
            "                        not supported with --interactive or other interactive options\n",
            "  --prompt-cache-ro     if specified, uses the prompt cache but does not update it.\n",
            "  --random-prompt       start with a randomized prompt.\n",
            "  --in-prefix-bos       prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "  --in-prefix STRING    string to prefix user inputs with (default: empty)\n",
            "  --in-suffix STRING    string to suffix after user inputs with (default: empty)\n",
            "  -f FNAME, --file FNAME\n",
            "                        prompt file to start generation.\n",
            "  -n N, --n-predict N   number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)\n",
            "  -c N, --ctx-size N    size of the prompt context (default: 512, 0 = loaded from model)\n",
            "  -b N, --batch-size N  batch size for prompt processing (default: 512)\n",
            "  --samplers            samplers that will be used for generation in the order, separated by ';', for example: \"top_k;tfs;typical;top_p;min_p;temp\"\n",
            "  --sampling-seq        simplified sequence for samplers that will be used (default: kfypmt)\n",
            "  --top-k N             top-k sampling (default: 40, 0 = disabled)\n",
            "  --top-p N             top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "  --min-p N             min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "  --tfs N               tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
            "  --typical N           locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "  --repeat-last-n N     last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)\n",
            "  --repeat-penalty N    penalize repeat sequence of tokens (default: 1.1, 1.0 = disabled)\n",
            "  --presence-penalty N  repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "  --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "  --mirostat N          use Mirostat sampling.\n",
            "                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.\n",
            "                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "  --mirostat-lr N       Mirostat learning rate, parameter eta (default: 0.1)\n",
            "  --mirostat-ent N      Mirostat target entropy, parameter tau (default: 5.0)\n",
            "  -l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS\n",
            "                        modifies the likelihood of token appearing in the completion,\n",
            "                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "  --grammar GRAMMAR     BNF-like grammar to constrain generations (see samples in grammars/ dir)\n",
            "  --grammar-file FNAME  file to read grammar from\n",
            "  --cfg-negative-prompt PROMPT\n",
            "                        negative prompt to use for guidance. (default: empty)\n",
            "  --cfg-negative-prompt-file FNAME\n",
            "                        negative prompt file to use for guidance. (default: empty)\n",
            "  --cfg-scale N         strength of guidance (default: 1.000000, 1.0 = disable)\n",
            "  --rope-scaling {none,linear,yarn}\n",
            "                        RoPE frequency scaling method, defaults to linear unless specified by the model\n",
            "  --rope-scale N        RoPE context scaling factor, expands context by a factor of N\n",
            "  --rope-freq-base N    RoPE base frequency, used by NTK-aware scaling (default: loaded from model)\n",
            "  --rope-freq-scale N   RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "  --yarn-orig-ctx N     YaRN: original context size of model (default: 0 = model training context size)\n",
            "  --yarn-ext-factor N   YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)\n",
            "  --yarn-attn-factor N  YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "  --yarn-beta-slow N    YaRN: high correction dim or alpha (default: 1.0)\n",
            "  --yarn-beta-fast N    YaRN: low correction dim or beta (default: 32.0)\n",
            "  --ignore-eos          ignore end of stream token and continue generating (implies --logit-bias 2-inf)\n",
            "  --no-penalize-nl      do not penalize newline token\n",
            "  --temp N              temperature (default: 0.8)\n",
            "  --logits-all          return logits for all tokens in the batch (default: disabled)\n",
            "  --hellaswag           compute HellaSwag score over random tasks from datafile supplied with -f\n",
            "  --hellaswag-tasks N   number of tasks to use when computing the HellaSwag score (default: 400)\n",
            "  --keep N              number of tokens to keep from the initial prompt (default: 0, -1 = all)\n",
            "  --draft N             number of tokens to draft for speculative decoding (default: 16)\n",
            "  --chunks N            max number of chunks to process (default: -1, -1 = all)\n",
            "  -np N, --parallel N   number of parallel sequences to decode (default: 1)\n",
            "  -ns N, --sequences N  number of sequences to decode (default: 1)\n",
            "  -pa N, --p-accept N   speculative decoding accept probability (default: 0.5)\n",
            "  -ps N, --p-split N    speculative decoding split probability (default: 0.1)\n",
            "  -cb, --cont-batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n",
            "  --mmproj MMPROJ_FILE  path to a multimodal projector file for LLaVA. see examples/llava/README.md\n",
            "  --image IMAGE_FILE    path to an image file. use with multimodal models\n",
            "  --mlock               force system to keep model in RAM rather than swapping or compressing\n",
            "  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using mlock)\n",
            "  --numa                attempt optimizations that help on some NUMA systems\n",
            "                        if run without this previously, it is recommended to drop the system page cache before using this\n",
            "                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "  --verbose-prompt      print prompt before generation\n",
            "  -dkvc, --dump-kv-cache\n",
            "                        verbose print of the KV cache\n",
            "  -nkvo, --no-kv-offload\n",
            "                        disable KV offload\n",
            "  -ctk TYPE, --cache-type-k TYPE\n",
            "                        KV cache data type for K (default: f16)\n",
            "  -ctv TYPE, --cache-type-v TYPE\n",
            "                        KV cache data type for V (default: f16)\n",
            "  --simple-io           use basic IO for better compatibility in subprocesses and limited consoles\n",
            "  --lora FNAME          apply LoRA adapter (implies --no-mmap)\n",
            "  --lora-scaled FNAME S apply LoRA adapter with user defined scaling S (implies --no-mmap)\n",
            "  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n",
            "  -m FNAME, --model FNAME\n",
            "                        model path (default: models/7B/ggml-model-f16.gguf)\n",
            "  -md FNAME, --model-draft FNAME\n",
            "                        draft model for speculative decoding\n",
            "  -ld LOGDIR, --logdir LOGDIR\n",
            "                        path under which to save YAML logs (no logging if unset)\n",
            "  --override-kv KEY=TYPE:VALUE\n",
            "                        advanced option to override model metadata by key. may be specified multiple times.\n",
            "                        types: int, float, bool. example: --override-kv tokenizer.ggml.add_bos_token=bool:false\n",
            "\n",
            "log options:\n",
            "  --log-test            Run simple logging test\n",
            "  --log-disable         Disable trace logs\n",
            "  --log-enable          Enable trace logs\n",
            "  --log-file            Specify a log filename (without extension)\n",
            "  --log-new             Create a separate new log file on start. Each log file will have unique name: \"<name>.<ID>.log\"\n",
            "  --log-append          Don't truncate the old log file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "更改-ngl 32要卸载到 GPU 的层数。如果您没有 GPU 加速，请将其删除。\n",
        "\n",
        "更改-c 4096为所需的序列长度。对于扩展序列模型 - 例如 8K、16K、32K - 从 GGUF 文件中读取必要的 RoPE 缩放参数并由 llama.cpp 自动设置。\n",
        "\n",
        "如果您想进行聊天式对话，请将-p <PROMPT>参数替换为-i -ins\n",
        "\n",
        "其他参数以及使用方法请参考llama.cpp文档:\n",
        "\n",
        "https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md\n",
        "\n"
      ],
      "metadata": {
        "id": "Ow3tw6DF7TX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m ./models/llama-2-7b-chat.Q4_K_M.gguf --color \\\n",
        "  -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 \\\n",
        "  --multiline-input \\\n",
        "  -p \"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n{prompt}[/INST]\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG_rVkzN-ldA",
        "outputId": "1cbdf74d-2c9e-4b72-c632-87dac8ad6da3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 1685 (28cb35a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1703239422\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
            "llm_load_tensors: system memory used  = 3891.35 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_build_graph: non-view tensors processed: 676/676\n",
            "llama_new_context_with_model: compute buffer total size = 291.19 MiB\n",
            "\n",
            "system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.700\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
            "generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m [INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n{prompt}[/INST]\u001b[0m  Of course! I'm here to help and provide respectful and accurate responses. I understand that it's important to be socially unbiased and positive in nature, and I will always strive to do so. Please feel free to ask me any question, and I will do my best to assist you. If a question is not factually coherent or does not make sense, I will explain why instead of providing an incorrect answer. And if I don't know the answer to a question, I will let you know rather than sharing false information. Please go ahead and ask me anything! [end of text]\n",
            "\n",
            "llama_print_timings:        load time =    6931.46 ms\n",
            "llama_print_timings:      sample time =      83.86 ms /   126 runs   (    0.67 ms per token,  1502.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =   22104.51 ms /   140 tokens (  157.89 ms per token,     6.33 tokens per second)\n",
            "llama_print_timings:        eval time =   26176.67 ms /   125 runs   (  209.41 ms per token,     4.78 tokens per second)\n",
            "llama_print_timings:       total time =   48429.09 ms\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m ./models/llama-2-7b-chat.Q4_K_M.gguf --color \\\n",
        "  -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 \\\n",
        "  --multiline-input \\\n",
        "  -i -ins\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHhrBmdI4u1a",
        "outputId": "c1b22f52-3270-4de8-ca3c-b9c80af4ca59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 1685 (28cb35a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1703239489\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
            "llm_load_tensors: system memory used  = 3891.35 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_build_graph: non-view tensors processed: 676/676\n",
            "llama_new_context_with_model: compute buffer total size = 291.19 MiB\n",
            "\n",
            "system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "main: interactive mode on.\n",
            "Reverse prompt: '### Instruction:\n",
            "\n",
            "'\n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.700\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
            "generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - To return control to LLaMa, end your input with '\\'.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            "\n",
            "\u001b[33m\u001b[0m\n",
            "> \u001b[1m\u001b[32mhi\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mhello!\n",
            "\n",
            "> \u001b[1m\u001b[32mwhat's your name \\\u001b[33m\b\\\b \b\n",
            "\u001b[0mMy name is John.\n",
            "\n",
            "> \u001b[1m\u001b[32mhow old are u\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mI am 27 years old.\n",
            "\n",
            "> \u001b[1m\u001b[32mwhat's transformer\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mA Transformer is a type of deep learning model that is commonly used for natural language processing tasks, such as machine translation and text classification. It was introduced in a paper by Vaswani et al. in 2017 and has since become widely adopted in the field. The Transformer model relies on self-attention mechanisms to process input sequences of arbitrary length and has been shown to achieve state-of-the-art results on many natural language processing tasks.\n",
            "\n",
            "> \u001b[1m\u001b[32mhow to learn it\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mThere are several ways to learn about the Transformer model and its applications in natural language processing. Here are some steps you can take:\n",
            "\n",
            "1. Read papers and research articles: One of the best ways to learn about the Transformer model is to read research papers and articles that discuss its development and applications. You can start by searching for papers on arXiv, a repository of electronic preprints in physics, mathematics, computer science, and related disciplines.\n",
            "2. Take online courses: There are many online courses available that cover the Transformer model and its applications. These courses can provide a comprehensive introduction to the model and its use cases, as well as practical experience in implementing and using it.\n",
            "3. Watch tutorials and videos: There are many tutorials and videos available online that explain the Transformer model and its applications. These can be a good way to get an overview of the model and how it works, as well as see examples of its use in action.\n",
            "4. Join communities and forums: Joining communities and forums dedicated to natural language processing and machine learning can be a great way to learn from others who are working with the Transformer model. These communities can provide valuable insights and practical advice on how to use the model, as well as opportunities to ask questions and get help when you need it.\n",
            "5. Experiment and practice: One of the best ways to really understand the Transformer model is to experiment with it and practice using it yourself. This can involve implementing the model from scratch or using pre-trained models and working through examples and exercises.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "> \u001b[1m\u001b[32m3q\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mI'm not sure I understand what you are saying with \"3q\". Could you explain?\n",
            "\n",
            "> \u001b[1m\u001b[32mthnk you :)\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mOf course! I'm glad to help. Do you have any questions about the Transformer model or natural language processing in general?\n",
            "\n",
            "> \u001b[1m\u001b[32mdo u kown io_uring?\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mI'm afraid I don't know anything about \"io_uring\". Could you please provide more context or information about what this is?\n",
            "\n",
            "> \u001b[1m\u001b[32mio_uring in linux\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mI apologize, but I'm not familiar with \"io_uring\" in Linux. It could be a specific function or system call in the Linux kernel, but without more information, I can't provide a detailed answer. Can you please provide more context or clarify what you are asking about?\n",
            "\n",
            "> \u001b[1m\u001b[32mio_uring is a new asynchronous I/\u001b[33m\b/\u001b[1m\u001b[32m\b/O API for Linux created by Jens Axboe from Facebook\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mThank you for the information! \"io_uring\" is indeed a new asynchronous I/O API for Linux, developed by Jens Axboe from Facebook. It's designed to provide a more efficient and flexible way of handling I/O operations in Linux, particularly for modern workloads that require high throughput and low latency.\n",
            "### Instruction:\n",
            "\n",
            "\n",
            "> \u001b[1m\u001b[32mio_uring \\\u001b[33m\b\\\b \b\n",
            "\u001b[0mGreat! So, \"io_uring\" is a new API for Linux that provides asynchronous I/O capabilities, which can help improve the performance and efficiency of I/O operations in modern workloads. Can you tell me more about how it works?\n",
            "\n",
            "> \u001b[1m\u001b[32mamazing, i want u tell me\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mOf course! \"io_uring\" is a kernel-level API that provides a more efficient and flexible way of handling I/O operations in Linux. It's designed to work with a variety of file systems, including block devices, character devices, and network sockets.\n",
            "Here are some key features of \"io_uring\":\n",
            "1. **Asynchronous I/O**: \"io_uring\" allows applications to perform I/O operations asynchronously, which means that the application can continue executing other tasks while waiting for I/O operations to complete. This can significantly improve system performance and responsiveness.\n",
            "2. **Multiplexing**: \"io_uring\" supports multiplexing, which means that an application can perform multiple I/O operations simultaneously on different file systems or devices. This can help improve overall system performance by utilizing the available resources more efficiently.\n",
            "3. **Readahead**: \"io_uring\" includes a readahead feature that allows applications to pre-read data from disk before it's actually needed, which can significantly reduce the amount of time spent waiting for I/O operations to complete.\n",
            "4. **Direct I/O**: \"io_uring\" supports direct I/O, which means that applications can perform I/O operations directly on the device without going through the file system. This can improve performance by reducing the overhead associated with file system operations.\n",
            "5. **Zero-copy**: \"io_uring\" includes a zero-copy feature that allows applications to perform I/O operations without copying data between memory and disk. This can significantly reduce the amount of time spent waiting for I/O operations to complete.\n",
            "Overall, \"io_uring\" provides a powerful and flexible way of handling I/O operations in Linux, which can help improve system performance and responsiveness in modern workloads.\n",
            "\n",
            "> \u001b[1m\u001b[32mif u interview acompany, how to get offer \\\u001b[33m\b\\\b \b\n",
            "\u001b[0mTo increase your chances of getting an offer from a company during an interview, here are some tips:\n",
            "1. **Research the company**: Before the interview, research the company's products or services, mission statement, values, and culture. This will help you understand their needs and expectations and show your enthusiasm for the job.\n",
            "2. **Dress appropriately**: Dress professionally and conservatively for the interview. Make sure your attire is clean, ironed, and appropriate for the type of job you are applying for.\n",
            "3. **Be punctual**: Arrive on time for the interview. Plan to arrive at least 10-15 minutes early to show that you are reliable and respectful of their time.\n",
            "4. **Bring copies of your resume**: Bring multiple copies of your resume to the interview, in case they ask for one. Make sure your resume is updated and tailored to the job description.\n",
            "5. **Prepare questions**: Prepare a list of thoughtful questions to ask during the interview. This shows that you are interested in the company and the position, and it can also give you valuable information about the job.\n",
            "6. **Be yourself**: Be honest and authentic during the interview. Don't try to be someone you're not. The interviewer wants to get to know the real you and understand how you will fit in with their company culture.\n",
            "7. **Follow up**: After the interview, send a thank-you note or email to the interviewer, reiterating your interest in the position and thanking them for their time. This shows that you are professional and appreciative of their consideration.\n",
            "By following these tips, you can increase your chances of getting an offer from the company during the interview process. Good luck!\n",
            "\n",
            "> \u001b[1m\u001b[32m"
          ]
        }
      ]
    }
  ]
}