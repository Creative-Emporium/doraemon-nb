{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNfvZAnM8dayBmsriNmtbEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/chat_bot_cpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfmcAQMns6aV",
        "outputId": "23be1471-27bb-4ab8-9cca-ca42d9e70527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chat-bot'...\n",
            "remote: Enumerating objects: 529, done.\u001b[K\n",
            "remote: Counting objects: 100% (529/529), done.\u001b[K\n",
            "remote: Compressing objects: 100% (325/325), done.\u001b[K\n",
            "remote: Total 529 (delta 208), reused 465 (delta 149), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (529/529), 4.05 MiB | 30.02 MiB/s, done.\n",
            "Resolving deltas: 100% (208/208), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/weedge/chat-bot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd chat-bot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a1qIXxQdJvV",
        "outputId": "23f484c2-e808-4b82-adda-2014e36a627b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chat-bot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install dependencies"
      ],
      "metadata": {
        "id": "jMvKdwJHlvhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwKwMQL9RNE4",
        "outputId": "b9dce0db-319a-4cc9-883b-ad45e75e31eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatTTS\n",
        "!git submodule update --init --recursive\n",
        "!pip install -q vocos\n",
        "!pip install vector_quantize_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHi3wGoeVwtT",
        "outputId": "669231c1-bb21-4854-829e-ffb9f0dd85e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting vector_quantize_pytorch\n",
            "  Downloading vector_quantize_pytorch-1.14.30-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from vector_quantize_pytorch) (0.8.0)\n",
            "Collecting einx>=0.2.2 (from vector_quantize_pytorch)\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from vector_quantize_pytorch) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from einx>=0.2.2->vector_quantize_pytorch) (1.25.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from einx>=0.2.2->vector_quantize_pytorch) (1.12.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from einx>=0.2.2->vector_quantize_pytorch) (2.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->vector_quantize_pytorch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->vector_quantize_pytorch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->vector_quantize_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->einx>=0.2.2->vector_quantize_pytorch) (1.3.0)\n",
            "Downloading vector_quantize_pytorch-1.14.30-py3-none-any.whl (37 kB)\n",
            "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einx, vector_quantize_pytorch\n",
            "Successfully installed einx-0.3.0 vector_quantize_pytorch-1.14.30\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python3-pyaudio && pip install pyannote.audio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNTGOdhEkXDX",
        "outputId": "d730e4eb-a1d8-400e-998f-e2990d10b5b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2\n",
            "Suggested packages:\n",
            "  python-pyaudio-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 python3-pyaudio\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 91.2 kB of archives.\n",
            "After this operation, 340 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-pyaudio amd64 0.2.11-1.3ubuntu1 [25.9 kB]\n",
            "Fetched 91.2 kB in 1s (166 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package python3-pyaudio.\n",
            "Preparing to unpack .../python3-pyaudio_0.2.11-1.3ubuntu1_amd64.deb ...\n",
            "Unpacking python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyannote.audio\n",
            "  Downloading pyannote.audio-3.3.1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting einops>=0.6.0 (from pyannote.audio)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.23.4)\n",
            "Collecting lightning>=2.0.1 (from pyannote.audio)\n",
            "  Downloading lightning-2.3.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<3.0,>=2.1 (from pyannote.audio)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.database-5.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
            "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n",
            "  Downloading pytorch_metric_learning-2.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (13.7.1)\n",
            "Collecting semver>=3.0.0 (from pyannote.audio)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.12.1)\n",
            "Collecting speechbrain>=1.0.0 (from pyannote.audio)\n",
            "  Downloading speechbrain-1.0.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tensorboardX>=2.6 (from pyannote.audio)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.3.0+cu121)\n",
            "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torch_audiomentations-0.11.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.3.0+cu121)\n",
            "Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.15.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.66.4)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading pytorch_lightning-2.3.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.11.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.0.3)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.12.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.2.2)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.7.1)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.12.1)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (2.16.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.16.0)\n",
            "Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.4.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.1.99)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.6->pyannote.audio) (3.20.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pyannote.audio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.10.2.post1)\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.9.5)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (3.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.0.1->pyannote.audio) (67.7.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.8.2)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.30)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (4.0.3)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.16.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.0.3)\n",
            "Downloading pyannote.audio-3.3.1-py2.py3-none-any.whl (898 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.database-5.1.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytorch_metric_learning-2.5.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading speechbrain-1.0.0-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.1/760.1 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_audiomentations-0.11.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
            "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading pytorch_lightning-2.3.0-py3-none-any.whl (812 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.2/812.2 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=e0cd2180c1a0fd28ab806efa7f511ffc25f16a0ee116edc997e68810acca2092\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=02c76ba950431a84ac0fdf5451f1026013485086fc4168a902d98475e5d97ef7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=0759666394c9250e9dbf6bdf3e054445e4862e72d3ce779eb9607e6c33b8989a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
            "Successfully built antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: primePy, docopt, antlr4-python3-runtime, tensorboardX, semver, ruamel.yaml.clib, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, Mako, lightning-utilities, einops, colorlog, ruamel.yaml, pyannote.core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alembic, optuna, nvidia-cusolver-cu12, hyperpyyaml, pyannote.database, torchmetrics, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, torch-audiomentations, lightning, pyannote.audio\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 colorlog-6.8.2 docopt-0.6.2 einops-0.8.0 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.3.0 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 optuna-3.6.1 primePy-1.3 pyannote.audio-3.3.1 pyannote.core-5.0.0 pyannote.database-5.1.0 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.3.0 pytorch-metric-learning-2.5.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 semver-3.0.2 speechbrain-1.0.0 tensorboardX-2.6.2.2 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.4 torchmetrics-1.4.0.post0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyaudio pydub redis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGEdDL-QhXLt",
        "outputId": "02e82ef0-778a-40f7-b88f-b910f94d996d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyaudio in /usr/lib/python3/dist-packages (0.2.11)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting redis\n",
            "  Downloading redis-5.0.6-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading redis-5.0.6-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, redis\n",
            "Successfully installed pydub-0.25.1 redis-5.0.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# asr whisper\n",
        "!pip install openai-whisper whisper-timestamped faster-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbujgZGJlJwc",
        "outputId": "43b5ef38-31da-4e75-83b5-f1acfc38957e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/798.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m788.5/798.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting whisper-timestamped\n",
            "  Downloading whisper_timestamped-1.15.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting faster-whisper\n",
            "  Downloading faster_whisper-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from whisper-timestamped) (3.0.10)\n",
            "Collecting dtw-python (from whisper-timestamped)\n",
            "  Downloading dtw_python-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av<13,>=11.0 (from faster-whisper)\n",
            "  Downloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
            "  Downloading ctranslate2-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.23.4)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.19.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (67.7.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.15.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dtw-python->whisper-timestamped) (1.11.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.6.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Downloading whisper_timestamped-1.15.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faster_whisper-1.0.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dtw_python-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801359 sha256=ba997651568f05d62f58b8666eaa137dd98bd0f474e8b6d3207079280a403587\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: humanfriendly, ctranslate2, av, tiktoken, dtw-python, coloredlogs, onnxruntime, openai-whisper, faster-whisper, whisper-timestamped\n",
            "Successfully installed av-12.1.0 coloredlogs-15.0.1 ctranslate2-4.3.1 dtw-python-1.5.1 faster-whisper-1.0.2 humanfriendly-10.0 onnxruntime-1.18.0 openai-whisper-20231117 tiktoken-0.7.0 whisper-timestamped-1.15.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llamacpp https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#installation\n",
        "# cpu\n",
        "!pip install llama-cpp-python\n",
        "#!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
        "#!CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n",
        "\n",
        "# gpu\n",
        "#!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wKEoTuwRZiq",
        "outputId": "ccff3c72-1a65-4c16-9b5d-5b7bc746c9b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=3712886 sha256=686325fafc40ffd71bc35bfb421b0ebda5c8aa42e09eda8c80c7c4d9b9fb3f38\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybwIxsrknMlO",
        "outputId": "c41b1573-da8e-4706-a953-770ca8e4395e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch-lightning                2.3.0\n",
            "pytorch-metric-learning          2.5.0\n",
            "torch                            2.3.0+cu121\n",
            "torch-audiomentations            0.11.1\n",
            "torch-pitch-shift                1.2.4\n",
            "torchaudio                       2.3.0+cu121\n",
            "torchmetrics                     1.4.0.post0\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.18.0\n",
            "torchvision                      0.18.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show omegaconf einops IPython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXUdeKJPtvV3",
        "outputId": "ada448c7-cb60-4d14-d421-06bd1d0f59fd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: omegaconf\n",
            "Version: 2.3.0\n",
            "Summary: A flexible configuration library\n",
            "Home-page: https://github.com/omry/omegaconf\n",
            "Author: Omry Yadan\n",
            "Author-email: omry@yadan.net\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: antlr4-python3-runtime, PyYAML\n",
            "Required-by: pyannote.audio\n",
            "---\n",
            "Name: einops\n",
            "Version: 0.8.0\n",
            "Summary: A new flavour of deep learning operations\n",
            "Home-page: https://github.com/arogozhnikov/einops\n",
            "Author: Alex Rogozhnikov\n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: encodec, pyannote.audio, vector-quantize-pytorch, vocos\n",
            "---\n",
            "Name: ipython\n",
            "Version: 7.34.0\n",
            "Summary: IPython: Productive Interactive Computing\n",
            "Home-page: https://ipython.org\n",
            "Author: The IPython Development Team\n",
            "Author-email: ipython-dev@python.org\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: backcall, decorator, jedi, matplotlib-inline, pexpect, pickleshare, prompt-toolkit, pygments, setuptools, traitlets\n",
            "Required-by: cufflinks, datascience, google-colab, ipykernel, ipython-sql, ipywidgets, jupyter-console\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download models"
      ],
      "metadata": {
        "id": "HkXuo0PSlriI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# asr openai whisper\n",
        "!huggingface-cli download openai/whisper-base  --local-dir ./models/openai/whisper-base --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUBGxsNH9Y1v",
        "outputId": "7ab89bb1-b04d-4c36-cb29-4feb6a38259e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Fetching 16 files:   0% 0/16 [00:00<?, ?it/s]Downloading 'generation_config.json' to 'models/openai/whisper-base/.huggingface/download/generation_config.json.38eda87a0b22fdb85af5dfa14cf065147337137a.incomplete'\n",
            "Downloading 'model.safetensors' to 'models/openai/whisper-base/.huggingface/download/model.safetensors.07cadb9f25677c8d50df603e66a98fbd842cce45047139baeb16e6219a1e807b.incomplete'\n",
            "Downloading 'README.md' to 'models/openai/whisper-base/.huggingface/download/README.md.71bf50808e42afc852774284aa234a4346189d85.incomplete'\n",
            "Downloading 'merges.txt' to 'models/openai/whisper-base/.huggingface/download/merges.txt.6038932a2a1f09a66991b1c2adae0d14066fa29e.incomplete'\n",
            "\n",
            "generation_config.json: 100% 3.81k/3.81k [00:00<00:00, 26.5MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/generation_config.json\n",
            "Downloading 'added_tokens.json' to 'models/openai/whisper-base/.huggingface/download/added_tokens.json.e3d256c988462aa153dcabe2aa38b8e9b436c06f.incomplete'\n",
            "Downloading 'flax_model.msgpack' to 'models/openai/whisper-base/.huggingface/download/flax_model.msgpack.f69b3fcc5b0044b1436bc9e4181548709129977871877b875bddfaf1b41fc667.incomplete'\n",
            "Downloading 'config.json' to 'models/openai/whisper-base/.huggingface/download/config.json.708cca09e7e8a9364e444efe8b4f9c2691fc5f51.incomplete'\n",
            "Downloading '.gitattributes' to 'models/openai/whisper-base/.huggingface/download/.gitattributes.3476589a58942b0e9ef94ae0efb29355707f95bb.incomplete'\n",
            "\n",
            "merges.txt:   0% 0.00/494k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "README.md: 100% 19.8k/19.8k [00:00<00:00, 71.1MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/README.md\n",
            "Downloading 'normalizer.json' to 'models/openai/whisper-base/.huggingface/download/normalizer.json.dd6ae819ad738ac1a546e9f9282ef325c33b9ea0.incomplete'\n",
            "\n",
            "\n",
            "added_tokens.json: 100% 34.6k/34.6k [00:00<00:00, 2.31MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/added_tokens.json\n",
            "\n",
            "\n",
            ".gitattributes: 100% 1.44k/1.44k [00:00<00:00, 9.59MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/.gitattributes\n",
            "Fetching 16 files:   6% 1/16 [00:00<00:07,  2.02it/s]Downloading 'preprocessor_config.json' to 'models/openai/whisper-base/.huggingface/download/preprocessor_config.json.c2048dfa9fd94a052e62e908d2c4dfb18534b4d2.incomplete'\n",
            "\n",
            "\n",
            "merges.txt: 100% 494k/494k [00:00<00:00, 5.96MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/merges.txt\n",
            "normalizer.json: 100% 52.7k/52.7k [00:00<00:00, 2.67MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/normalizer.json\n",
            "Downloading 'pytorch_model.bin' to 'models/openai/whisper-base/.huggingface/download/pytorch_model.bin.c37f294c9563a49217d108687fbd3cda5bdf0441a451512d87bf66343ab31a87.incomplete'\n",
            "Downloading 'special_tokens_map.json' to 'models/openai/whisper-base/.huggingface/download/special_tokens_map.json.bf69932dca4b3719b59fdd8f6cc1978109509f6c.incomplete'\n",
            "\n",
            "preprocessor_config.json:   0% 0.00/185k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model.safetensors:   0% 0.00/290M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "config.json: 100% 1.98k/1.98k [00:00<00:00, 12.1MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/config.json\n",
            "Downloading 'tf_model.h5' to 'models/openai/whisper-base/.huggingface/download/tf_model.h5.a997bb84b79970c89a3b5dd6188b18c1e0f1814cfecefbe40039ed052361129b.incomplete'\n",
            "Downloading 'tokenizer.json' to 'models/openai/whisper-base/.huggingface/download/tokenizer.json.1e95340ff836fad1b5932e800fb7b8c5e6d78a74.incomplete'\n",
            "preprocessor_config.json: 100% 185k/185k [00:00<00:00, 4.06MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/preprocessor_config.json\n",
            "\n",
            "special_tokens_map.json: 100% 2.19k/2.19k [00:00<00:00, 19.0MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/special_tokens_map.json\n",
            "Downloading 'tokenizer_config.json' to 'models/openai/whisper-base/.huggingface/download/tokenizer_config.json.d13b786c04765fb1a06492b53587752cd67665ea.incomplete'\n",
            "Downloading 'vocab.json' to 'models/openai/whisper-base/.huggingface/download/vocab.json.d7016e21da8776c8a9d577d0f559600f09a240eb.incomplete'\n",
            "\n",
            "tokenizer_config.json:   0% 0.00/283k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "vocab.json:   0% 0.00/836k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   4% 10.5M/290M [00:00<00:04, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 283k/283k [00:00<00:00, 3.64MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/tokenizer_config.json\n",
            "vocab.json: 100% 836k/836k [00:00<00:00, 11.3MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/vocab.json\n",
            "\n",
            "\n",
            "model.safetensors:  11% 31.5M/290M [00:00<00:02, 116MB/s] \u001b[A\u001b[A\n",
            "flax_model.msgpack:   0% 0.00/290M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model.safetensors:  18% 52.4M/290M [00:00<00:01, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 2.48M/2.48M [00:00<00:00, 10.6MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/tokenizer.json\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:   0% 0.00/290M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  25% 73.4M/290M [00:00<00:01, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:   0% 0.00/291M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:   4% 10.5M/290M [00:00<00:06, 42.8MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  32% 94.4M/290M [00:00<00:01, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:   4% 10.5M/290M [00:00<00:06, 43.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  11% 31.5M/290M [00:00<00:02, 87.7MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  40% 115M/290M [00:00<00:01, 168MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  47% 136M/290M [00:00<00:00, 172MB/s]\u001b[A\u001b[A\n",
            "flax_model.msgpack:  18% 52.4M/290M [00:00<00:02, 104MB/s] \u001b[A\n",
            "\n",
            "model.safetensors:  54% 157M/290M [00:00<00:00, 176MB/s]\u001b[A\u001b[A\n",
            "flax_model.msgpack:  25% 73.4M/290M [00:00<00:01, 115MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  61% 178M/290M [00:01<00:00, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  69% 199M/290M [00:01<00:00, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  11% 31.5M/290M [00:00<00:06, 38.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  76% 220M/290M [00:01<00:00, 180MB/s]\u001b[A\u001b[A\n",
            "flax_model.msgpack:  32% 94.4M/290M [00:01<00:02, 93.7MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  83% 241M/290M [00:01<00:00, 181MB/s]\u001b[A\u001b[A\n",
            "flax_model.msgpack:  36% 105M/290M [00:01<00:01, 94.0MB/s] \u001b[A\n",
            "\n",
            "model.safetensors:  90% 262M/290M [00:01<00:00, 184MB/s]\u001b[A\u001b[A\n",
            "flax_model.msgpack:  40% 115M/290M [00:01<00:01, 94.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  18% 52.4M/290M [00:01<00:04, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  97% 283M/290M [00:01<00:00, 180MB/s]\u001b[A\u001b[A\n",
            "flax_model.msgpack:  43% 126M/290M [00:01<00:01, 94.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.safetensors: 100% 290M/290M [00:01<00:00, 168MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/model.safetensors\n",
            "\n",
            "flax_model.msgpack:  47% 136M/290M [00:01<00:01, 93.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  29% 83.9M/290M [00:01<00:03, 66.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  51% 147M/290M [00:01<00:01, 94.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:   4% 10.5M/291M [00:01<00:38, 7.23MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  54% 157M/290M [00:01<00:01, 92.9MB/s]\u001b[A\n",
            "flax_model.msgpack:  58% 168M/290M [00:01<00:01, 93.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  36% 105M/290M [00:01<00:02, 67.1MB/s] \u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  61% 178M/290M [00:01<00:01, 93.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:   7% 21.0M/291M [00:01<00:20, 13.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  43% 126M/290M [00:01<00:01, 83.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  65% 189M/290M [00:02<00:01, 91.8MB/s]\u001b[A\n",
            "flax_model.msgpack:  69% 199M/290M [00:02<00:00, 92.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  11% 31.5M/291M [00:01<00:13, 19.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  47% 136M/290M [00:02<00:02, 72.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  72% 210M/290M [00:02<00:00, 92.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  14% 41.9M/291M [00:02<00:08, 28.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  76% 220M/290M [00:02<00:00, 91.5MB/s]\u001b[A\n",
            "flax_model.msgpack:  79% 231M/290M [00:02<00:00, 92.0MB/s]\u001b[A\n",
            "flax_model.msgpack:  83% 241M/290M [00:02<00:00, 91.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  18% 52.4M/291M [00:02<00:08, 29.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  87% 252M/290M [00:02<00:00, 91.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  54% 157M/290M [00:02<00:02, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  25% 73.4M/291M [00:02<00:04, 48.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  90% 262M/290M [00:02<00:00, 91.2MB/s]\u001b[A\n",
            "flax_model.msgpack:  94% 273M/290M [00:02<00:00, 91.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  58% 168M/290M [00:02<00:02, 53.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "flax_model.msgpack:  97% 283M/290M [00:03<00:00, 92.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "flax_model.msgpack: 100% 290M/290M [00:03<00:00, 92.1MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/flax_model.msgpack\n",
            "Fetching 16 files:  31% 5/16 [00:04<00:09,  1.20it/s]\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  36% 105M/291M [00:03<00:03, 49.9MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  65% 189M/290M [00:03<00:02, 46.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  43% 126M/291M [00:03<00:02, 64.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  72% 210M/290M [00:03<00:01, 53.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  47% 136M/291M [00:03<00:02, 56.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  79% 231M/290M [00:03<00:00, 67.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  83% 241M/290M [00:03<00:00, 69.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  54% 157M/291M [00:04<00:02, 55.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  90% 262M/290M [00:04<00:00, 71.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  61% 178M/291M [00:04<00:01, 70.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:  97% 283M/290M [00:04<00:00, 82.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin: 100% 290M/290M [00:04<00:00, 62.6MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/pytorch_model.bin\n",
            "Fetching 16 files:  69% 11/16 [00:05<00:02,  2.13it/s]\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  72% 210M/291M [00:04<00:01, 65.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  79% 231M/291M [00:04<00:00, 79.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  83% 241M/291M [00:05<00:00, 68.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5:  90% 262M/291M [00:05<00:00, 70.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5: 100% 291M/291M [00:05<00:00, 50.4MB/s]\n",
            "Download complete. Moving file to models/openai/whisper-base/tf_model.h5\n",
            "Fetching 16 files: 100% 16/16 [00:06<00:00,  2.32it/s]\n",
            "/content/chat-bot/models/openai/whisper-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llm llamacpp phi-3-mini-4k-instruct\n",
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir ./models --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELUVP0Gx9qAf",
        "outputId": "c1019de3-b45a-4ef7-f2fc-9efd1e01363d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'Phi-3-mini-4k-instruct-q4.gguf' to 'models/.huggingface/download/Phi-3-mini-4k-instruct-q4.gguf.8a83c7fb9049a9b2e92266fa7ad04933bb53aa1e85136b7b30f1b8000ff2edef.incomplete'\n",
            "Phi-3-mini-4k-instruct-q4.gguf: 100% 2.39G/2.39G [00:13<00:00, 172MB/s]\n",
            "Download complete. Moving file to models/Phi-3-mini-4k-instruct-q4.gguf\n",
            "models/Phi-3-mini-4k-instruct-q4.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tts chatTTS\n",
        "!huggingface-cli download 2Noise/ChatTTS  --local-dir ./models/2Noise/ChatTTS --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIf4oBlZmH9j",
        "outputId": "19ec634d-4c42-4f50-9a8b-706a2c6a90e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]Downloading 'asset/Vocos.pt' to 'models/2Noise/ChatTTS/.huggingface/download/asset/Vocos.pt.09a670eda1c08b740013679c7a90ebb7f1a97646ea7673069a6838e6b51d6c58.incomplete'\n",
            "Downloading 'asset/Decoder.pt' to 'models/2Noise/ChatTTS/.huggingface/download/asset/Decoder.pt.9964e36e840f0e3a748c5f716fe6de6490d2135a5f5155f4a642d51860e2ec38.incomplete'\n",
            "Downloading 'asset/GPT.pt' to 'models/2Noise/ChatTTS/.huggingface/download/asset/GPT.pt.d7d4ee6461ea097a2be23eb40d73fb94ad3b3d39cb64fbb50cb3357fd466cadb.incomplete'\n",
            "Downloading 'asset/tokenizer.pt' to 'models/2Noise/ChatTTS/.huggingface/download/asset/tokenizer.pt.e911ae7c6a7c27953433f35c44227a67838fe229a1f428503bdb6cd3d1bcc69c.incomplete'\n",
            "Downloading 'asset/spk_stat.pt' to 'models/2Noise/ChatTTS/.huggingface/download/asset/spk_stat.pt.3228d8a4cbbf349d107a1b76d2f47820865bd3c9928c4bdfe1cefd5c7071105f.incomplete'\n",
            "Downloading '.gitattributes' to 'models/2Noise/ChatTTS/.huggingface/download/.gitattributes.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'asset/DVAE.pt' to 'models/2Noise/ChatTTS/.huggingface/download/asset/DVAE.pt.613cb128adf89188c93ea5880ea0b798e66b1fe6186d0c535d99bcd87bfd6976.incomplete'\n",
            "Downloading 'README.md' to 'models/2Noise/ChatTTS/.huggingface/download/README.md.c1018f04f4d516b764702c1c44d83a9272d63418.incomplete'\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 8.39MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/.gitattributes\n",
            "Fetching 13 files:   8% 1/13 [00:00<00:05,  2.11it/s]\n",
            "README.md: 100% 1.93k/1.93k [00:00<00:00, 11.0MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/README.md\n",
            "Downloading 'config/decoder.yaml' to 'models/2Noise/ChatTTS/.huggingface/download/config/decoder.yaml.9c7b3d09af3f9fea19072d4a35aecee15779f51c.incomplete'\n",
            "Downloading 'config/dvae.yaml' to 'models/2Noise/ChatTTS/.huggingface/download/config/dvae.yaml.8df13367906f6cd6b1f88b3cc6f1f15599b19e94.incomplete'\n",
            "\n",
            "config/decoder.yaml: 100% 117/117 [00:00<00:00, 696kB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/config/decoder.yaml\n",
            "\n",
            "config/dvae.yaml: 100% 143/143 [00:00<00:00, 814kB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/config/dvae.yaml\n",
            "\n",
            "Vocos.pt:   0% 0.00/54.4M [00:00<?, ?B/s]\u001b[ADownloading 'config/gpt.yaml' to 'models/2Noise/ChatTTS/.huggingface/download/config/gpt.yaml.be32c1231832c60ddad7e0c2e8bd027f51a183b2.incomplete'\n",
            "Downloading 'config/path.yaml' to 'models/2Noise/ChatTTS/.huggingface/download/config/path.yaml.5ea569e3431b0ed2aa1c699461017c7174d2f56d.incomplete'\n",
            "\n",
            "\n",
            "DVAE.pt:   0% 0.00/27.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Decoder.pt:   0% 0.00/104M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "config/gpt.yaml: 100% 346/346 [00:00<00:00, 1.96MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/config/gpt.yaml\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spk_stat.pt: 100% 4.26k/4.26k [00:00<00:00, 19.2MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/asset/spk_stat.pt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   0% 0.00/901M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config/path.yaml: 100% 309/309 [00:00<00:00, 2.22MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/config/path.yaml\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.pt:   0% 0.00/337k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'config/vocos.yaml' to 'models/2Noise/ChatTTS/.huggingface/download/config/vocos.yaml.12d59e7d0af9ccfd5deb4ec01b4db3855f3d7314.incomplete'\n",
            "tokenizer.pt: 100% 337k/337k [00:00<00:00, 4.47MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/asset/tokenizer.pt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config/vocos.yaml: 100% 460/460 [00:00<00:00, 2.35MB/s]\n",
            "\n",
            "Vocos.pt:  19% 10.5M/54.4M [00:00<00:00, 52.8MB/s]\u001b[ADownload complete. Moving file to models/2Noise/ChatTTS/config/vocos.yaml\n",
            "\n",
            "\n",
            "DVAE.pt:  38% 10.5M/27.7M [00:00<00:00, 48.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   1% 10.5M/901M [00:00<00:17, 51.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Decoder.pt:  10% 10.5M/104M [00:00<00:02, 44.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Vocos.pt:  39% 21.0M/54.4M [00:00<00:00, 68.5MB/s]\u001b[A\n",
            "\n",
            "DVAE.pt:  76% 21.0M/27.7M [00:00<00:00, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Decoder.pt:  20% 21.0M/104M [00:00<00:01, 62.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   3% 31.5M/901M [00:00<00:10, 85.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "DVAE.pt: 100% 27.7M/27.7M [00:00<00:00, 64.2MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/asset/DVAE.pt\n",
            "Fetching 13 files:  23% 3/13 [00:01<00:03,  2.72it/s]\n",
            "\n",
            "\n",
            "Vocos.pt: 100% 54.4M/54.4M [00:00<00:00, 91.2MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/asset/Vocos.pt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   6% 52.4M/901M [00:00<00:08, 104MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Decoder.pt:  61% 62.9M/104M [00:00<00:00, 107MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   8% 73.4M/901M [00:00<00:07, 114MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Decoder.pt:  81% 83.9M/104M [00:00<00:00, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  10% 94.4M/901M [00:00<00:06, 118MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Decoder.pt: 100% 104M/104M [00:01<00:00, 101MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/asset/Decoder.pt\n",
            "Fetching 13 files:  31% 4/13 [00:01<00:04,  2.23it/s]\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  13% 115M/901M [00:01<00:06, 123MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  15% 136M/901M [00:01<00:05, 128MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  17% 157M/901M [00:01<00:05, 131MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  20% 178M/901M [00:01<00:05, 132MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  22% 199M/901M [00:01<00:05, 133MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  24% 220M/901M [00:01<00:05, 132MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  27% 241M/901M [00:01<00:05, 128MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  29% 262M/901M [00:02<00:05, 123MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  31% 283M/901M [00:02<00:05, 122MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  34% 304M/901M [00:02<00:05, 118MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  36% 325M/901M [00:02<00:04, 117MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  38% 346M/901M [00:02<00:04, 119MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  41% 367M/901M [00:03<00:04, 120MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  43% 388M/901M [00:03<00:04, 121MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  45% 409M/901M [00:03<00:04, 119MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  48% 430M/901M [00:03<00:04, 117MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  50% 451M/901M [00:03<00:03, 117MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  52% 472M/901M [00:03<00:03, 115MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  55% 493M/901M [00:04<00:03, 118MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  57% 514M/901M [00:04<00:03, 117MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  59% 535M/901M [00:04<00:03, 117MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  62% 556M/901M [00:04<00:02, 122MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  64% 577M/901M [00:04<00:02, 123MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  66% 598M/901M [00:04<00:02, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  69% 619M/901M [00:05<00:02, 129MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  71% 640M/901M [00:05<00:01, 131MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  73% 661M/901M [00:05<00:01, 132MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  76% 682M/901M [00:05<00:01, 133MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  78% 703M/901M [00:05<00:01, 134MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  80% 724M/901M [00:05<00:01, 134MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  83% 744M/901M [00:06<00:01, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  85% 765M/901M [00:06<00:01, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  87% 786M/901M [00:06<00:00, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  90% 807M/901M [00:06<00:00, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  92% 828M/901M [00:06<00:00, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  94% 849M/901M [00:06<00:00, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  97% 870M/901M [00:06<00:00, 135MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt: 100% 901M/901M [00:07<00:00, 124MB/s]\n",
            "Download complete. Moving file to models/2Noise/ChatTTS/asset/GPT.pt\n",
            "Fetching 13 files: 100% 13/13 [00:07<00:00,  1.63it/s]\n",
            "/content/chat-bot/models/2Noise/ChatTTS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run chat bot BE"
      ],
      "metadata": {
        "id": "b2IgjS7bmDzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "redis_pwd = userdata.get('REDIS_PASSWORD')"
      ],
      "metadata": {
        "id": "NIzdOsCZfmQo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p log"
      ],
      "metadata": {
        "id": "T2dP-UcWgCT4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!REDIS_PASSWORD=$redis_pwd RUN_OP=be TQDM_DISABLE=True python -m src.cmd.remote-queue-chat.generate_audio2audio > ./log/be_std_out.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei6CpMWhdhAs",
        "outputId": "d97be045-37fa-4029-94ba-9fa0126683cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /content/chat-bot/models/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "llm_load_vocab: special tokens cache size = 323\n",
            "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi3\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
            "llm_load_print_meta: general.name     = Phi3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2281.66 MiB\n",
            "...........................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    10.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "start loop_asr_llm_generate...\n",
            "start loop_tts_synthesize...\n",
            "\n",
            "llama_print_timings:        load time =     677.27 ms\n",
            "llama_print_timings:      sample time =      94.25 ms /   148 runs   (    0.64 ms per token,  1570.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9839.14 ms /   138 tokens (   71.30 ms per token,    14.03 tokens per second)\n",
            "llama_print_timings:        eval time =   16678.82 ms /   147 runs   (  113.46 ms per token,     8.81 tokens per second)\n",
            "llama_print_timings:       total time =   26762.44 ms /   285 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     677.27 ms\n",
            "llama_print_timings:      sample time =     163.89 ms /   256 runs   (    0.64 ms per token,  1562.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12381.44 ms /   166 tokens (   74.59 ms per token,    13.41 tokens per second)\n",
            "llama_print_timings:        eval time =   32204.52 ms /   255 runs   (  126.29 ms per token,     7.92 tokens per second)\n",
            "llama_print_timings:       total time =   45000.63 ms /   421 tokens\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/chat-bot/src/cmd/remote-queue-chat/generate_audio2audio.py\", line 42, in <module>\n",
            "    main()\n",
            "  File \"/content/chat-bot/src/cmd/remote-queue-chat/generate_audio2audio.py\", line 35, in main\n",
            "    run_be(conn)\n",
            "  File \"/content/chat-bot/src/cmd/be.py\", line 36, in run_be\n",
            "    asr_llm_gen_t.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.10/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1567, in _shutdown\n",
            "    lock.acquire()\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}