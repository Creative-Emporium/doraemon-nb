{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM25LYIKYdQMZfmW3hMjwcy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/ds_examples_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://www.deepspeed.ai/training/\n",
        "- https://www.deepspeed.ai/inference/"
      ],
      "metadata": {
        "id": "xSwHgQn66yHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvwew6-7XV4W",
        "outputId": "0e397cb2-46c3-485e-c068-db2aeb0777f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 13 02:48:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --query-gpu=timestamp,memory.total,memory.free,memory.used,name,utilization.gpu,utilization.memory --format=csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvYIi6UDYuPq",
        "outputId": "6f0af455-ad56-4fed-9237-232c558fbfc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timestamp, memory.total [MiB], memory.free [MiB], memory.used [MiB], name, utilization.gpu [%], utilization.memory [%]\n",
            "2023/12/13 02:48:25.999, 40960 MiB, 40513 MiB, 0 MiB, NVIDIA A100-SXM4-40GB, 0 %, 0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🐕DeepSpeed-Chat：轻松、快速且经济实惠地对所有规模的类似 ChatGPT 模型进行 RLHF 训练🐕\n",
        "\n",
        "- https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/README.md\n",
        "- https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-chat/chinese/README.md\n",
        "\n",
        "本着使 ChatGPT 式模型及其功能民主化的精神，DeepSpeed 很自豪地推出一个通用系统框架，用于为类似 ChatGPT 的模型提供端到端的训练体验，名为DeepSpeed Chat。它可以自动将您最喜欢的预训练大型语言模型通过 OpenAI InstructGPT 风格的三个阶段来生成您自己的高质量 ChatGPT 风格模型。\n",
        "\n",
        "DeepSpeed Chat 使高质量 ChatGPT 式模型的训练变得简单、快速、经济且可扩展。\n",
        "\n",
        "\n",
        "只需单击一下，您就可以在 1.36 小时内在具有 48GB 内存的单个消费级 NVIDIA A6000 GPU 上训练、生成和服务 13 亿个参数的 ChatGPT 模型。在具有 8 个 NVIDIA A100-40G GPU 的单个 DGX 节点上，DeepSpeed-Chat 可以在 13.6 小时内训练 130 亿个参数的 ChatGPT 模型。在多GPU多节点系统（云场景）上，即8个DGX节点和8个NVIDIA A100 GPU/节点，DeepSpeed-Chat可以在9小时内训练出660亿参数的ChatGPT模型。最后，它的训练速度比现有 RLHF 系统快 15 倍，并且可以处理具有超过 2000 亿个参数的类似 ChatGPT 模型的训练：这是现有系统的另一个不可能完成的任务。有关 DeepSpeed-Chat 实现的各种模型大小和低训练成本的全面讨论，请参阅发布博客和训练性能评估。\n",
        "\n",
        "除此版本之外，DeepSpeed 系统一直自豪地充当系统后端，用于加速一系列正在进行的快速训练/微调聊天式模型（例如 LLaMA）的工作。以下是由 DeepSpeed 提供支持的一些开源示例：\n",
        "\n",
        "- [Databricks Dolly](https://github.com/databrickslabs/dolly)\n",
        "- [LMFlow](https://github.com/OptimalScale/LMFlow)\n",
        "- [CarperAI-TRLX](https://github.com/CarperAI/trlx)\n",
        "- [Huggingface-PEFT](https://github.com/huggingface/peft)\n",
        "\n",
        "\n",
        "DeepSpeed Chat 的摘要包括：\n",
        "\n",
        "- DeepSpeed Chat：完整的端到端三阶段 OpenAI InstructGPT 训练策略，具有强化学习人类反馈（RLHF），从用户最喜欢的预训练大语言模型检查点生成高质量的 ChatGPT 式模型；\n",
        "- DeepSpeed 混合引擎：一个新的系统支持快速、经济且可扩展的所有规模的 RLHF 训练。它基于您最喜爱的 DeepSpeed 系统功能（例如 ZeRO 技术和 DeepSpeed-Inference）构建；\n",
        "轻松轻松的训练体验：单个脚本能够采用预先训练的 Huggingface 模型并运行 RLHF 训练的所有三个步骤， a) 监督微调（SFT），b) 奖励模型微调 ，c) 基于人类反馈的强化学习（RLHF）。\n",
        "- 对当今类似 ChatGPT 的模型训练的通用系统支持：DeepSpeed Chat 不仅可以作为基于 3 步指令的 RLHF 管道的系统后端，还可以作为当前的单一模型微调探索（例如，以 LLaMA 为中心的微调）和适用于各种模型和场景的通用 RLHF 训练。\n",
        "\n",
        "参考：\n",
        "- SFT: https://cameronrwolfe.substack.com/p/understanding-and-using-supervised\n",
        "- RHLF: https://huggingface.co/blog/zh/rlhf , https://huyenchip.com/2023/05/02/rlhf.html\n",
        "- [**Training language models to follow instructions with human feedback**](https://arxiv.org/abs/2203.02155)\n",
        "- [**ZeRO: Memory Optimizations Toward Training Trillion Parameter Models**](https://arxiv.org/abs/1910.02054)\n",
        "- [**LoRA: Low-Rank Adaptation of Large Language Models**](https://arxiv.org/abs/2106.09685)\n",
        "- [**Proximal Policy Optimization**](https://arxiv.org/abs/1707.06347)\n",
        "\n",
        "Tips:\n",
        "- 相关微调操作可以通过 PERF: https://huggingface.co/docs/peft/index + TRL：https://huggingface.co/docs/trl/index 进行学习\n",
        "- gradient_accumulation: https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation"
      ],
      "metadata": {
        "id": "RtZwG9GiCrtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安装依赖"
      ],
      "metadata": {
        "id": "547SnlTHLLyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2XAEW8P7CGiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2acf9f-488f-492f-9daf-50d52dbac921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.12.4.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.13)\n",
            "Collecting pynvml (from deepspeed)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.12.4-py3-none-any.whl size=1290638 sha256=41bf5a9eabee25ccb50331e300c4fd26d919ee0373a29cb86ce6c3beb7f847d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/0c/52/f464610477b069120f740202a9d84a27f9d7235cbf035c4b75\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: ninja, hjson, pynvml, deepspeed\n",
            "Successfully installed deepspeed-0.12.4 hjson-3.1.0 ninja-1.11.1.1 pynvml-11.5.0\n"
          ]
        }
      ],
      "source": [
        "# gpu SM arch 8.0+ (Ampere+), A100\n",
        "# use V100 is ok\n",
        "!pip install deepspeed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ds_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3SSoMfzXBws",
        "outputId": "11dd83cb-1f50-4f0b-ad0c-4e57612a4c69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 02:49:13,067] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "async_io ............... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "fused_adam ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_lion ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "evoformer_attn ......... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "fused_lamb ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lion ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "inference_core_ops ..... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cutlass_ops ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "ragged_device_ops ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "ragged_ops ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/usr/local/lib/python3.10/dist-packages/torch']\n",
            "torch version .................... 2.1.0+cu118\n",
            "deepspeed install path ........... ['/usr/local/lib/python3.10/dist-packages/deepspeed']\n",
            "deepspeed info ................... 0.12.4, unknown, unknown\n",
            "torch cuda version ............... 11.8\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.1, cuda 11.8\n",
            "shared memory (/dev/shm) size .... 40.75 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!deepspeed -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjI56g2LgjYQ",
        "outputId": "e84999e1-1698-424d-f2f0-43f488ddf403"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-12 14:50:53,792] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "usage: deepspeed [-h] [-H HOSTFILE] [-i INCLUDE] [-e EXCLUDE] [--num_nodes NUM_NODES]\n",
            "                 [--min_elastic_nodes MIN_ELASTIC_NODES] [--max_elastic_nodes MAX_ELASTIC_NODES]\n",
            "                 [--num_gpus NUM_GPUS] [--master_port MASTER_PORT] [--master_addr MASTER_ADDR]\n",
            "                 [--launcher LAUNCHER] [--launcher_args LAUNCHER_ARGS] [--module] [--no_python]\n",
            "                 [--no_local_rank] [--no_ssh_check] [--force_multi] [--save_pid]\n",
            "                 [--enable_each_rank_log ENABLE_EACH_RANK_LOG] [--autotuning {tune,run}]\n",
            "                 [--elastic_training] [--bind_cores_to_rank] [--bind_core_list BIND_CORE_LIST]\n",
            "                 [--ssh_port SSH_PORT]\n",
            "                 user_script ...\n",
            "\n",
            "DeepSpeed runner to help launch distributed multi-node/multi-gpu training jobs.\n",
            "\n",
            "positional arguments:\n",
            "  user_script           User script to launch, followed by any required arguments.\n",
            "  user_args\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -H HOSTFILE, --hostfile HOSTFILE\n",
            "                        Hostfile path (in MPI style) that defines the resource pool available to\n",
            "                        the job (e.g., worker-0 slots=4) (default: /job/hostfile)\n",
            "  -i INCLUDE, --include INCLUDE\n",
            "                        Specify hardware resources to use during execution. String format is\n",
            "                        NODE_SPEC[@NODE_SPEC ...], where NODE_SPEC=NAME[:SLOT[,SLOT ...]]. If\n",
            "                        :SLOT is omitted, include all slots on that host. Example: -i\n",
            "                        \"worker-0@worker-1:0,2\" will use all slots on worker-0 and slots [0, 2] on\n",
            "                        worker-1. (default: )\n",
            "  -e EXCLUDE, --exclude EXCLUDE\n",
            "                        Specify hardware resources to NOT use during execution. Mutually exclusive\n",
            "                        with --include. Resource formatting is the same as --include. Example: -e\n",
            "                        \"worker-1:0\" will use all available resources except slot 0 on worker-1.\n",
            "                        (default: )\n",
            "  --num_nodes NUM_NODES\n",
            "                        Total number of worker nodes to run on, this will use the top N hosts from\n",
            "                        the given hostfile. (default: -1)\n",
            "  --min_elastic_nodes MIN_ELASTIC_NODES\n",
            "                        Minimum number of nodes to run elastic training on. Default is 1 when\n",
            "                        elastic training is enabled (default: -1)\n",
            "  --max_elastic_nodes MAX_ELASTIC_NODES\n",
            "                        Maximum number of nodes to run elastic training on. Default is num_nodes\n",
            "                        when elastic training is enabled (default: -1)\n",
            "  --num_gpus NUM_GPUS, --num_accelerators NUM_GPUS\n",
            "                        Max number of GPUs to use on each node, will use [0:N) GPU ids on each\n",
            "                        node. (default: -1)\n",
            "  --master_port MASTER_PORT\n",
            "                        (optional) Port used by PyTorch distributed for communication during\n",
            "                        training. (default: 29500)\n",
            "  --master_addr MASTER_ADDR\n",
            "                        (optional) IP address of node 0, will be inferred via 'hostname -I' if not\n",
            "                        specified. (default: )\n",
            "  --launcher LAUNCHER   (optional) choose launcher backend for multi-node training. Options\n",
            "                        currently include PDSH, OpenMPI, MVAPICH, SLURM, MPICH, IMPI. (default:\n",
            "                        pdsh)\n",
            "  --launcher_args LAUNCHER_ARGS\n",
            "                        (optional) pass launcher specific arguments as a single quoted argument.\n",
            "                        (default: )\n",
            "  --module              Change each process to interpret the launch script as a Python module,\n",
            "                        executing with the same behavior as 'python -m'. (default: False)\n",
            "  --no_python           Skip prepending the training script with 'python' - just execute it\n",
            "                        directly. (default: False)\n",
            "  --no_local_rank       Do not pass local_rank as an argument when calling the user's training\n",
            "                        script. (default: False)\n",
            "  --no_ssh_check        Do not perform ssh check in multi-node launcher model (default: False)\n",
            "  --force_multi         Force multi-node launcher mode, helps in cases where user wants to launch\n",
            "                        on single remote node. (default: False)\n",
            "  --save_pid            Save file containing launcher process id (pid) at /tmp/<main-pid>.ds,\n",
            "                        where <main-pid> is the pid of the first process that invoked `deepspeed`.\n",
            "                        Useful when launching deepspeed processes programmatically. (default:\n",
            "                        False)\n",
            "  --enable_each_rank_log ENABLE_EACH_RANK_LOG\n",
            "                        redirect the stdout and stderr from each rank into different log files\n",
            "                        (default: None)\n",
            "  --autotuning {tune,run}\n",
            "                        Run DeepSpeed autotuner to discover optimal configuration parameters\n",
            "                        before running job. (default: )\n",
            "  --elastic_training    Enable elastic training support in DeepSpeed. (default: False)\n",
            "  --bind_cores_to_rank  Bind each rank to different cores of the host (default: False)\n",
            "  --bind_core_list BIND_CORE_LIST\n",
            "                        List of cores to bind to with comma separated list of numbers and range.\n",
            "                        i.e. 1,3-5,7 => [1,3,4,5,7]. When not specified, all cores on system would\n",
            "                        be used rank binding (default: None)\n",
            "  --ssh_port SSH_PORT   SSH port to use for remote connections (default: None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/DeepSpeedExamples.git\n"
      ],
      "metadata": {
        "id": "hGn0aGAmDgHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6257ff-7ba0-4442-f3ea-d3869a7e75f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepSpeedExamples'...\n",
            "remote: Enumerating objects: 9729, done.\u001b[K\n",
            "remote: Counting objects: 100% (3128/3128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (888/888), done.\u001b[K\n",
            "remote: Total 9729 (delta 2361), reused 2767 (delta 2165), pack-reused 6601\u001b[K\n",
            "Receiving objects: 100% (9729/9729), 119.28 MiB | 14.76 MiB/s, done.\n",
            "Resolving deltas: 100% (5453/5453), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/ && pip install -r requirements.txt && pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLtBP_iPVHYi",
        "outputId": "3323c0ce-b120-458c-db1d-d9b738b1f551"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets>=2.8.0 (from -r requirements.txt (line 1))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece>=0.1.97 (from -r requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.20.3)\n",
            "Collecting accelerate>=0.15.0 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.1.0+cu118)\n",
            "Requirement already satisfied: deepspeed>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.12.4)\n",
            "Requirement already satisfied: transformers!=4.33.2,>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.35.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.8.0->-r requirements.txt (line 1))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.8.0->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.8.0->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r requirements.txt (line 5)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r requirements.txt (line 5)) (2.1.0)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.0->-r requirements.txt (line 6)) (1.11.1.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.0->-r requirements.txt (line 6)) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.0->-r requirements.txt (line 6)) (1.10.13)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.0->-r requirements.txt (line 6)) (11.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.33.2,>=4.31.0->-r requirements.txt (line 7)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.33.2,>=4.31.0->-r requirements.txt (line 7)) (0.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.5.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->-r requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->-r requirements.txt (line 1)) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.8.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.8.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 8)) (3.2.2)\n",
            "Installing collected packages: sentencepiece, pyarrow-hotfix, dill, multiprocess, accelerate, datasets\n",
            "Successfully installed accelerate-0.25.0 datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6 sentencepiece-0.1.99\n",
            "Obtaining file:///content/DeepSpeedExamples/applications/DeepSpeed-Chat\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (2.15.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (0.1.99)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (3.20.3)\n",
            "Requirement already satisfied: accelerate>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (0.25.0)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: deepspeed>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (0.12.4)\n",
            "Requirement already satisfied: transformers!=4.33.2,>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (4.35.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from deepspeed-chat==0.1) (2.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->deepspeed-chat==0.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->deepspeed-chat==0.1) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->deepspeed-chat==0.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->deepspeed-chat==0.1) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->deepspeed-chat==0.1) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.15.0->deepspeed-chat==0.1) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->deepspeed-chat==0.1) (3.9.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.2->deepspeed-chat==0.1) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.2->deepspeed-chat==0.1) (1.11.1.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.2->deepspeed-chat==0.1) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.2->deepspeed-chat==0.1) (1.10.13)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.2->deepspeed-chat==0.1) (11.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->deepspeed-chat==0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->deepspeed-chat==0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->deepspeed-chat==0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->deepspeed-chat==0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->deepspeed-chat==0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->deepspeed-chat==0.1) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.33.2,>=4.31.0->deepspeed-chat==0.1) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.33.2,>=4.31.0->deepspeed-chat==0.1) (0.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (3.5.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deepspeed-chat==0.1) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->deepspeed-chat==0.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->deepspeed-chat==0.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->deepspeed-chat==0.1) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->deepspeed-chat==0.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->deepspeed-chat==0.1) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->deepspeed-chat==0.1) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deepspeed-chat==0.1) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deepspeed-chat==0.1) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deepspeed-chat==0.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->deepspeed-chat==0.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->deepspeed-chat==0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->deepspeed-chat==0.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->deepspeed-chat==0.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->deepspeed-chat==0.1) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->deepspeed-chat==0.1) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.8.0->deepspeed-chat==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.8.0->deepspeed-chat==0.1) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->deepspeed-chat==0.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->deepspeed-chat==0.1) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->deepspeed-chat==0.1) (3.2.2)\n",
            "Installing collected packages: deepspeed-chat\n",
            "  Running setup.py develop for deepspeed-chat\n",
            "Successfully installed deepspeed-chat-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据集\n",
        "\n",
        "[Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)\n",
        "\n",
        "[Dahoas/full-hh-rlhf](https://huggingface.co/datasets/Dahoas/full-hh-rlhf)\n",
        "\n",
        "[Dahoas/synthetic-instruct-gptj-pairwise](https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise)\n",
        "\n",
        "[rlhf-reward-datasets](https://huggingface.co/datasets/rlhf-reward-datasets)\n",
        "\n",
        "[openai/webgpt_comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons)\n",
        "\n",
        "[stanfordnlp/SHP](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
        "\n",
        "其他自定义的数据，如果用于公开研究目的，可以上传至huggingface，通过DataLoader去加载数据；\n",
        "如果是公司内部专有数据，有构建数据湖的能力，可以直接通过s3协议加载，\n"
      ],
      "metadata": {
        "id": "LCGWdxisgRbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trainning 训练"
      ],
      "metadata": {
        "id": "6BFQ4vhXP9wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "值得注意的是，训练大型语言模型 (LLM) 和基于人类反馈的强化学习 (RLHF) 仍然是存在许多未知数的悬而未决的问题。DeepSpeed-Chat旨在提供端到端的RLHF训练管道以及高效、快速的系统支持，而不是RLHF训练的全面解决方案。由于这个领域相对较新，对于用户和开发人员来说都存在各种未知因素。\n",
        "\n",
        "\n",
        "\n",
        "以下示例展示了如何使用一个脚本，以预训练的 OPT-1.3B 作为 actor 模型，OPT-350M 作为 reward 模型，生成一个最终的 13 亿参数的 ChatGPT 类型的模型：\n"
      ],
      "metadata": {
        "id": "wSNcAJvERRQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 单卡gpu\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/ \\\n",
        "  && python e2e_rlhf.py --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type single_gpu\n",
        "\n",
        "# v100-16G or A100-40G OOM\n",
        "# https://github.com/microsoft/DeepSpeedExamples/issues/271"
      ],
      "metadata": {
        "id": "E0O0dJy0P_q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b38b16a-e38b-4412-e718-7874dba2222d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---=== Running Step 1 ===---\n",
            "Running:\n",
            "bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 196, in main\n",
            "    launch_cmd(args, step_num, cmd)\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 176, in launch_cmd\n",
            "    raise RuntimeError('\\n\\n'.join((\n",
            "RuntimeError: Step 1 exited with non-zero status 1\n",
            "\n",
            "Launch command: bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b \n",
            "\n",
            "Log output: /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b/training.log\n",
            "\n",
            "Please see our tutorial at https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning\n",
            "\n",
            "Please check that you have installed our requirements: `pip install -r requirements.txt`\n",
            "\n",
            "If you are seeing an OOM error, try modifying /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh:\n",
            "\n",
            "  - Reduce `--per_device_*_batch_size`\n",
            "\n",
            "  - Increase `--zero_stage {0,1,2,3}` on multi-gpu setups\n",
            "\n",
            "  - Enable `--gradient_checkpointing` or `--only_optimize_lora`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 单卡gpu\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/ \\\n",
        "  && python e2e_rlhf.py --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type single_gpu"
      ],
      "metadata": {
        "id": "f7xE2u9leu-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143da43c-08c0-4d31-d460-731e39c2228a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---=== Running Step 1 ===---\n",
            "Running:\n",
            "bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b \n",
            "---=== Finished Step 1 in 0:51:24 ===---\n",
            "---=== Running Step 2 ===---\n",
            "Running:\n",
            "bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/opt/single_gpu/run_350m.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 196, in main\n",
            "    launch_cmd(args, step_num, cmd)\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 176, in launch_cmd\n",
            "    raise RuntimeError('\\n\\n'.join((\n",
            "RuntimeError: Step 2 exited with non-zero status 1\n",
            "\n",
            "Launch command: bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/opt/single_gpu/run_350m.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m \n",
            "\n",
            "Log output: /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m/training.log\n",
            "\n",
            "Please see our tutorial at https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning\n",
            "\n",
            "Please check that you have installed our requirements: `pip install -r requirements.txt`\n",
            "\n",
            "If you are seeing an OOM error, try modifying /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/opt/single_gpu/run_350m.sh:\n",
            "\n",
            "  - Reduce `--per_device_*_batch_size`\n",
            "\n",
            "  - Increase `--zero_stage {0,1,2,3}` on multi-gpu setups\n",
            "\n",
            "  - Enable `--gradient_checkpointing` or `--only_optimize_lora`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b\n",
        "!head DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b/merges.txt\n",
        "!cat DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b/config.json\n",
        "!head DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b/vocab.json\n"
      ],
      "metadata": {
        "id": "GP64gKtrkk_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT actor-model is ok; skip step 1\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/ \\\n",
        "  && python e2e_rlhf.py --step 2 3 --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type single_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtgVB-eNW7BB",
        "outputId": "4b0aeb24-461b-4092-cf5a-9357a82303bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---=== Running Step 2 ===---\n",
            "Running:\n",
            "bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/opt/single_gpu/run_350m.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m \n",
            "---=== Finished Step 2 in 0:23:58 ===---\n",
            "---=== Running Step 3 ===---\n",
            "Running:\n",
            "bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m '' '' /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 211, in <module>\n",
            "    main(args)\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 196, in main\n",
            "    launch_cmd(args, step_num, cmd)\n",
            "  File \"/content/DeepSpeedExamples/applications/DeepSpeed-Chat/e2e_rlhf.py\", line 176, in launch_cmd\n",
            "    raise RuntimeError('\\n\\n'.join((\n",
            "RuntimeError: Step 3 exited with non-zero status 2\n",
            "\n",
            "Launch command: bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m '' '' /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b\n",
            "\n",
            "Log output: /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b/training.log\n",
            "\n",
            "Please see our tutorial at https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning\n",
            "\n",
            "Please check that you have installed our requirements: `pip install -r requirements.txt`\n",
            "\n",
            "If you are seeing an OOM error, try modifying /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh:\n",
            "\n",
            "  - Reduce `--per_device_*_batch_size`\n",
            "\n",
            "  - Increase `--zero_stage {0,1,2,3}` on multi-gpu setups\n",
            "\n",
            "  - Enable `--gradient_checkpointing` or `--only_optimize_lora`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m\n",
        "!head DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m/merges.txt\n",
        "!cat DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m/config.json\n",
        "!head DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m/vocab.json\n"
      ],
      "metadata": {
        "id": "Wx67Kr4GlHwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT actor-model and RM reward-model are ok; skip step 1,2\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/ \\\n",
        "  && python e2e_rlhf.py --step 3 --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type single_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpJZvIRLd5zl",
        "outputId": "d71d7305-234f-4749-8576-82b76ea5101b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---=== Running Step 3 ===---\n",
            "Running:\n",
            "bash /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m '' '' /content/DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b\n",
        "!head DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b/merges.txt\n",
        "!cat DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b/config.json\n",
        "!head DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b/vocab.json\n"
      ],
      "metadata": {
        "id": "TEY45BHglUaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tips：\n",
        "- 以上训练需要调整 batch_size to device 以防OOM, 可以监控内存使用率进行调整到gpu利用率最高的状态"
      ],
      "metadata": {
        "id": "ULXgZWPxgJUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练记录\n"
      ],
      "metadata": {
        "id": "Wn4ZEaktSzI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### step1_supervised_finetuning\n",
        "\n",
        "```\n",
        "/content# cat /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh\n",
        "#!/bin/bash\n",
        "# Copyright (c) Microsoft Corporation.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "# DeepSpeed Team\n",
        "\n",
        "# Note that usually LoRA needs to use larger learning rate\n",
        "OUTPUT=$1\n",
        "ZERO_STAGE=$2\n",
        "if [ \"$OUTPUT\" == \"\" ]; then\n",
        "    OUTPUT=./output\n",
        "fi\n",
        "if [ \"$ZERO_STAGE\" == \"\" ]; then\n",
        "    ZERO_STAGE=0\n",
        "fi\n",
        "mkdir -p $OUTPUT\n",
        "\n",
        "# u can change num_train_epochs default 1\n",
        "deepspeed --num_gpus 1 main.py --model_name_or_path facebook/opt-1.3b \\\n",
        "   --per_device_train_batch_size 2 \\\n",
        "   --per_device_eval_batch_size 2 \\\n",
        "   --max_seq_len 512 \\\n",
        "   --learning_rate 9.65e-6 \\\n",
        "   --weight_decay 0.1 \\\n",
        "   --num_train_epochs 2 \\\n",
        "   --gradient_accumulation_steps 1 \\\n",
        "   --lr_scheduler_type cosine \\\n",
        "   --num_warmup_steps 0 \\\n",
        "   --seed 1234 \\\n",
        "   --zero_stage $ZERO_STAGE \\\n",
        "   --deepspeed \\\n",
        "   --only_optimize_lora \\\n",
        "   --output_dir $OUTPUT &> $OUTPUT/training.log\n",
        "\n",
        "#   --gradient_accumulation_steps 8 --lora_dim 128 --zero_stage $ZERO_STAGE \\\n",
        "#   --enable_tensorboard \\\n",
        "#   --tensorboard_path $OUTPUT \\\n",
        "```\n",
        "\n",
        "```\n",
        "/content# nvidia-smi --query-gpu=timestamp,memory.total,memory.free,memory.used,name,utilization.gpu,utilization.memory --format=csv -l 3\n",
        "timestamp, memory.total [MiB], memory.free [MiB], memory.used [MiB], name, utilization.gpu [%], utilization.memory [%]\n",
        "2023/12/13 03:26:17.335, 40960 MiB, 3244 MiB, 37269 MiB, NVIDIA A100-SXM4-40GB, 90 %, 65 %\n",
        "2023/12/13 03:26:20.336, 40960 MiB, 3244 MiB, 37269 MiB, NVIDIA A100-SXM4-40GB, 90 %, 66 %\n",
        "2023/12/13 03:26:23.337, 40960 MiB, 3244 MiB, 37269 MiB, NVIDIA A100-SXM4-40GB, 88 %, 63 %\n",
        "2023/12/13 03:26:26.338, 40960 MiB, 3244 MiB, 37269 MiB, NVIDIA A100-SXM4-40GB, 90 %, 66 %\n",
        "2023/12/13 03:26:29.340, 40960 MiB, 3244 MiB, 37269 MiB, NVIDIA A100-SXM4-40GB, 90 %, 64 %\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "61FzfS0xQIaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step2_reward_model_finetuning\n",
        "\n",
        "```\n",
        "/content# cat /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/opt/single_gpu/run_350m.sh\n",
        "#!/bin/bash\n",
        "# Copyright (c) Microsoft Corporation.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "# DeepSpeed Team\n",
        "OUTPUT=$1\n",
        "ZERO_STAGE=$2\n",
        "if [ \"$OUTPUT\" == \"\" ]; then\n",
        "    OUTPUT=./output\n",
        "fi\n",
        "if [ \"$ZERO_STAGE\" == \"\" ]; then\n",
        "    ZERO_STAGE=0\n",
        "fi\n",
        "mkdir -p $OUTPUT\n",
        "\n",
        "deepspeed --num_gpus 1 main.py --model_name_or_path facebook/opt-350m \\\n",
        "   --per_device_train_batch_size 2 \\\n",
        "   --per_device_eval_batch_size 2 \\\n",
        "   --only_optimize_lora \\\n",
        "   --num_padding_at_beginning 1 \\\n",
        "   --weight_decay 0.1 \\\n",
        "   --dropout 0.0 \\\n",
        "   --gradient_accumulation_steps 4 --zero_stage $ZERO_STAGE \\\n",
        "   --enable_tensorboard \\\n",
        "   --tensorboard_path $OUTPUT \\\n",
        "   --deepspeed --output_dir $OUTPUT &> $OUTPUT/training.log\n",
        "```\n",
        "\n",
        "```\n",
        "/content# nvidia-smi --query-gpu=timestamp,memory.total,memory.free,memory.used,name,utilization.gpu,utilization.memory --format=csv -l 3\n",
        "timestamp, memory.total [MiB], memory.free [MiB], memory.used [MiB], name, utilization.gpu [%], utilization.memory [%]\n",
        "2023/12/13 04:11:43.910, 40960 MiB, 29262 MiB, 11251 MiB, NVIDIA A100-SXM4-40GB, 82 %, 46 %\n",
        "2023/12/13 04:11:46.910, 40960 MiB, 29262 MiB, 11251 MiB, NVIDIA A100-SXM4-40GB, 83 %, 46 %\n",
        "2023/12/13 04:11:49.911, 40960 MiB, 29262 MiB, 11251 MiB, NVIDIA A100-SXM4-40GB, 86 %, 48 %\n",
        "2023/12/13 04:11:52.913, 40960 MiB, 29262 MiB, 11251 MiB, NVIDIA A100-SXM4-40GB, 81 %, 46 %\n",
        "```\n",
        "\n",
        "```\n",
        "/content# tail -f DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m/training.log\n",
        "[2023-12-13 04:31:49,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=29, lr=[1.4959651352601607e-06, 1.4959651352601607e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:31:49,421] [INFO] [timer.py:260:stop] epoch=0/micro_step=13680/global_step=3420, RunningAvgSamplesPerSec=22.258547892879996, CurrSamplesPerSec=22.2247897865576, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:31:53,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=29, lr=[1.4265805656755864e-06, 1.4265805656755864e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:31:53,045] [INFO] [timer.py:260:stop] epoch=0/micro_step=13720/global_step=3430, RunningAvgSamplesPerSec=22.258493057977653, CurrSamplesPerSec=22.348077786347293, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:31:56,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=29, lr=[1.3587962402902082e-06, 1.3587962402902082e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:31:56,653] [INFO] [timer.py:260:stop] epoch=0/micro_step=13760/global_step=3440, RunningAvgSamplesPerSec=22.2586868350505, CurrSamplesPerSec=22.411875188689073, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:32:00,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=29, lr=[1.29261676053547e-06, 1.29261676053547e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:32:00,268] [INFO] [timer.py:260:stop] epoch=0/micro_step=13800/global_step=3450, RunningAvgSamplesPerSec=22.258773386410347, CurrSamplesPerSec=22.11042815422408, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:32:03,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=29, lr=[1.228046618900422e-06, 1.228046618900422e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:32:03,939] [INFO] [timer.py:260:stop] epoch=0/micro_step=13840/global_step=3460, RunningAvgSamplesPerSec=22.25792098458994, CurrSamplesPerSec=22.08814474216896, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:32:07,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=29, lr=[1.1650901986267365e-06, 1.1650901986267365e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:32:07,564] [INFO] [timer.py:260:stop] epoch=0/micro_step=13880/global_step=3470, RunningAvgSamplesPerSec=22.257832439582216, CurrSamplesPerSec=22.273370674193252, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:32:11,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=29, lr=[1.1037517734111851e-06, 1.1037517734111851e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:32:11,186] [INFO] [timer.py:260:stop] epoch=0/micro_step=13920/global_step=3480, RunningAvgSamplesPerSec=22.25781006707543, CurrSamplesPerSec=22.34028109749196, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:32:14,091] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 100 iterations\n",
        "[2023-12-13 04:32:14,091] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0\n",
        "[2023-12-13 04:32:14,436] [INFO] [fused_optimizer.py:344:_update_scale]\n",
        "Grad overflow on iteration 3488\n",
        "[2023-12-13 04:32:14,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
        "[2023-12-13 04:32:14,436] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0\n",
        "[2023-12-13 04:32:14,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=30, lr=[1.0499340201976543e-06, 1.0499340201976543e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:32:14,802] [INFO] [timer.py:260:stop] epoch=0/micro_step=13960/global_step=3490, RunningAvgSamplesPerSec=22.25792125305279, CurrSamplesPerSec=22.130596141276783, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "[2023-12-13 04:32:18,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=30, lr=[9.916811660250824e-07, 9.916811660250824e-07], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 04:32:18,427] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=3500, RunningAvgSamplesPerSec=22.257833648158577, CurrSamplesPerSec=22.329458081397593, MemAllocated=4.34GB, MaxMemAllocated=9.18GB\n",
        "```\n",
        "\n",
        "```\n",
        "tail -f DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b/training.log\n",
        "\n",
        "[2023-12-13 03:30:35,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=8140, skipped=80, lr=[4.394243808375539e-06, 4.394243808375539e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 03:30:35,572] [INFO] [timer.py:260:stop] epoch=1/micro_step=514/global_step=8140, RunningAvgSamplesPerSec=10.306346983794935, CurrSamplesPerSec=10.236351300561811, MemAllocated=17.27GB, MaxMemAllocated=29.52GB\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.85, Samples/sec: 10.20, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.99, Samples/sec: 10.23, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 42.01, Samples/sec: 10.24, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.98, Samples/sec: 10.23, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 42.06, Samples/sec: 10.25, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.96, Samples/sec: 10.23, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 42.02, Samples/sec: 10.24, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.19s, TFLOPs: 42.08, Samples/sec: 10.26, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.97, Samples/sec: 10.23, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 42.01, Samples/sec: 10.24, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "[2023-12-13 03:30:37,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=8150, skipped=80, lr=[4.384345924461392e-06, 4.384345924461392e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
        "[2023-12-13 03:30:37,544] [INFO] [timer.py:260:stop] epoch=1/micro_step=524/global_step=8150, RunningAvgSamplesPerSec=10.306265178726324, CurrSamplesPerSec=10.151742299006925, MemAllocated=17.27GB, MaxMemAllocated=29.52GB\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.49, Samples/sec: 10.11, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.99, Samples/sec: 10.23, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 42.03, Samples/sec: 10.24, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 42.05, Samples/sec: 10.25, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.95, Samples/sec: 10.22, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.92, Samples/sec: 10.22, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.20s, TFLOPs: 41.93, Samples/sec: 10.22, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "Model Parameters: 1.316 B, Latency: 0.19s, TFLOPs: 42.19, Samples/sec: 10.28, Time/seq 0.10s, Batch Size: 2, Sequence Length: 512\n",
        "```"
      ],
      "metadata": {
        "id": "XDKBqNZxXt3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step3_rlhf_finetuning\n",
        "\n",
        "```\n",
        "/contentcat /content/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/opt/single_gpu/run_1.3b.sh\n",
        "#!/bin/bash\n",
        "# Copyright (c) Microsoft Corporation.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "# DeepSpeed Team\n",
        "ACTOR_MODEL_PATH=$1\n",
        "CRITIC_MODEL_PATH=$2\n",
        "ACTOR_ZERO_STAGE=$3\n",
        "CRITIC_ZERO_STAGE=$4\n",
        "OUTPUT=$5\n",
        "if [ \"$OUTPUT\" == \"\" ]; then\n",
        "    OUTPUT=./output\n",
        "fi\n",
        "if [ \"$ACTOR_ZERO_STAGE\" == \"\" ]; then\n",
        "    ACTOR_ZERO_STAGE=0\n",
        "fi\n",
        "if [ \"$CRITIC_ZERO_STAGE\" == \"\" ]; then\n",
        "    CRITIC_ZERO_STAGE=0\n",
        "fi\n",
        "mkdir -p $OUTPUT\n",
        "\n",
        "deepspeed --num_gpus 1 main.py \\\n",
        "   --actor_model_name_or_path $ACTOR_MODEL_PATH --critic_model_name_or_path $CRITIC_MODEL_PATH \\\n",
        "   --actor_zero_stage $ACTOR_ZERO_STAGE --critic_zero_stage $CRITIC_ZERO_STAGE \\\n",
        "   --per_device_generation_batch_size 4 \\\n",
        "   --per_device_training_batch_size 4 \\\n",
        "   --only_optimize_lora \\\n",
        "   --num_padding_at_beginning 1 --gradient_accumulation_steps 2 \\\n",
        "   --deepspeed --actor_lora_dim 128 --enable_hybrid_engine --actor_gradient_checkpointing --actor_dropout 0.0 \\\n",
        "   --output_dir $OUTPUT &> $OUTPUT/training.log\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "/content# nvidia-smi --query-gpu=timestamp,memory.total,memory.free,memory.used,name,utilization.gpu,utilization.memory --format=csv -l 3\n",
        "timestamp, memory.total [MiB], memory.free [MiB], memory.used [MiB], name, utilization.gpu [%], utilization.memory [%]\n",
        "2023/12/13 04:49:57.829, 40960 MiB, 19124 MiB, 21389 MiB, NVIDIA A100-SXM4-40GB, 88 %, 52 %\n",
        "2023/12/13 04:50:00.843, 40960 MiB, 19124 MiB, 21389 MiB, NVIDIA A100-SXM4-40GB, 67 %, 38 %\n",
        "2023/12/13 04:50:03.845, 40960 MiB, 19124 MiB, 21389 MiB, NVIDIA A100-SXM4-40GB, 55 %, 31 %\n",
        "2023/12/13 04:50:06.846, 40960 MiB, 19124 MiB, 21389 MiB, NVIDIA A100-SXM4-40GB, 57 %, 32 %\n",
        "2023/12/13 04:50:09.846, 40960 MiB, 19124 MiB, 21389 MiB, NVIDIA A100-SXM4-40GB, 57 %, 32 %\n",
        "2023/12/13 04:50:12.847, 40960 MiB, 19124 MiB, 21389 MiB, NVIDIA A100-SXM4-40GB, 84 %, 47 %\n",
        "```\n",
        "\n",
        "```\n",
        "/content# tail -f DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b/training.log\n",
        "-------------------------------------------------------------------------------------\n",
        "|E2E latency=0.78s |Gather latency=0.00s (0.00%) |Generate time=0.39s (50.18%) |Training time=0.30s (38.80%) |Others=0.09 (11.02%)|CurSamplesPerSec=5.15 |AvgSamplesPerSec=2.48\n",
        "Epoch: 0 | Step: 287 | PPO Epoch: 1 | Actor Loss: 0.06512451171875 | Critic Loss: 0.030548095703125 | Unsupervised Loss: 0.0\n",
        "End-to-End => Latency: 0.78s, TFLOPs: 34.54, Samples/sec: 5.11, Time/seq 0.20s, Batch Size: 4, Total Seq. Length: 512\n",
        "Generation => Latency: 0.47s, Per-token Latency 1.84 ms, TFLOPs: 11.64, BW: 1556.94 GB/sec, Answer Seq. Length: 256\n",
        "Training   => Latency: 0.31s, TFLOPs: 68.88\n",
        "Actor Model Parameters => 1.429 B, Critic Model Parameters => 0.331 B\n",
        "Average reward score: -42.5 | EMA reward score: -41.4414139346459\n",
        "-------------------------------------------------------------------------------------\n",
        "|E2E latency=0.93s |Gather latency=0.00s (0.00%) |Generate time=0.46s (49.93%) |Training time=0.33s (35.98%) |Others=0.13 (14.09%)|CurSamplesPerSec=4.31 |AvgSamplesPerSec=2.48\n",
        "Epoch: 0 | Step: 288 | PPO Epoch: 1 | Actor Loss: 0.0499267578125 | Critic Loss: 0.0175628662109375 | Unsupervised Loss: 0.0\n",
        "End-to-End => Latency: 0.49s, TFLOPs: 55.67, Samples/sec: 8.23, Time/seq 0.12s, Batch Size: 4, Total Seq. Length: 512\n",
        "Generation => Latency: 0.25s, Per-token Latency 0.97 ms, TFLOPs: 22.09, BW: 2954.18 GB/sec, Answer Seq. Length: 256\n",
        "Training   => Latency: 0.24s, TFLOPs: 90.57\n",
        "Actor Model Parameters => 1.429 B, Critic Model Parameters => 0.331 B\n",
        "Average reward score: -41.1875 | EMA reward score: -41.4414139346459\n",
        "-------------------------------------------------------------------------------------\n",
        "|E2E latency=0.63s |Gather latency=0.00s (0.00%) |Generate time=0.24s (38.04%) |Training time=0.31s (48.50%) |Others=0.09 (13.45%)|CurSamplesPerSec=6.32 |AvgSamplesPerSec=2.49\n",
        "Epoch: 0 | Step: 289 | PPO Epoch: 1 | Actor Loss: -0.0202178955078125 | Critic Loss: 0.06353759765625 | Unsupervised Loss: 0.0\n",
        "End-to-End => Latency: 0.60s, TFLOPs: 44.97, Samples/sec: 6.65, Time/seq 0.15s, Batch Size: 4, Total Seq. Length: 512\n",
        "Generation => Latency: 0.28s, Per-token Latency 1.10 ms, TFLOPs: 19.36, BW: 2588.93 GB/sec, Answer Seq. Length: 256\n",
        "Training   => Latency: 0.32s, TFLOPs: 67.66\n",
        "Actor Model Parameters => 1.429 B, Critic Model Parameters => 0.331 B\n",
        "Average reward score: -42.21875 | EMA reward score: -41.467585041181316\n",
        "-------------------------------------------------------------------------------------\n",
        "```"
      ],
      "metadata": {
        "id": "jsTWwgjxgy1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 耗时"
      ],
      "metadata": {
        "id": "P7rIr6tPS3rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "| Model Sizes                      | Step 1 | Step 2 | Step 3 | Total  |\n",
        "|--------------------------------- |:------:|:------:|:------:|:------:|\n",
        "| Actor: OPT-1.3B, Reward: OPT-350M | hr\t| hr | hr | hr |\n",
        "\n",
        "*表 1. 在单卡（V100-16G），针对不同的RLHF步骤， 使用DeepSpeed-Chat训练OPT-1.3b所需的时间。*\n",
        "\n",
        "\n",
        "| Model Sizes                      | Step 1 | Step 2 | Step 3 | Total  |\n",
        "|--------------------------------- |:------:|:------:|:------:|:------:|\n",
        "| Actor: OPT-1.3B, Reward: OPT-350M | 00:51:24 hr\t| 0:23:58 hr | hr | hr |\n",
        "\n",
        "*表 2. 在单卡（A100-40G），针对不同的RLHF步骤， 使用DeepSpeed-Chat训练OPT-1.3b所需的时间。*\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "- V100: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf\n",
        "- **A100**: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf"
      ],
      "metadata": {
        "id": "QXxxzG3BSJc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 思考\n",
        "1. 这个是单机训练实验，测试开发训练模型使用，属于离线数据训练模型；如果工程规范化，需要引入监控，调度引擎等等，使用沉淀好的开源分布式机器学习平台 [ray](https://github.com/ray-project/ray) 框架, [Ray: A Distributed Framework for Emerging AI Applications](https://arxiv.org/abs/1712.05889)\n",
        "2. 以往大数据时代，已经积累工程化实践的平台底座；现在已经进入算力时代，如何在垂直领域使用好数据和算力，训练/推理加速和模型性能增强，以及端到端的快速构建，降低成本，低碳\n",
        "3. 相关LLM论文很多，学习能力非常重要啦，需要鉴别基础论文，学会在巨人肩膀上思考问题，保持好奇心\n",
        "4. 算力离不开底层硬件和对应库支持，需要掌握好这些工具，算子加速\n",
        "5. 参数调试比较耗时，需要一套方法论 :)"
      ],
      "metadata": {
        "id": "kmCBxl4yYaSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 三个训练步骤\n",
        "为了实现无缝的训练体验，我们遵循 InstructGPT 论文的方法，并在 DeepSpeed-Chat 中整合了一个端到端的训练流程，如图所示:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/assets/image/ppo_trainer.png?raw=true)\n",
        "\n",
        "</div>\n",
        "我们的流程包括三个主要步骤：\n",
        "\n",
        "- 步骤1：监督微调（SFT） —— 使用精选的人类回答来微调预训练的语言模型以应对各种查询；\n",
        "- 步骤2：奖励模型微调 —— 使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立的（通常比 SFT 小的）奖励模型（RW）；\n",
        "- 步骤3：RLHF 训练 —— 利用 Proximal Policy Optimization（PPO）算法，根据 RW 模型的奖励反馈进一步微调 SFT 模型。\n",
        "\n",
        "在步骤3中，我们提供了两个额外的功能，以帮助提高模型质量：\n",
        "\n",
        "- 指数移动平均（EMA） —— 可以选择基于 EMA 的检查点进行最终评估\n",
        "- 混合训练 —— 将预训练目标（即下一个单词预测）与 PPO 目标混合，以防止在像 SQuAD2.0 这样的公开基准测试中的性能损失\n",
        "\n",
        "这两个训练功能，EMA 和混合训练，常常被其他的开源框架所忽略，因为它们并不会妨碍训练的进行。然而，根据 InstructGPT，EMA 通常比传统的最终训练模型提供更好的响应质量，而混合训练可以帮助模型保持预训练基准解决能力。因此，我们为用户提供这些功能，以便充分获得 InstructGPT 中描述的训练体验，并争取更高的模型质量。\n",
        "\n",
        "除了与 InstructGPT 论文高度一致外，我们还提供了一项方便的功能，以支持研究人员和从业者使用多个数据资源训练他们自己的 RLHF 模型：\n",
        "\n",
        "- 数据抽象和混合能力： DeepSpeed-Chat 能够使用多个不同来源的数据集训练模型以获得更好的模型质量。它配备了（1）一个抽象数据集层，以统一不同数据集的格式；以及（2）数据拆分/混合功能，以便多个数据集在 3 个训练阶段中被适当地混合然后拆分。\n",
        "\n"
      ],
      "metadata": {
        "id": "iHyUd-vFSv8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. supervised_finetuning(SFT) 监督微调\n",
        "\n",
        "有监督微调（SFT）确实在大语言模型（LLM）领域取得了重大进展。然而，诸如重复内容生成以及困惑度 (PPL) 分数与生成能力之间不一致等意外行为仍然可能发生。\n",
        "\n",
        "根据我们的测试，有几个术语会影响生成行为：\n",
        "\n",
        "- weight decay：OPT 模型经过权重衰减预训练。此后，微调通常会继承此设置。但是，它可能无法生成所需的模型。特别是，对于我们的 OPT-1.3B 示例，我们禁用了权重衰减。\n",
        "dropout：与上面类似，OPT预训练中使用了dropout。然而，SFT不一定需要它。特别是，对于我们的 OPT-1.3B 示例，我们启用了 dropout。\n",
        "- dataset：使用更多数据通常可以提供更好的模型质量。但如果数据集的来源差异太大，可能会损害性能。对于我们的 OPT-1.3B 示例，我们使用以下四个数据集\n",
        "```\n",
        "Dahoas/rm-static\n",
        "Dahoas/full-hh-rlhf\n",
        "Dahoas/synthetic-instruct-gptj-pairwise\n",
        "yitingxie/rlhf-reward-datasets\n",
        "```\n",
        "- training epochs通常，为了避免过度拟合，如果较小的训练周期可以达到相似的模型质量，我们会选择较小的训练周期而不是较长的训练周期（在这种情况下，我们使用 PPL 作为指标）。然而，与 InstructGPT 指出的类似，我们发现即使由于较长的训练而导致过度拟合，仍然建议使用较长的训练周期以获得更好的生成质量。特别是，对于我们的 OPT-1.3B 示例，我们使用 16 个 epoch，尽管我们发现 1 或 2 个 epoch 训练可以达到相同的 PPL 分数。\n",
        "\n",
        "\n",
        "https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/README.md\n"
      ],
      "metadata": {
        "id": "Cgng2DKnTAOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move into the first step of the pipeline\n",
        "# Run the training script\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/ \\\n",
        "  && bash -x training_scripts/opt/single_gpu/run_1.3b.sh"
      ],
      "metadata": {
        "id": "R6eS0-8LTGN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/ \\\n",
        "  && bash -x evaluation_scripts/run_prompt.sh"
      ],
      "metadata": {
        "id": "rmbOTE0JTQO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. reward_model_finetuning(RM) 奖励模型微调\n",
        "\n",
        "奖励模型（RM）微调确实与SFT类似，主要区别在于：\n",
        "1. 训练数据集不同——RM对同一查询既需要好的响应，也需要坏的响应；\n",
        "2. 训练loss不同——RM需要pairrankingloss作为优化目标。\n",
        "\n",
        "我们为奖励模型提供了两个指标：\n",
        "1. 接受的响应（和不良响应）的奖励分数;\n",
        "2. 准确性，即当接受的响应可以获得比拒绝的响应更高的分数时。\n",
        "\n",
        "有时，我们观察到准确性非常高，但接受答案的平均奖励分数为负，或者拒绝答案的分数与接受答案相似。这会影响第 3 步模型的质量吗？如果我们在第 3 步中使用度量奖励分数增益，这可能不会有任何问题。然而，这个机器学习指标（奖励分数增益/增加）并不能真正反映step-3模型生成的质量。因此，我们还没有明确的答案。\n",
        "\n",
        "在这里，我们分享更多关于我们在探索过程中观察到的情况：\n",
        "\n",
        "- weight decay：对于我们的 OPT-350m 示例，我们启用了 0.1 的权重衰减。\n",
        "- dropout：对于我们的 OPT-350m 示例，我们禁用了 dropout。\n",
        "- dataset：对于我们的 OPT-350m 示例，我们使用以下四个数据集：\n",
        "  - Dahoas/rm-static\n",
        "  - Dahoas/full-hh-rlhf\n",
        "  - Dahoas/synthetic-instruct-gptj-pairwise\n",
        "  - yitingxie/rlhf-reward-datasets\n",
        "- training epochsInstructGPT 建议对模型进行 1 epoch 的微调，因为过度拟合会损害第 3 步的性能。在我们的探索过程中，当我们增加训练周期时，我们没有看到过度拟合行为。但是，请遵循作者的指示。我们将训练纪元设置为 1。\n",
        "\n",
        "此外，我们在这里提供了更多的探索，即使我们没有将它们设置为选项或将它们包含在我们当前的管道中\n",
        "- multiple answers for one prompt在 InstructGPT 中，作者特别提到，对一个提示使用配对的拒绝和接受答案不适合奖励模型训练。因此，InstructGPT 会根据每个提示构建包含 4--9 个答案的数据集。然而，我们没有找到具有此功能的良好数据集。\n",
        "- initialize RM with SFT or Pretrained checkpoint我们对此进行了内部测试，但没有发现准确性或奖励分数有很大差异。此外，在 InstructGPT 中，作者也有相同的发现。但是，我们鼓励用户尝试自己使用。\n",
        "- Reward score calculation我们使用最终令牌（或第一个填充令牌）来获得奖励分数。然而，这可能不是最佳选择。例如，用户可以尝试整个答案的平均分等。\n",
        "- Reward loss objective我们只是使用排名损失作为目标。然而，其他的，比如 MSE，也可以是一个选择。\n",
        "\n",
        "\n",
        "https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/README.md"
      ],
      "metadata": {
        "id": "R7Cjv9LrTjFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move into the second step of the pipeline\n",
        "# Run the training script\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/ \\\n",
        "  && bash -x training_scripts/opt/single_gpu/run_350m.sh"
      ],
      "metadata": {
        "id": "up8ecyNiUZOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/ \\\n",
        "  && bash -x evaluation_scripts/run_eval.sh"
      ],
      "metadata": {
        "id": "OOTaVrH5UgVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. rlhf_finetuning 利用人类反馈进行强化学习\n",
        "\n",
        "RLHF 微调是三步训练中最复杂的一步。与SFT类似，奖励分数并不能真正反映模型生成的质量。此外，我们有时会观察到奖励分数在某个点下降到初始阶段，然后迅速恢复。更糟糕的是，我们还看到训练很容易出现发散。我们在这里分享我们的设置和观察。\n",
        "\n",
        "- weight decay：对于我们的 OPT-1.3B/350m（演员/评论家）示例，我们禁用了两个模型的权重衰减。\n",
        "- dropout：我们禁用了 OPT-1.3B 的 droppout，并启用了 OPT-350m。\n",
        "- dataset：我们使用以下单个数据集：Dahoas/rm-static。\n",
        "- training epochs奖励分数很快就会变得平庸。因此，我们将 OPT-1.3B/350m（演员/评论家）示例的训练纪元设置为 1。然而，更长的训练可能会像 SFT 一样带来更好的模型质量。\n",
        "- ema checkpoint我们观察到 ema 检查点通常可以带来更好的模型生成质量，如 InstructGPT 中所述。\n",
        "- PPO related hyperparametersPPO训练有很多超参数。目前，我们为用户对它们进行了硬编码，但您可能需要根据自己的使用情况进行调整。\n",
        "- mix unsupervised trainingInstructGPT 建议混合 PPO 和无监督训练，以防止模型基准质量的损失。然而，当我们直接应用 Instruct 中的超参数时，模型无法收敛。因此，我们停止对此进行探索。但是，我们鼓励用户对其进行测试并根据自己的使用情况调整超参数。\n",
        "- diverging issue我们发现使用不同的生成训练批量大小 ( --per_device_generation_batch_size) 和 PPO 训练批量大小 ( --per_device_training_batch_size)、超过一个 PPO 训练周期 ( --ppo_epochs) 或超过一个生成批量 ( --generation_batches 1) 是非常不稳定的。这些都指向同一个问题：我们无法在生成实验数据后多次更新参与者模型。因此，在我们所有成功的运行中，我们都设置了per_device_generation_batch_size=per_device_training_batch_size和ppo_epochs=generation_batches=1。这对于标准强化学习训练流程来说是意想不到的，我们尝试了不同的方法来克服这个问题，但都失败了。造成这种不稳定的最可能原因之一是我们发现函数中使用的log_probs和即使在连续两次迭代内也会很快发散，这导致对应的值很大。设置严格的上界可以缓解这个问题，但并不能完全解决收敛问题。old_log_probsactor_loss_fnratio\n",
        "\n",
        "\n",
        "https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/README.md\n",
        "\n",
        "\n",
        "RLHF: Reinforcement Learning with Human Feedback\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/assets/image/ppo_trainer.png?raw=true)\n",
        "\n",
        "</div>\n",
        "\n",
        "在步骤3中，我们提供了两个额外的功能，以帮助提高模型质量：\n",
        "\n",
        "- 指数移动平均（EMA） —— 可以选择基于 EMA 的检查点进行最终评估\n",
        "- 混合训练 —— 将预训练目标（即下一个单词预测）与 PPO 目标混合，以防止在像 SQuAD2.0 这样的公开基准测试中的性能损失\n",
        "\n",
        "这两个训练功能，EMA 和混合训练，常常被其他的开源框架所忽略，因为它们并不会妨碍训练的进行。然而，根据 InstructGPT，EMA 通常比传统的最终训练模型提供更好的响应质量，而混合训练可以帮助模型保持预训练基准解决能力。因此，我们为用户提供这些功能，以便充分获得 InstructGPT 中描述的训练体验，并争取更高的模型质量。\n",
        "\n",
        "作为整个 3 步 InstructGPT 管道中最复杂的一步，DeepSpeed Chat 的混合引擎实现了足够的加速，以避免大量的训练时间（成本）影响。如果已经有了经过微调的参与者和奖励模型检查点ckpt，则只需运行以下脚本即可启用 PPO 训练。\n",
        "\n"
      ],
      "metadata": {
        "id": "1i9mW3KmTvjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move into the final step of the pipeline\n",
        "# Run the training script\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/ \\\n",
        "  && bash -x training_scripts/opt/single_gpu/run_1.3b.sh"
      ],
      "metadata": {
        "id": "5Oe_BzmPUW5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepSpeed Hybrid Engine —— 统一的高效混合引擎，为 RLHF 训练提供动力并进行优化\n",
        "\n",
        "DeepSpeed-Chat流程的前两步与大型模型的常规微调相似，得益于基于[ZeRO的内存管理优化 ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)和DeepSpeed训练中的并行策略灵活组合，实现了规模和速度的提升。然而，流程的第三步在性能方面是最具挑战性的部分。每次迭代都需要高效处理两个阶段：\n",
        "1. 生成回答的推理阶段，为训练提供输入；\n",
        "2. 更新 actor 和 reward 模型权重的训练阶段，以及它们之间的交互和调度。\n",
        "\n",
        "这引入了两个主要困难：\n",
        "1. 内存成本，因为在第三阶段的整个过程中需要运行多个SFT和RW模型；\n",
        "2. 生成回答阶段的速度较慢，如果没有正确加速，将显著拖慢整个第三阶段。\n",
        "\n",
        "此外，我们在第三阶段中添加的两个重要可选功能，包括指数移动平均（EMA）收集和混合训练，将产生额外的内存和训练成本。\n",
        "\n",
        "为了应对这些挑战，我们**将DeepSpeed训练和推理的系统功能整合为一个统一的基础设施，称为混合引擎（Hybrid Engine）**。它利用原始DeepSpeed引擎进行高速训练模式，同时轻松应用DeepSpeed推理引擎进行生成/评估模式，为第三阶段的RLHF训练提供了一个明显更快的训练系统。\n",
        "\n",
        "如图2所示，DeepSpeed训练和推理引擎之间的过渡是无缝的：通过为actor模型启用典型的eval和train模式，当运行推理和训练流程时，DeepSpeed选择其不同的优化来运行模型更快并提高整个系统吞吐量。\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/hybrid-engine.png?raw=true\" width=\"600px\" alt=\"DeepSpeed-Chat!\"/>\n",
        "\n",
        "*Figure 2. 设计图解：DeepSpeed Hybrid Engine，用于加速 RLHF 流程中最耗时的部分。*\n",
        "\n",
        "</div>\n",
        "\n",
        "在RLHF训练的经验生成阶段的推理执行过程中，DeepSpeed混合引擎使用轻量级内存管理系统来处理KV缓存和中间结果，同时使用高度优化的推理CUDA核和张量并行计算。与现有解决方案相比，DeepSpeed-HE显著提高了吞吐量（每秒token数）。\n",
        "\n",
        "在训练执行过程中，混合引擎使用了多种内存优化技术，如DeepSpeed的ZeRO系列技术和现在流行的LoRA方法。这些技术在混合引擎中可以彼此兼容，并可以组合在一起以提供最高训练效率。\n",
        "\n",
        "DeepSpeed-HE可以在训练和推理之间无缝更改模型分区，以支持基于张量并行计算的推理和基于ZeRO的分片机制进行训练。它还会重新配置内存系统以在此期间最大化内存可用性。DeepSpeed-HE还通过规避内存分配瓶颈和支持大批量大小来进一步提高性能。混合引擎集成了DeepSpeed训练和推理的一系列系统技术，突破了现有RLHF训练的极限，并为RLHF工作负载提供了无与伦比的规模和系统效率。\n",
        "\n",
        "参考：\n",
        "- [**ZeRO: Memory Optimizations Toward Training Trillion Parameter Models**](https://arxiv.org/abs/1910.02054)\n",
        "- [**LoRA: Low-Rank Adaptation of Large Language Models**](https://arxiv.org/abs/2106.09685)"
      ],
      "metadata": {
        "id": "glG-zXWyI8Lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# inference 模型推理"
      ],
      "metadata": {
        "id": "mT9YkXAx3mbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# serve the final model\n",
        "!cd DeepSpeedExamples/applications/DeepSpeed-Chat/ && python chat.py --path  ${PATH-to-your-actor-model}\n"
      ],
      "metadata": {
        "id": "Ca3y9yjM3rgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# more\n",
        "\n",
        "RLHF（人类反馈强化学习）训练仍然是一个悬而未决的问题，而 DeepSpeed-Chat 旨在成为研究人员和从业者以高效、快速的训练体验进行研究的起点。混合引擎和其他高效组件（例如 LoRA）可以从 DeepSpeed-Chat 继承，使您能够开发自己的 RLHF 训练管道，用于探索、研究和其他目的。\n",
        "\n"
      ],
      "metadata": {
        "id": "Wbf7YKcclXRJ"
      }
    }
  ]
}