{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO53cgF0+rEY6I5cFlrhrNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/chatglm_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " https://github.com/li-plus/chatglm.cpp.git"
      ],
      "metadata": {
        "id": "ropBuqvZOTzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLa6OBcJM2Fo",
        "outputId": "b40f4ec3-1e39-4a56-df34-913b77a542f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatglm.cpp'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (340/340), done.\u001b[K\n",
            "remote: Compressing objects: 100% (170/170), done.\u001b[K\n",
            "remote: Total 597 (delta 260), reused 201 (delta 169), pack-reused 257\u001b[K\n",
            "Receiving objects: 100% (597/597), 1.44 MiB | 5.05 MiB/s, done.\n",
            "Resolving deltas: 100% (348/348), done.\n",
            "Submodule 'third_party/ggml' (https://github.com/ggerganov/ggml.git) registered for path 'third_party/ggml'\n",
            "Submodule 'third_party/pybind11' (https://github.com/pybind/pybind11.git) registered for path 'third_party/pybind11'\n",
            "Submodule 'third_party/sentencepiece' (https://github.com/google/sentencepiece.git) registered for path 'third_party/sentencepiece'\n",
            "Cloning into '/content/chatglm.cpp/third_party/ggml'...\n",
            "remote: Enumerating objects: 5154, done.        \n",
            "remote: Counting objects: 100% (207/207), done.        \n",
            "remote: Compressing objects: 100% (104/104), done.        \n",
            "remote: Total 5154 (delta 119), reused 148 (delta 92), pack-reused 4947        \n",
            "Receiving objects: 100% (5154/5154), 6.60 MiB | 22.39 MiB/s, done.\n",
            "Resolving deltas: 100% (3077/3077), done.\n",
            "Cloning into '/content/chatglm.cpp/third_party/pybind11'...\n",
            "remote: Enumerating objects: 27476, done.        \n",
            "remote: Counting objects: 100% (227/227), done.        \n",
            "remote: Compressing objects: 100% (100/100), done.        \n",
            "remote: Total 27476 (delta 124), reused 189 (delta 108), pack-reused 27249        \n",
            "Receiving objects: 100% (27476/27476), 10.49 MiB | 14.17 MiB/s, done.\n",
            "Resolving deltas: 100% (19401/19401), done.\n",
            "Cloning into '/content/chatglm.cpp/third_party/sentencepiece'...\n",
            "remote: Enumerating objects: 4841, done.        \n",
            "remote: Counting objects: 100% (1875/1875), done.        \n",
            "remote: Compressing objects: 100% (208/208), done.        \n",
            "remote: Total 4841 (delta 1692), reused 1677 (delta 1666), pack-reused 2966        \n",
            "Receiving objects: 100% (4841/4841), 26.68 MiB | 18.79 MiB/s, done.\n",
            "Resolving deltas: 100% (3345/3345), done.\n",
            "Submodule path 'third_party/ggml': checked out '6549d12f2e3176050040a86334f17c001e170f13'\n",
            "Submodule path 'third_party/pybind11': checked out '8b03ffa7c06cd9c8a38297b1c8923695d1ff1b07'\n",
            "Submodule path 'third_party/sentencepiece': checked out '635fe8423a249b6e081aacd290d8aef7476c6a28'\n",
            "/content/chatglm.cpp\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/li-plus/chatglm.cpp.git\n",
        "%cd chatglm.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch tabulate tqdm transformers accelerate sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY4P8tX9OlVv",
        "outputId": "12c9bfac-5ab8-467e-e55e-216fb955652c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: sentencepiece, accelerate\n",
            "Successfully installed accelerate-0.25.0 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python chatglm_cpp/convert.py -i THUDM/chatglm3-6b -t q4_0 -o chatglm3-6b-ggml.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9-JL43ZOnXJ",
        "outputId": "1a5dc470-df85-4395-88d5-9122252e70eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 244/244 [00:00<00:00, 1.10MB/s]\n",
            "tokenization_chatglm.py: 100% 12.3k/12.3k [00:00<00:00, 40.5MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
            "- tokenization_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "tokenizer.model: 100% 1.02M/1.02M [00:00<00:00, 47.5MB/s]\n",
            "config.json: 100% 1.32k/1.32k [00:00<00:00, 6.91MB/s]\n",
            "configuration_chatglm.py: 100% 2.33k/2.33k [00:00<00:00, 13.9MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
            "- configuration_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "modeling_chatglm.py: 100% 55.7k/55.7k [00:00<00:00, 685kB/s]\n",
            "quantization.py: 100% 14.7k/14.7k [00:00<00:00, 49.2MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
            "- quantization.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
            "- modeling_chatglm.py\n",
            "- quantization.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "pytorch_model.bin.index.json: 100% 20.4k/20.4k [00:00<00:00, 63.1MB/s]\n",
            "Downloading shards:   0% 0/7 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00007.bin:   0% 0.00/1.83G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:   1% 10.5M/1.83G [00:00<00:19, 93.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:   2% 31.5M/1.83G [00:00<00:12, 149MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00007.bin:   4% 73.4M/1.83G [00:00<00:07, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:   6% 115M/1.83G [00:00<00:06, 262MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00007.bin:   8% 147M/1.83G [00:00<00:06, 274MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  10% 189M/1.83G [00:00<00:05, 298MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  12% 220M/1.83G [00:00<00:05, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  14% 252M/1.83G [00:00<00:05, 286MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  16% 294M/1.83G [00:01<00:05, 301MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  18% 325M/1.83G [00:01<00:04, 303MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  20% 367M/1.83G [00:01<00:04, 312MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  22% 409M/1.83G [00:01<00:04, 324MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  25% 451M/1.83G [00:01<00:04, 333MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  27% 493M/1.83G [00:01<00:04, 331MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  29% 535M/1.83G [00:01<00:04, 320MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  32% 577M/1.83G [00:01<00:03, 317MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  34% 619M/1.83G [00:02<00:03, 315MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  36% 661M/1.83G [00:02<00:03, 299MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  38% 692M/1.83G [00:02<00:03, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  40% 724M/1.83G [00:02<00:03, 292MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  41% 755M/1.83G [00:02<00:03, 296MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  43% 786M/1.83G [00:02<00:03, 296MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  45% 818M/1.83G [00:02<00:03, 296MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  46% 849M/1.83G [00:02<00:03, 293MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  48% 881M/1.83G [00:02<00:03, 293MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  50% 912M/1.83G [00:03<00:03, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  52% 944M/1.83G [00:03<00:02, 297MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  53% 975M/1.83G [00:03<00:02, 296MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  55% 1.01G/1.83G [00:03<00:02, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  57% 1.04G/1.83G [00:03<00:02, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  59% 1.07G/1.83G [00:03<00:02, 298MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  61% 1.11G/1.83G [00:03<00:02, 309MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  63% 1.14G/1.83G [00:03<00:02, 310MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  64% 1.17G/1.83G [00:03<00:02, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  67% 1.22G/1.83G [00:04<00:01, 314MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  68% 1.25G/1.83G [00:04<00:02, 285MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  70% 1.28G/1.83G [00:04<00:02, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  72% 1.31G/1.83G [00:04<00:02, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  73% 1.34G/1.83G [00:04<00:01, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  75% 1.37G/1.83G [00:04<00:01, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  77% 1.42G/1.83G [00:04<00:01, 276MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  80% 1.46G/1.83G [00:05<00:01, 295MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  82% 1.50G/1.83G [00:05<00:01, 307MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  84% 1.53G/1.83G [00:05<00:00, 303MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  85% 1.56G/1.83G [00:05<00:00, 299MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  87% 1.59G/1.83G [00:05<00:00, 280MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  89% 1.63G/1.83G [00:05<00:00, 273MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  91% 1.66G/1.83G [00:05<00:00, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  92% 1.69G/1.83G [00:05<00:00, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  94% 1.72G/1.83G [00:06<00:00, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  96% 1.75G/1.83G [00:06<00:00, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin:  98% 1.78G/1.83G [00:06<00:00, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00007.bin: 100% 1.83G/1.83G [00:06<00:00, 282MB/s]\n",
            "Downloading shards:  14% 1/7 [00:06<00:40,  6.73s/it]\n",
            "pytorch_model-00002-of-00007.bin:   0% 0.00/1.97G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:   1% 21.0M/1.97G [00:00<00:11, 175MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:   3% 52.4M/1.97G [00:00<00:09, 205MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:   4% 83.9M/1.97G [00:00<00:08, 217MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:   6% 115M/1.97G [00:00<00:07, 241MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00007.bin:   7% 147M/1.97G [00:00<00:07, 257MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:   9% 178M/1.97G [00:00<00:06, 264MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  11% 210M/1.97G [00:00<00:06, 272MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  12% 241M/1.97G [00:00<00:06, 277MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  14% 273M/1.97G [00:01<00:06, 276MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  15% 304M/1.97G [00:01<00:06, 258MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  17% 336M/1.97G [00:01<00:06, 259MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  19% 367M/1.97G [00:01<00:06, 264MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  20% 398M/1.97G [00:01<00:05, 267MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  22% 430M/1.97G [00:01<00:06, 256MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  23% 461M/1.97G [00:01<00:05, 260MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  25% 493M/1.97G [00:01<00:05, 262MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  27% 524M/1.97G [00:02<00:05, 272MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  28% 556M/1.97G [00:02<00:05, 277MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  30% 587M/1.97G [00:02<00:04, 280MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  31% 619M/1.97G [00:02<00:04, 274MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  33% 650M/1.97G [00:02<00:05, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  35% 682M/1.97G [00:02<00:05, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  36% 713M/1.97G [00:02<00:04, 265MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  38% 744M/1.97G [00:02<00:04, 274MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  39% 776M/1.97G [00:02<00:04, 274MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  41% 807M/1.97G [00:03<00:04, 269MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  43% 839M/1.97G [00:03<00:04, 281MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  44% 870M/1.97G [00:03<00:04, 274MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  46% 912M/1.97G [00:03<00:03, 283MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  48% 944M/1.97G [00:03<00:04, 228MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  50% 975M/1.97G [00:03<00:04, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  51% 1.01G/1.97G [00:03<00:04, 220MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  53% 1.04G/1.97G [00:04<00:04, 230MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  54% 1.07G/1.97G [00:04<00:03, 228MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  56% 1.10G/1.97G [00:04<00:03, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  58% 1.13G/1.97G [00:04<00:03, 213MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  59% 1.16G/1.97G [00:04<00:03, 213MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  61% 1.20G/1.97G [00:04<00:03, 223MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  62% 1.23G/1.97G [00:04<00:03, 235MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  64% 1.26G/1.97G [00:05<00:02, 241MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  66% 1.29G/1.97G [00:05<00:02, 230MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  67% 1.32G/1.97G [00:05<00:02, 236MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  69% 1.35G/1.97G [00:05<00:02, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  70% 1.38G/1.97G [00:05<00:02, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  72% 1.42G/1.97G [00:05<00:02, 251MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  74% 1.45G/1.97G [00:05<00:02, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  75% 1.48G/1.97G [00:05<00:01, 254MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  77% 1.51G/1.97G [00:06<00:01, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  78% 1.54G/1.97G [00:06<00:01, 263MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  80% 1.57G/1.97G [00:06<00:01, 253MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  82% 1.60G/1.97G [00:06<00:01, 261MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  83% 1.64G/1.97G [00:06<00:01, 267MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  85% 1.67G/1.97G [00:06<00:01, 265MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  86% 1.70G/1.97G [00:06<00:00, 276MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  88% 1.73G/1.97G [00:06<00:00, 277MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  89% 1.76G/1.97G [00:06<00:00, 277MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  91% 1.79G/1.97G [00:07<00:00, 278MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  93% 1.82G/1.97G [00:07<00:00, 273MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  94% 1.86G/1.97G [00:07<00:00, 266MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  96% 1.89G/1.97G [00:07<00:00, 252MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin:  97% 1.92G/1.97G [00:07<00:00, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00007.bin: 100% 1.97G/1.97G [00:07<00:00, 253MB/s]\n",
            "Downloading shards:  29% 2/7 [00:14<00:37,  7.49s/it]\n",
            "pytorch_model-00003-of-00007.bin:   0% 0.00/1.93G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:   1% 10.5M/1.93G [00:00<00:30, 63.5MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:   2% 31.5M/1.93G [00:00<00:16, 116MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00007.bin:   3% 52.4M/1.93G [00:00<00:13, 138MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:   4% 83.9M/1.93G [00:00<00:10, 180MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:   6% 115M/1.93G [00:00<00:08, 212MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00007.bin:   8% 147M/1.93G [00:00<00:07, 238MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  10% 189M/1.93G [00:00<00:06, 265MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  11% 220M/1.93G [00:01<00:07, 237MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  13% 252M/1.93G [00:01<00:07, 217MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  15% 283M/1.93G [00:01<00:07, 229MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  16% 315M/1.93G [00:01<00:06, 241MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  18% 346M/1.93G [00:01<00:07, 206MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  20% 377M/1.93G [00:01<00:07, 219MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  21% 409M/1.93G [00:01<00:06, 233MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  23% 440M/1.93G [00:02<00:06, 234MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  24% 472M/1.93G [00:02<00:06, 217MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  26% 503M/1.93G [00:02<00:06, 233MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  28% 535M/1.93G [00:02<00:05, 237MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  29% 566M/1.93G [00:02<00:05, 242MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  31% 598M/1.93G [00:02<00:06, 216MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  33% 629M/1.93G [00:02<00:06, 210MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  34% 661M/1.93G [00:03<00:05, 222MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  36% 692M/1.93G [00:03<00:05, 237MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  38% 724M/1.93G [00:03<00:05, 214MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  39% 755M/1.93G [00:03<00:05, 221MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  41% 786M/1.93G [00:03<00:04, 236MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  42% 818M/1.93G [00:03<00:04, 255MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  44% 849M/1.93G [00:03<00:04, 259MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  46% 881M/1.93G [00:03<00:03, 263MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  47% 912M/1.93G [00:04<00:03, 268MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  49% 944M/1.93G [00:04<00:03, 277MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  51% 975M/1.93G [00:04<00:03, 277MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  52% 1.01G/1.93G [00:04<00:03, 277MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  54% 1.04G/1.93G [00:04<00:03, 278MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  55% 1.07G/1.93G [00:04<00:03, 283MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  57% 1.10G/1.93G [00:04<00:02, 281MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  59% 1.13G/1.93G [00:04<00:02, 283MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  60% 1.16G/1.93G [00:04<00:02, 284MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  62% 1.20G/1.93G [00:05<00:02, 264MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  64% 1.23G/1.93G [00:05<00:02, 268MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  65% 1.26G/1.93G [00:05<00:02, 272MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  67% 1.29G/1.93G [00:05<00:02, 239MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  69% 1.32G/1.93G [00:05<00:02, 252MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  71% 1.36G/1.93G [00:05<00:02, 271MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  72% 1.39G/1.93G [00:05<00:01, 270MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  75% 1.44G/1.93G [00:05<00:01, 289MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  76% 1.47G/1.93G [00:06<00:01, 274MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  78% 1.50G/1.93G [00:06<00:01, 272MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  80% 1.54G/1.93G [00:06<00:01, 289MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  82% 1.57G/1.93G [00:06<00:01, 289MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  84% 1.61G/1.93G [00:06<00:01, 304MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  85% 1.65G/1.93G [00:06<00:00, 299MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  87% 1.68G/1.93G [00:06<00:00, 296MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  89% 1.71G/1.93G [00:06<00:00, 299MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  91% 1.75G/1.93G [00:06<00:00, 313MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  92% 1.78G/1.93G [00:07<00:00, 313MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  94% 1.81G/1.93G [00:07<00:00, 308MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  96% 1.85G/1.93G [00:07<00:00, 302MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin:  97% 1.88G/1.93G [00:07<00:00, 301MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00007.bin: 100% 1.93G/1.93G [00:07<00:00, 254MB/s]\n",
            "Downloading shards:  43% 3/7 [00:22<00:30,  7.63s/it]\n",
            "pytorch_model-00004-of-00007.bin:   0% 0.00/1.82G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:   1% 10.5M/1.82G [00:00<00:41, 43.6MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:   2% 41.9M/1.82G [00:00<00:13, 135MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00007.bin:   4% 73.4M/1.82G [00:00<00:09, 191MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:   6% 105M/1.82G [00:00<00:07, 225MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00007.bin:   8% 136M/1.82G [00:00<00:06, 240MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:   9% 168M/1.82G [00:00<00:07, 227MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  11% 199M/1.82G [00:00<00:06, 246MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  13% 231M/1.82G [00:01<00:06, 255MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  14% 262M/1.82G [00:01<00:05, 264MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  16% 294M/1.82G [00:01<00:05, 273MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  18% 325M/1.82G [00:01<00:05, 274MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  20% 357M/1.82G [00:01<00:05, 275MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  21% 388M/1.82G [00:01<00:05, 279MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  23% 419M/1.82G [00:01<00:04, 283MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  25% 451M/1.82G [00:01<00:04, 286MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  27% 482M/1.82G [00:01<00:04, 285MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  28% 514M/1.82G [00:02<00:04, 282MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  30% 545M/1.82G [00:02<00:04, 281MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  32% 577M/1.82G [00:02<00:04, 272MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  34% 608M/1.82G [00:02<00:04, 257MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  35% 640M/1.82G [00:02<00:04, 260MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  37% 671M/1.82G [00:02<00:04, 266MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  39% 703M/1.82G [00:02<00:04, 273MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  40% 734M/1.82G [00:02<00:04, 258MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  42% 765M/1.82G [00:03<00:04, 219MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  44% 797M/1.82G [00:03<00:04, 217MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  46% 828M/1.82G [00:03<00:04, 231MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  47% 860M/1.82G [00:03<00:03, 244MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  49% 891M/1.82G [00:03<00:03, 255MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  51% 923M/1.82G [00:03<00:03, 255MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  53% 965M/1.82G [00:03<00:03, 280MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  55% 996M/1.82G [00:03<00:02, 287MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  57% 1.04G/1.82G [00:04<00:02, 305MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  59% 1.07G/1.82G [00:04<00:02, 307MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  61% 1.10G/1.82G [00:04<00:02, 308MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  62% 1.13G/1.82G [00:04<00:02, 301MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  64% 1.16G/1.82G [00:04<00:02, 292MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  66% 1.20G/1.82G [00:04<00:02, 297MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  68% 1.23G/1.82G [00:04<00:02, 286MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  69% 1.26G/1.82G [00:04<00:02, 265MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  71% 1.29G/1.82G [00:04<00:01, 265MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  73% 1.32G/1.82G [00:05<00:01, 271MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  75% 1.35G/1.82G [00:05<00:01, 274MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  76% 1.38G/1.82G [00:05<00:01, 271MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  78% 1.42G/1.82G [00:05<00:01, 278MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  80% 1.45G/1.82G [00:05<00:01, 283MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  81% 1.48G/1.82G [00:05<00:01, 268MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  83% 1.51G/1.82G [00:05<00:01, 258MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  85% 1.54G/1.82G [00:05<00:01, 266MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  87% 1.57G/1.82G [00:06<00:00, 267MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  88% 1.60G/1.82G [00:06<00:00, 271MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  90% 1.64G/1.82G [00:06<00:00, 271MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  92% 1.67G/1.82G [00:06<00:00, 275MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  94% 1.70G/1.82G [00:06<00:00, 277MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  95% 1.73G/1.82G [00:06<00:00, 259MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin:  97% 1.76G/1.82G [00:06<00:00, 258MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00007.bin: 100% 1.82G/1.82G [00:06<00:00, 261MB/s]\n",
            "Downloading shards:  57% 4/7 [00:29<00:22,  7.44s/it]\n",
            "pytorch_model-00005-of-00007.bin:   0% 0.00/1.97G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:   1% 10.5M/1.97G [00:00<00:49, 39.8MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:   2% 31.5M/1.97G [00:00<00:23, 83.7MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:   3% 52.4M/1.97G [00:00<00:15, 121MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00007.bin:   4% 83.9M/1.97G [00:00<00:10, 175MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:   6% 126M/1.97G [00:00<00:07, 230MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00007.bin:   9% 168M/1.97G [00:00<00:06, 266MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  11% 210M/1.97G [00:00<00:06, 290MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  13% 252M/1.97G [00:01<00:05, 308MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  15% 294M/1.97G [00:01<00:05, 321MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  17% 336M/1.97G [00:01<00:04, 332MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  19% 377M/1.97G [00:01<00:04, 335MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  21% 419M/1.97G [00:01<00:04, 340MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  23% 461M/1.97G [00:01<00:04, 319MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  26% 503M/1.97G [00:01<00:04, 298MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  27% 535M/1.97G [00:02<00:04, 295MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  29% 566M/1.97G [00:02<00:04, 295MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  30% 598M/1.97G [00:02<00:04, 298MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  32% 640M/1.97G [00:02<00:04, 302MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  34% 671M/1.97G [00:02<00:04, 301MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  36% 703M/1.97G [00:02<00:04, 303MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  37% 734M/1.97G [00:02<00:04, 290MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  39% 765M/1.97G [00:02<00:05, 221MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  40% 797M/1.97G [00:03<00:05, 219MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  42% 828M/1.97G [00:03<00:04, 230MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  44% 860M/1.97G [00:03<00:04, 238MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  45% 891M/1.97G [00:03<00:04, 246MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  47% 933M/1.97G [00:03<00:03, 271MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  50% 975M/1.97G [00:03<00:03, 295MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  52% 1.02G/1.97G [00:03<00:03, 311MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  54% 1.06G/1.97G [00:03<00:02, 321MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  56% 1.10G/1.97G [00:04<00:02, 332MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  58% 1.14G/1.97G [00:04<00:02, 331MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  60% 1.18G/1.97G [00:04<00:02, 303MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  62% 1.22G/1.97G [00:04<00:02, 303MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  63% 1.25G/1.97G [00:04<00:02, 298MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  65% 1.28G/1.97G [00:04<00:02, 297MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  67% 1.32G/1.97G [00:04<00:02, 305MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  69% 1.35G/1.97G [00:04<00:02, 276MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  70% 1.38G/1.97G [00:05<00:02, 284MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  72% 1.42G/1.97G [00:05<00:01, 288MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  74% 1.45G/1.97G [00:05<00:02, 230MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  75% 1.48G/1.97G [00:05<00:02, 208MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  77% 1.51G/1.97G [00:05<00:02, 213MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  78% 1.54G/1.97G [00:05<00:01, 234MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  80% 1.57G/1.97G [00:06<00:02, 173MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  82% 1.60G/1.97G [00:06<00:01, 198MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  83% 1.64G/1.97G [00:06<00:01, 221MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  85% 1.67G/1.97G [00:06<00:01, 241MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  86% 1.70G/1.97G [00:06<00:01, 252MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  88% 1.73G/1.97G [00:06<00:00, 260MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  89% 1.76G/1.97G [00:06<00:00, 273MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  91% 1.79G/1.97G [00:06<00:00, 271MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  93% 1.82G/1.97G [00:06<00:00, 273MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  94% 1.86G/1.97G [00:07<00:00, 281MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  96% 1.89G/1.97G [00:07<00:00, 284MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin:  97% 1.92G/1.97G [00:07<00:00, 289MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00007.bin: 100% 1.97G/1.97G [00:07<00:00, 266MB/s]\n",
            "Downloading shards:  71% 5/7 [00:37<00:15,  7.55s/it]\n",
            "pytorch_model-00006-of-00007.bin:   0% 0.00/1.93G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:   1% 10.5M/1.93G [00:00<00:21, 91.2MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:   2% 31.5M/1.93G [00:00<00:13, 143MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00007.bin:   3% 62.9M/1.93G [00:00<00:09, 204MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:   5% 94.4M/1.93G [00:00<00:07, 240MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:   7% 126M/1.93G [00:00<00:07, 255MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00007.bin:   8% 157M/1.93G [00:00<00:08, 216MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  10% 189M/1.93G [00:00<00:08, 194MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  11% 210M/1.93G [00:01<00:08, 193MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  12% 231M/1.93G [00:01<00:09, 186MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  13% 252M/1.93G [00:01<00:09, 182MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  14% 273M/1.93G [00:01<00:09, 180MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  15% 294M/1.93G [00:01<00:09, 177MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  16% 315M/1.93G [00:01<00:09, 172MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  17% 336M/1.93G [00:01<00:09, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  18% 357M/1.93G [00:01<00:09, 167MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  20% 377M/1.93G [00:02<00:09, 165MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  21% 398M/1.93G [00:02<00:09, 164MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  22% 419M/1.93G [00:02<00:09, 164MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  23% 440M/1.93G [00:02<00:08, 166MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  24% 461M/1.93G [00:02<00:09, 163MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  25% 482M/1.93G [00:02<00:08, 172MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  26% 503M/1.93G [00:02<00:08, 170MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  27% 524M/1.93G [00:02<00:08, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  28% 545M/1.93G [00:03<00:07, 173MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  29% 566M/1.93G [00:03<00:07, 172MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  30% 587M/1.93G [00:03<00:07, 173MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  32% 608M/1.93G [00:03<00:07, 170MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  33% 629M/1.93G [00:03<00:07, 171MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  34% 650M/1.93G [00:03<00:07, 165MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  35% 671M/1.93G [00:03<00:07, 166MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  36% 692M/1.93G [00:03<00:07, 172MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  37% 713M/1.93G [00:04<00:07, 171MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  38% 734M/1.93G [00:04<00:07, 170MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  39% 755M/1.93G [00:04<00:06, 177MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  40% 776M/1.93G [00:04<00:06, 173MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  41% 797M/1.93G [00:04<00:06, 178MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  42% 818M/1.93G [00:04<00:06, 175MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  44% 839M/1.93G [00:04<00:06, 173MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  45% 860M/1.93G [00:04<00:06, 174MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  46% 881M/1.93G [00:05<00:06, 172MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  47% 902M/1.93G [00:05<00:05, 177MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  48% 923M/1.93G [00:05<00:05, 181MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  49% 944M/1.93G [00:05<00:05, 178MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  50% 965M/1.93G [00:05<00:05, 176MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  51% 986M/1.93G [00:05<00:05, 172MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  52% 1.01G/1.93G [00:05<00:05, 176MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  53% 1.03G/1.93G [00:05<00:05, 176MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  54% 1.05G/1.93G [00:05<00:05, 173MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  55% 1.07G/1.93G [00:06<00:05, 171MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  57% 1.09G/1.93G [00:06<00:04, 169MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  58% 1.11G/1.93G [00:06<00:04, 169MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  59% 1.13G/1.93G [00:06<00:04, 172MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  60% 1.15G/1.93G [00:06<00:04, 175MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  61% 1.17G/1.93G [00:06<00:04, 175MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  62% 1.20G/1.93G [00:06<00:04, 176MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  63% 1.22G/1.93G [00:06<00:04, 178MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  64% 1.24G/1.93G [00:07<00:03, 174MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  65% 1.26G/1.93G [00:07<00:03, 174MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  66% 1.28G/1.93G [00:07<00:03, 179MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  67% 1.30G/1.93G [00:07<00:03, 176MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  69% 1.32G/1.93G [00:07<00:03, 169MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  70% 1.34G/1.93G [00:07<00:03, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  71% 1.36G/1.93G [00:07<00:03, 167MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  72% 1.38G/1.93G [00:07<00:03, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  73% 1.41G/1.93G [00:08<00:03, 169MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  74% 1.43G/1.93G [00:08<00:03, 166MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  75% 1.45G/1.93G [00:08<00:02, 162MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  76% 1.47G/1.93G [00:08<00:02, 165MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  77% 1.49G/1.93G [00:08<00:02, 167MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  78% 1.51G/1.93G [00:08<00:02, 167MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  79% 1.53G/1.93G [00:08<00:02, 171MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  81% 1.55G/1.93G [00:08<00:02, 169MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  82% 1.57G/1.93G [00:09<00:02, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  83% 1.59G/1.93G [00:09<00:02, 165MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  84% 1.61G/1.93G [00:09<00:01, 161MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  85% 1.64G/1.93G [00:09<00:01, 165MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  86% 1.66G/1.93G [00:09<00:01, 170MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  87% 1.68G/1.93G [00:09<00:01, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  88% 1.70G/1.93G [00:09<00:01, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  89% 1.72G/1.93G [00:09<00:01, 165MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  90% 1.74G/1.93G [00:10<00:01, 167MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  91% 1.76G/1.93G [00:10<00:01, 111MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  92% 1.78G/1.93G [00:10<00:01, 123MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  94% 1.80G/1.93G [00:10<00:00, 132MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  95% 1.82G/1.93G [00:10<00:00, 144MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  96% 1.85G/1.93G [00:10<00:00, 150MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  97% 1.87G/1.93G [00:11<00:00, 160MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  98% 1.89G/1.93G [00:11<00:00, 164MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin:  99% 1.91G/1.93G [00:11<00:00, 169MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00007.bin: 100% 1.93G/1.93G [00:11<00:00, 169MB/s]\n",
            "Downloading shards:  86% 6/7 [00:49<00:08,  8.93s/it]\n",
            "pytorch_model-00007-of-00007.bin:   0% 0.00/1.05G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:   1% 10.5M/1.05G [00:00<00:11, 88.9MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:   2% 21.0M/1.05G [00:00<00:14, 68.8MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:   4% 41.9M/1.05G [00:00<00:10, 94.2MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:   6% 62.9M/1.05G [00:00<00:07, 124MB/s] \u001b[A\n",
            "pytorch_model-00007-of-00007.bin:   8% 83.9M/1.05G [00:00<00:06, 139MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  10% 105M/1.05G [00:00<00:06, 148MB/s] \u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  12% 126M/1.05G [00:00<00:06, 154MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  14% 147M/1.05G [00:01<00:05, 159MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  16% 168M/1.05G [00:01<00:05, 163MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  18% 189M/1.05G [00:01<00:05, 166MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  20% 210M/1.05G [00:01<00:05, 169MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  22% 231M/1.05G [00:01<00:04, 169MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  24% 252M/1.05G [00:01<00:04, 168MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  26% 273M/1.05G [00:01<00:04, 167MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  28% 294M/1.05G [00:01<00:04, 168MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  30% 315M/1.05G [00:02<00:04, 170MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  32% 336M/1.05G [00:02<00:04, 172MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  34% 357M/1.05G [00:02<00:04, 173MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  36% 377M/1.05G [00:02<00:03, 174MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  38% 398M/1.05G [00:02<00:03, 174MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  40% 419M/1.05G [00:02<00:03, 180MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  42% 440M/1.05G [00:02<00:03, 173MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  44% 461M/1.05G [00:02<00:03, 176MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  46% 482M/1.05G [00:03<00:03, 173MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  48% 503M/1.05G [00:03<00:03, 171MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  50% 524M/1.05G [00:03<00:03, 167MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  52% 545M/1.05G [00:03<00:03, 159MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  54% 566M/1.05G [00:03<00:03, 158MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  56% 587M/1.05G [00:03<00:02, 163MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  58% 608M/1.05G [00:03<00:02, 162MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  60% 629M/1.05G [00:03<00:02, 172MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  62% 650M/1.05G [00:04<00:02, 167MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  64% 671M/1.05G [00:04<00:02, 165MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  66% 692M/1.05G [00:04<00:02, 159MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  68% 713M/1.05G [00:04<00:02, 160MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  70% 734M/1.05G [00:04<00:01, 160MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  72% 755M/1.05G [00:04<00:01, 160MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  74% 776M/1.05G [00:04<00:01, 164MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  76% 797M/1.05G [00:04<00:01, 161MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  78% 818M/1.05G [00:05<00:01, 158MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  80% 839M/1.05G [00:05<00:01, 161MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  82% 860M/1.05G [00:05<00:01, 160MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  84% 881M/1.05G [00:05<00:01, 165MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  86% 902M/1.05G [00:05<00:00, 160MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  88% 923M/1.05G [00:05<00:00, 159MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  90% 944M/1.05G [00:05<00:00, 163MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  92% 965M/1.05G [00:05<00:00, 166MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  94% 986M/1.05G [00:06<00:00, 168MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  96% 1.01G/1.05G [00:06<00:00, 170MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin:  98% 1.03G/1.05G [00:06<00:00, 166MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00007.bin: 100% 1.05G/1.05G [00:06<00:00, 161MB/s]\n",
            "Downloading shards: 100% 7/7 [00:55<00:00,  7.98s/it]\n",
            "Loading checkpoint shards: 100% 7/7 [00:29<00:00,  4.18s/it]\n",
            "Processing model states: 100% 199/199 [00:58<00:00,  3.39it/s]\n",
            "+---------------------------------------------------------------------+---------------------------+---------+\n",
            "| name                                                                | shape                     | dtype   |\n",
            "|---------------------------------------------------------------------+---------------------------+---------|\n",
            "| transformer.embedding.word_embeddings.weight                        | torch.Size([65024, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.0.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.0.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.0.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.0.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.0.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.0.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.0.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.1.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.1.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.1.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.1.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.1.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.1.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.1.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.2.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.2.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.2.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.2.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.2.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.2.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.2.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.3.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.3.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.3.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.3.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.3.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.3.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.3.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.4.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.4.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.4.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.4.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.4.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.4.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.4.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.5.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.5.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.5.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.5.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.5.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.5.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.5.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.6.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.6.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.6.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.6.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.6.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.6.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.6.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.7.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.7.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.7.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.7.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.7.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.7.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.7.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.8.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.8.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.8.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.8.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.8.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.8.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.8.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.9.input_layernorm.weight                 | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.9.self_attention.query_key_value.weight  | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.9.self_attention.query_key_value.bias    | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.9.self_attention.dense.weight            | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.9.post_attention_layernorm.weight        | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.9.mlp.dense_h_to_4h.weight               | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.9.mlp.dense_4h_to_h.weight               | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.10.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.10.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.10.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.10.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.10.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.10.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.10.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.11.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.11.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.11.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.11.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.11.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.11.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.11.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.12.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.12.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.12.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.12.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.12.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.12.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.12.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.13.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.13.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.13.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.13.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.13.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.13.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.13.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.14.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.14.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.14.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.14.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.14.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.14.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.14.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.15.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.15.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.15.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.15.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.15.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.15.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.15.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.16.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.16.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.16.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.16.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.16.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.16.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.16.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.17.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.17.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.17.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.17.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.17.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.17.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.17.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.18.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.18.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.18.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.18.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.18.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.18.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.18.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.19.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.19.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.19.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.19.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.19.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.19.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.19.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.20.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.20.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.20.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.20.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.20.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.20.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.20.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.21.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.21.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.21.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.21.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.21.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.21.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.21.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.22.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.22.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.22.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.22.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.22.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.22.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.22.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.23.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.23.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.23.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.23.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.23.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.23.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.23.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.24.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.24.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.24.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.24.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.24.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.24.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.24.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.25.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.25.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.25.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.25.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.25.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.25.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.25.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.26.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.26.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.26.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.26.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.26.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.26.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.26.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.layers.27.input_layernorm.weight                | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.27.self_attention.query_key_value.weight | torch.Size([4608, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.27.self_attention.query_key_value.bias   | torch.Size([4608])        | F32     |\n",
            "| transformer.encoder.layers.27.self_attention.dense.weight           | torch.Size([4096, 4096])  | Q4_0    |\n",
            "| transformer.encoder.layers.27.post_attention_layernorm.weight       | torch.Size([4096])        | F32     |\n",
            "| transformer.encoder.layers.27.mlp.dense_h_to_4h.weight              | torch.Size([27392, 4096]) | Q4_0    |\n",
            "| transformer.encoder.layers.27.mlp.dense_4h_to_h.weight              | torch.Size([4096, 13696]) | Q4_0    |\n",
            "| transformer.encoder.final_layernorm.weight                          | torch.Size([4096])        | F32     |\n",
            "| transformer.output_layer.weight                                     | torch.Size([65024, 4096]) | Q4_0    |\n",
            "+---------------------------------------------------------------------+---------------------------+---------+\n",
            "GGML model saved to chatglm3-6b-ggml.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -B build -DGGML_OPENBLAS=ON && cmake --build build -j --config Release\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-LqHWAfP17o",
        "outputId": "603a22b2-ae8a-4d74-93bc-7bd7e7d27333"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMake Deprecation Warning at third_party/ggml/CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Linux detected\n",
            "\u001b[0mCMake Deprecation Warning at third_party/sentencepiece/CMakeLists.txt:15 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- VERSION: 0.2.00\n",
            "-- Configuring done (1.1s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/chatglm.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object third_party/ggml/src/CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object third_party/ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/builder.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/unicode_script.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object third_party/ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/trainer_factory.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/trainer_interface.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/unigram_model_trainer.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/word_model_trainer.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/char_model_trainer.cc.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/bpe_model_trainer.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/sentencepiece_trainer.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece_train-static.dir/pretokenizer_for_training.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/char_model.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/model_factory.cc.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/filesystem.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/error.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/model_interface.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/normalizer.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/util.cc.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/sentencepiece-static.dir/word_model.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual google::protobuf::uint8* google::protobuf::internal::ImplicitWeakMessage::_InternalSerialize(google::protobuf::uint8*, google::protobuf::io::EpsCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:85:28\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/chatglm.cpp/third_party/sentencepiece/third_party/protobuf-lite/message_lite.cc:419:30\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K’ specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libsentencepiece.a\u001b[0m\n",
            "[ 77%] Built target sentencepiece-static\n",
            "[ 78%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/spm_encode.dir/spm_encode_main.cc.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/spm_decode.dir/spm_decode_main.cc.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/spm_export_vocab.dir/spm_export_vocab_main.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking C shared library ../../../lib/libggml.so\u001b[0m\n",
            "[ 82%] Built target ggml\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/chatglm.dir/chatglm.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/spm_export_vocab\u001b[0m\n",
            "[ 85%] Built target spm_export_vocab\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/spm_decode\u001b[0m\n",
            "[ 87%] Built target spm_decode\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/spm_encode\u001b[0m\n",
            "[ 88%] Built target spm_encode\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libsentencepiece_train.a\u001b[0m\n",
            "[ 90%] Built target sentencepiece_train-static\n",
            "[ 91%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/spm_normalize.dir/spm_normalize_main.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object third_party/sentencepiece/src/CMakeFiles/spm_train.dir/spm_train_main.cc.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/spm_normalize\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/spm_train\u001b[0m\n",
            "[ 95%] Built target spm_normalize\n",
            "[ 95%] Built target spm_train\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX static library lib/libchatglm.a\u001b[0m\n",
            "[ 97%] Built target chatglm\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable bin/main\u001b[0m\n",
            "[100%] Built target main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Uad7n7VVD4w",
        "outputId": "7a5332ab-ef48-4f4d-a65d-dd41a82363b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: ./build/bin/main [options]\n",
            "\n",
            "options:\n",
            "  -h, --help              show this help message and exit\n",
            "  -m, --model PATH        model path (default: chatglm-ggml.bin)\n",
            "  --mode                  inference mode chosen from {chat, generate} (default: chat)\n",
            "  --sync                  synchronized generation without streaming\n",
            "  -p, --prompt PROMPT     prompt to start generation with (default: 你好)\n",
            "  --pp, --prompt_path     path to the plain text file that stores the prompt\n",
            "  -s, --system SYSTEM     system message to set the behavior of the assistant\n",
            "  --sp, --system_path     path to the plain text file that stores the system message\n",
            "  -i, --interactive       run in interactive mode\n",
            "  -l, --max_length N      max total length including prompt and output (default: 2048)\n",
            "  --max_new_tokens N      max number of tokens to generate, ignoring the number of prompt tokens\n",
            "  -c, --max_context_length N\n",
            "                          max context length (default: 512)\n",
            "  --top_k N               top-k sampling (default: 0)\n",
            "  --top_p N               top-p sampling (default: 0.7)\n",
            "  --temp N                temperature (default: 0.95)\n",
            "  --repeat_penalty N      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "  -t, --threads N         number of threads for inference\n",
            "  -v, --verbose           display verbose output including config/system/performance info\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -m chatglm3-6b-ggml.bin -p 你好"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJFLP_ptRDDH",
        "outputId": "e4071419-bc87-4e02-db24-968d3e1fc51c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好👋！我是人工智能助手 ChatGLM3-6B，很高兴见到你，欢迎问我任何问题。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -m chatglm3-6b-ggml.bin -i\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag1MrMAQRMp0",
        "outputId": "3a6d174a-f9bf-412f-e258-9e5ff31cccf3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ________          __  ________    __  ___                 \n",
            "   / ____/ /_  ____ _/ /_/ ____/ /   /  |/  /_________  ____  \n",
            "  / /   / __ \\/ __ `/ __/ / __/ /   / /|_/ // ___/ __ \\/ __ \\ \n",
            " / /___/ / / / /_/ / /_/ /_/ / /___/ /  / // /__/ /_/ / /_/ / \n",
            " \\____/_/ /_/\\__,_/\\__/\\____/_____/_/  /_(_)___/ .___/ .___/  \n",
            "                                              /_/   /_/       \n",
            "\n",
            "Welcome to ChatGLM.cpp! Ask whatever you want. Type 'clear' to clear context. Type 'stop' to exit.\n",
            "\n",
            "Prompt   > 你好\n",
            "ChatGLM3 > 你好👋！我是人工智能助手 ChatGLM3-6B，很高兴见到你，欢迎问我任何问题。\n",
            "Prompt   > 现在几点\n",
            "ChatGLM3 > 很抱歉，我不能回答这个问题，因为我没有访问当前时间的权限。\n",
            "Prompt   > 你叫什么名字\n",
            "ChatGLM3 > 我叫 ChatGLM3-6B，是清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
            "Prompt   > 1+1=？\n",
            "ChatGLM3 > 1+1=2\n",
            "Prompt   > 1+1 确定等于2吗？\n",
            "ChatGLM3 > 是的，1+1 确定等于 2。\n",
            "Prompt   > 哈哈\n",
            "ChatGLM3 > 看到您开心，我也很高兴。如果您还有其他问题或者需要帮助，请随时告诉我。\n",
            "Prompt   > 讲个笑话吧\n",
            "ChatGLM3 > 好的，给您讲个笑话：\n",
            "\n",
            "有一天，一只蚂蚁和一只大象走在一起。蚂蚁突然说：“哇，我们俩一起走，真是地动山摇啊！”大象回答：“是啊，都怪我走路太重了，把你震得都飞起来了。”\n",
            "Prompt   > 我去，还有吗？\n",
            "ChatGLM3 > 另外还有一个笑话：\n",
            "\n",
            "有一只鸟飞到了银行门口，突然drop了了一个色的报纸， implements 上学的鸟朋友 2019 超过了30 岁，就火葬了。\n",
            "\n",
            "希望您喜欢这些笑话！如果您还需要其他类型的笑话或信息，请告诉我。\n",
            "Prompt   > 。。。你是程序员开发的吗？\n",
            "ChatGLM3 > 是的，我是由程序员开发的。作为一个语言模型，我是基于人工智能技术和自然语言处理算法训练出来的。\n",
            "Prompt   > 程序员的职业发展有什么建议呢？\n",
            "ChatGLM3 > 程序员的职业发展可以参考以下建议：\n",
            "\n",
            "1. 不断学习新技术：随着技术的发展，不断学习新的编程语言、框架、库等，保持自己的竞争力。\n",
            "\n",
            "2. 深度专研：在某个领域深入研究，成为该领域的专家，能够为团队解决问题，并获得更高的回报。\n",
            "\n",
            "3. 代码审查和重构：通过审查其他程序员的代码，提高自己的代码质量和编程能力。\n",
            "\n",
            "4. 沟通和团队协作：锻炼自己的沟通和团队协作能力，能够更好地与同事、客户沟通，理解需求，提高工作效率。\n",
            "\n",
            "5. 项目管理：学习项目管理知识，了解如何规划项目、控制进度、处理风险等，提高自己的领导能力。\n",
            "\n",
            "6. 培养创新意识：不断挑战自己，尝试创新，为自己的职业发展注入新的动力。\n",
            "\n",
            "7. 建立个人品牌：通过技术博客、开源项目、演讲等方式，建立自己的个人品牌，提高自己的知名度。\n",
            "\n",
            "8. 持续学习：保持对新技术的敏感度，不断学习，不断进步。\n",
            "\n",
            "希望这些建议能对您有所帮助。\n",
            "Prompt   > 怎么看待程序员35岁现象\n",
            "ChatGLM3 > 程序员35岁现象是指程序员在35岁前后出现职业瓶颈的现象。这个年龄段的程序员通常已经在职场中有一定的经验和技能，但面临着事业发展的挑战。\n",
            "\n",
            "关于程序员35岁现象，我的看法如下：\n",
            "\n",
            "1. 这是一个普遍现象：随着年龄的增长，职场中许多人都会面临职业瓶颈，这不仅仅局限于程序员行业，而是普遍存在的现象。\n",
            "\n",
            "2. 原因：程序员在35岁左右时，可能会遇到职场晋升的障碍，如职场竞争、工作压力、技能更新等。\n",
            "\n",
            "3. 应对策略：\n",
            "\n",
            "a. 学习新技能：不断学习新技术，提升自己的技能，增强自己的竞争力。\n",
            "\n",
            "b. 提升沟通能力：提高自己的沟通能力，学会更好地与同事、客户沟通，理解需求。\n",
            "\n",
            "c. 培养领导力：学习项目管理知识，提高自己的领导能力，为团队注入新的动力。\n",
            "\n",
            "d. 寻找新的职业发展路径：根据自己的兴趣和能力，寻找新的职业发展路径，如转行、升职等。\n",
            "\n",
            "e. 保持积极心态：保持积极的心态，对自己的职业发展有信心，相信自己能够克服困难，实现自己的目标。\n",
            "\n",
            "总之，面对程序员35岁现象，我们应该积极应对，不断提升自己的能力和素质，寻找新的职业发展机会，为自己的职业生涯继续前进。\n",
            "Prompt   > 解释下人情世故吧\n",
            "ChatGLM3 > 人情世故，简称人际交往，指的是人与人之间在学校、工作场所或其他场合中进行沟通、交流和互动的过程。这些过程中所涉及到的种种事情和交往技巧，就是人情世故。人情世故包括了人际关系、社交礼仪、情感交流等方面的内容。\n",
            "\n",
            "人情世故的重要性在于：\n",
            "\n",
            "1. 建立良好的人际关系：在学校、工作场所或其他场合中，良好的人际关系有助于我们建立信任、友谊，为我们的发展创造有利条件。\n",
            "\n",
            "2. 提高社交技巧：掌握人情世故的技巧有助于我们在社交场合中更加自信、从容地应对各种情况，提高自己的社交能力。\n",
            "\n",
            "3. 增进职场成功：在职场中，人情世故的技巧有助于我们更好地了解同事、领导的需求和期望，为自己的事业发展创造条件。\n",
            "\n",
            "4. 促进个人成长：通过人情世故的交往，我们可以更好地了解自己和他人的需求，提高自己的沟通能力、情商等。\n",
            "\n",
            "然而，人情世故也存在一定的困扰和压力，如处理人际关系中的矛盾、冲突等。因此，我们应该在注重人情世故的同时，保持积极、坦诚的心态，克服困难，实现自己的目标。\n",
            "Prompt   > 谢谢你的回答\n",
            "ChatGLM3 > 不客气，希望我的回答能对你有所帮助。如果你还有其他问题或需要进一步的解释，请随时提问。我会尽力为你解答。\n",
            "Prompt   > ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -m chatglm3-6b-ggml.bin -p 你好 --top_p 0.8 --temp 0.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7RIqtTFU_4Z",
        "outputId": "28762ca8-003a-48a1-f613-2ad9dc582957"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好👋！我是人工智能助手 ChatGLM3-6B，很高兴见到你，欢迎问我任何问题。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -m chatglm3-6b-ggml.bin -p 你好 \\\n",
        "  -s \"You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUcJaXzPUzxh",
        "outputId": "f35ddea7-34a7-4ac5-c02d-19efda1efbf2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好👋！我是 ChatGLM3，有什么问题可以帮到你吗？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## function_call"
      ],
      "metadata": {
        "id": "rxQIrrM-VY6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -m chatglm3-6b-ggml.bin --top_p 0.8 --temp 0.8 \\\n",
        "  --sp examples/system/function_call.txt -i\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ9OxCbiVIQQ",
        "outputId": "8865e23e-cc5a-470a-a6d9-89c2cb9c870f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ________          __  ________    __  ___                 \n",
            "   / ____/ /_  ____ _/ /_/ ____/ /   /  |/  /_________  ____  \n",
            "  / /   / __ \\/ __ `/ __/ / __/ /   / /|_/ // ___/ __ \\/ __ \\ \n",
            " / /___/ / / / /_/ / /_/ /_/ / /___/ /  / // /__/ /_/ / /_/ / \n",
            " \\____/_/ /_/\\__,_/\\__/\\____/_____/_/  /_(_)___/ .___/ .___/  \n",
            "                                              /_/   /_/       \n",
            "\n",
            "Welcome to ChatGLM.cpp! Ask whatever you want. Type 'clear' to clear context. Type 'stop' to exit.\n",
            "\n",
            "System   > Answer the following questions as best as you can. You have access to the following tools:\n",
            "{\n",
            "    \"random_number_generator\": {\n",
            "        \"name\": \"random_number_generator\",\n",
            "        \"description\": \"Generates a random number x, s.t. range[0] <= x < range[1]\",\n",
            "        \"params\": [\n",
            "            {\n",
            "                \"name\": \"seed\",\n",
            "                \"description\": \"The random seed used by the generator\",\n",
            "                \"type\": \"int\",\n",
            "                \"required\": true\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"range\",\n",
            "                \"description\": \"The range of the generated numbers\",\n",
            "                \"type\": \"tuple[int, int]\",\n",
            "                \"required\": true\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"get_weather\": {\n",
            "        \"name\": \"get_weather\",\n",
            "        \"description\": \"Get the current weather for `city_name`\",\n",
            "        \"params\": [\n",
            "            {\n",
            "                \"name\": \"city_name\",\n",
            "                \"description\": \"The name of the city to be queried\",\n",
            "                \"type\": \"str\",\n",
            "                \"required\": true\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "}\n",
            "Prompt   > 生成一个随机数\n",
            "ChatGLM3 > random_number_generator\n",
            "```python\n",
            "tool_call(seed=42, range=(0, 100))\n",
            "```\n",
            "Function Call > Please manually call function `random_number_generator` with args `tool_call(seed=42, range=(0, 100))` and provide the results below.\n",
            "Observation   > 23\n",
            "ChatGLM3 > 根据API调用结果，我生成了一个在0到100范围内的随机数，结果为23。请问有什么我可以帮您解决的问题吗？\n",
            "Prompt   > 北京现在天气？\n",
            "ChatGLM3 > 用户询问关于北京的天气，需要调用天气API获取信息。 get_weather\n",
            "```python\n",
            "tool_call(city_name='北京')\n",
            "```\n",
            "Function Call > Please manually call function `` with args `tool_call(city_name='北京')` and provide the results below.\n",
            "Observation   > 北京\n",
            "ChatGLM3 > 北京现在的天气是晴天，气温为25℃。\n",
            "Prompt   > ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code interpreter"
      ],
      "metadata": {
        "id": "luDwb2j7WKFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -m chatglm3-ggml.bin --top_p 0.8 --temp 0.8 \\\n",
        "  --sp examples/system/code_interpreter.txt -i"
      ],
      "metadata": {
        "id": "lls0lxEyWLQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Demo\n",
        "\n"
      ],
      "metadata": {
        "id": "DmrYwKLvXOUs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P678woPXXV47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Server\n"
      ],
      "metadata": {
        "id": "SvbXBhC8XwV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'chatglm-cpp[api]'\n"
      ],
      "metadata": {
        "id": "JJWON14cX4lP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}