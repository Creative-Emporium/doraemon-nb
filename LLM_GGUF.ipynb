{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8qGd1RioKhUyxhavKDh8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/LLM_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://github.com/ggerganov/ggml/blob/master/docs/gguf.md gguf规范文档\n",
        "- https://github.com/weedge/ggml/blob/master/docs/gguf-cn.md gguf规范中文版\n",
        "- [用于量化 GGUF 模型的量化格式](https://github.com/ggerganov/ggml/blob/master/src/ggml-quants.h)。\n"
      ],
      "metadata": {
        "id": "pkuU6RdkGXF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GGUF 工具\n"
      ],
      "metadata": {
        "id": "ccyoyfJjNLj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H8kqOupGEdq",
        "outputId": "196d2318-1902-497e-f507-e9d6f385c3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gguf-tools'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 136 (delta 81), reused 102 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (136/136), 59.25 KiB | 4.94 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/antirez/gguf-tools.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gguf-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayvpmx1vOMcY",
        "outputId": "508b458a-137a-486c-ab67-848a2a3c36c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gguf-tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnZzswceOO3I",
        "outputId": "5ec8549e-db4d-4b69-c9b0-2a0d3f00055a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cc gguf-tools.c gguflib.c sds.c fp16.c \\\n",
            "\t-march=native -flto -ffast-math \\\n",
            "\t-g -ggdb -Wall -W -pedantic -O3 -o gguf-tools\n",
            "\u001b[01m\u001b[Kgguf-tools.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgguf_tools_show\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgguf-tools.c:169:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  169 |         printf(\"%s tensor %.*s @\u001b[01;35m\u001b[K%llu\u001b[m\u001b[K, %llu weights, %llu bytes\\n\",\n",
            "      |                                 \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                    \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                    \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "......\n",
            "  173 |             \u001b[32m\u001b[Ktensor.offset\u001b[m\u001b[K,\n",
            "      |             \u001b[32m\u001b[K~~~~~~~~~~~~~\u001b[m\u001b[K           \n",
            "      |                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguf-tools.c:169:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 6 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  169 |         printf(\"%s tensor %.*s @%llu, \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K weights, %llu bytes\\n\",\n",
            "      |                                       \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                          \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                       \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "......\n",
            "  174 |             \u001b[32m\u001b[Ktensor.num_weights\u001b[m\u001b[K,\n",
            "      |             \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K            \n",
            "      |                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguf-tools.c:169:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 7 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  169 |         printf(\"%s tensor %.*s @%llu, %llu weights, \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K bytes\\n\",\n",
            "      |                                                     \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                        \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                                     \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "......\n",
            "  175 |             \u001b[32m\u001b[Ktensor.bsize\u001b[m\u001b[K);\n",
            "      |             \u001b[32m\u001b[K~~~~~~~~~~~~\u001b[m\u001b[K                                \n",
            "      |                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguf-tools.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgguf_tools_split_mixtral\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgguf-tools.c:311:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  311 |     printf(\"Output file: after writing tensors info, file size is: \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K\\n\", \u001b[32m\u001b[Koutput->size\u001b[m\u001b[K);\n",
            "      |                                                                    \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K     \u001b[32m\u001b[K~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K           \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                                                    \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgguf_print_value_callback\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgguflib.c:362:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  362 |             printf(\"... \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K more items of %llu\", \u001b[32m\u001b[Karray_len-in_array+1\u001b[m\u001b[K,\n",
            "      |                         \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                      \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                            \u001b[01;35m\u001b[K|\u001b[m\u001b[K                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                            \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "      |                         \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:362:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  362 |             printf(\"... %llu more items of \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K\", array_len-in_array+1,\n",
            "      |                                            \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                            \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "  363 |                                                   \u001b[32m\u001b[Karray_len\u001b[m\u001b[K);\n",
            "      |                                                   \u001b[32m\u001b[K~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:395:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  395 |             printf(\"\u001b[01;35m\u001b[K%llu\u001b[m\u001b[K\", \u001b[32m\u001b[Kval->uint64\u001b[m\u001b[K); break;\n",
            "      |                     \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K   \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                     \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:397:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  397 |             printf(\"\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K\", \u001b[32m\u001b[Kval->int64\u001b[m\u001b[K); break;\n",
            "      |                     \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K   \u001b[32m\u001b[K~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                     \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7qKB8JbRlsH",
        "outputId": "a3c8ab42-4c23-4476-caea-02698fefc595"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/phi-2-GGUF phi-2.Q8_0.gguf --local-dir ./models --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I6tuSujRoQv",
        "outputId": "6eda09ab-c5dd-4b70-bdcb-0ce1925bd2af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q8_0.gguf to /root/.cache/huggingface/hub/tmp8bsvbj_7\n",
            "phi-2.Q8_0.gguf: 100% 2.96G/2.96G [00:11<00:00, 250MB/s]\n",
            "./models/phi-2.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools show file.gguf\n",
        "显示有关 GGUF 文件的详细信息。这将包括所有键值对，包括数组和详细的张量信息。张量的偏移量将相对于文件的开头（因此它们实际上是绝对偏移量），而不是像 GGUF 格式中的数据部分的开头。\n",
        "\n"
      ],
      "metadata": {
        "id": "UszNqIMVTqZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools show models/phi-2.Q8_0.gguf | head -100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF6p-epxSEik",
        "outputId": "e3316433-3ed5-46cc-bdd2-d77b3d3c12c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/phi-2.Q8_0.gguf (ver 3): 20 key-value pairs, 325 tensors\n",
            "general.architecture: [string] phi2\n",
            "general.name: [string] Phi2\n",
            "phi2.context_length: [uint32] 2048\n",
            "phi2.embedding_length: [uint32] 2560\n",
            "phi2.feed_forward_length: [uint32] 10240\n",
            "phi2.block_count: [uint32] 32\n",
            "phi2.attention.head_count: [uint32] 32\n",
            "phi2.attention.head_count_kv: [uint32] 32\n",
            "phi2.attention.layer_norm_epsilon: [float32] 0.000010\n",
            "phi2.rope.dimension_count: [uint32] 32\n",
            "general.file_type: [uint32] 7\n",
            "tokenizer.ggml.add_bos_token: [bool] false\n",
            "tokenizer.ggml.model: [string] gpt2\n",
            "tokenizer.ggml.tokens: [array] [!, \", #, $, %, &, ', (, ), *, +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, <, =, >, ... 51170 more items of 51200]\n",
            "tokenizer.ggml.token_type: [array] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... 51170 more items of 51200]\n",
            "tokenizer.ggml.merges: [array] [Ġ t, Ġ a, h e, i n, r e, o n, Ġt he, e r, Ġ s, a t, Ġ w, Ġ o, e n, Ġ c, i t, i s, a n, o r, e s, Ġ b, e d, Ġ f, in g, Ġ p, o u, Ġa n, a l, a r, Ġt o, Ġ m, ... 49970 more items of 50000]\n",
            "tokenizer.ggml.bos_token_id: [uint32] 50256\n",
            "tokenizer.ggml.eos_token_id: [uint32] 50256\n",
            "tokenizer.ggml.unknown_token_id: [uint32] 50256\n",
            "general.quantization_version: [uint32] 2\n",
            "q8_0 tensor token_embd.weight @1806176, 131072000 weights, 139264000 bytes\n",
            "f32 tensor blk.0.attn_norm.bias @141070176, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.0.attn_norm.weight @141080416, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.0.attn_qkv.bias @141090656, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.0.attn_qkv.weight @141121376, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.0.attn_output.bias @162010976, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.0.attn_output.weight @162021216, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.0.ffn_up.bias @168984416, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.0.ffn_up.weight @169025376, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.0.ffn_down.bias @196878176, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.0.ffn_down.weight @196888416, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.1.attn_norm.bias @224741216, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.1.attn_norm.weight @224751456, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.1.attn_qkv.bias @224761696, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.1.attn_qkv.weight @224792416, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.1.attn_output.bias @245682016, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.1.attn_output.weight @245692256, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.1.ffn_up.bias @252655456, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.1.ffn_up.weight @252696416, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.1.ffn_down.bias @280549216, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.1.ffn_down.weight @280559456, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.10.attn_norm.bias @308412256, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.10.attn_norm.weight @308422496, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.10.attn_qkv.bias @308432736, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.10.attn_qkv.weight @308463456, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.10.attn_output.bias @329353056, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.10.attn_output.weight @329363296, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.10.ffn_up.bias @336326496, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.10.ffn_up.weight @336367456, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.10.ffn_down.bias @364220256, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.10.ffn_down.weight @364230496, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.11.attn_norm.bias @392083296, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.11.attn_norm.weight @392093536, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.11.attn_qkv.bias @392103776, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.11.attn_qkv.weight @392134496, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.11.attn_output.bias @413024096, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.11.attn_output.weight @413034336, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.11.ffn_up.bias @419997536, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.11.ffn_up.weight @420038496, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.11.ffn_down.bias @447891296, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.11.ffn_down.weight @447901536, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.12.attn_norm.bias @475754336, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.12.attn_norm.weight @475764576, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.12.attn_qkv.bias @475774816, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.12.attn_qkv.weight @475805536, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.12.attn_output.bias @496695136, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.12.attn_output.weight @496705376, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.12.ffn_up.bias @503668576, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.12.ffn_up.weight @503709536, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.12.ffn_down.bias @531562336, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.12.ffn_down.weight @531572576, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.13.attn_norm.bias @559425376, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.13.attn_norm.weight @559435616, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.13.attn_qkv.bias @559445856, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.13.attn_qkv.weight @559476576, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.13.attn_output.bias @580366176, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.13.attn_output.weight @580376416, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.13.ffn_up.bias @587339616, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.13.ffn_up.weight @587380576, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.13.ffn_down.bias @615233376, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.13.ffn_down.weight @615243616, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.14.attn_norm.bias @643096416, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.14.attn_norm.weight @643106656, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.14.attn_qkv.bias @643116896, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.14.attn_qkv.weight @643147616, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.14.attn_output.bias @664037216, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.14.attn_output.weight @664047456, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.14.ffn_up.bias @671010656, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.14.ffn_up.weight @671051616, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.14.ffn_down.bias @698904416, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.14.ffn_down.weight @698914656, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.15.attn_norm.bias @726767456, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.15.attn_norm.weight @726777696, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.15.attn_qkv.bias @726787936, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.15.attn_qkv.weight @726818656, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.15.attn_output.bias @747708256, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.15.attn_output.weight @747718496, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.15.ffn_up.bias @754681696, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.15.ffn_up.weight @754722656, 26214400 weights, 27852800 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools compare file1.gguf file2.gguf\n",
        "此工具有助于了解两个 LLM（或其他以 GGUF 文件形式分发的模型）是否相关，例如一个是否是另一个的微调，或者两者是否都是从同一个父模型进行微调的。\n",
        "\n",
        "对于每个匹配的张量（名称和参数数量相同），该命令计算平均权重差异（以百分比表示，因此在 -N 到 +N 范围内的随机分布平均上与另一个随机分布相比较将达到 100% 差异）。这有助于查看模型是否是另一个模型的微调版本，它被微调了多少，哪些层在微调时被冻结等。请注意，由于量化，即使功能上等效的张量可能也存在一些小的平均差异。\n",
        "\n",
        "示例输出：\n"
      ],
      "metadata": {
        "id": "Pj3kCAfcTyD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q8_0.gguf \\\n",
        "  --local-dir ./models --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYp6BgRaTxgp",
        "outputId": "5440b844-d87d-47d7-b273-16030af95044"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf to /root/.cache/huggingface/hub/tmpexfcef09\n",
            "mistral-7b-instruct-v0.2.Q8_0.gguf: 100% 7.70G/7.70G [00:28<00:00, 273MB/s]\n",
            "./models/mistral-7b-instruct-v0.2.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf \\\n",
        "  --local-dir ./models --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2DuYxFgUQ9L",
        "outputId": "a94b4e9d-c454-4372-b1a8-73adf7bbffa6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF/resolve/main/solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf to /root/.cache/huggingface/hub/tmp6ymjwy_8\n",
            "(…)10.7b-instruct-v1.0-uncensored.Q8_0.gguf: 100% 11.4G/11.4G [03:57<00:00, 48.0MB/s]\n",
            "./models/solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools compare models/mistral-7b-instruct-v0.2.Q8_0.gguf models/solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPKCQjTSWKUv",
        "outputId": "96e28df7-1e9c-4ff0-fc40-d24601e33242"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[token_embd.weight]: avg weights difference: 44.539944%\n",
            "[blk.0.attn_q.weight]: avg weights difference: 48.717736%\n",
            "[blk.0.attn_k.weight]: avg weights difference: 56.201885%\n",
            "[blk.0.attn_v.weight]: avg weights difference: 47.087249%\n",
            "[blk.0.attn_output.weight]: avg weights difference: 47.663048%\n",
            "[blk.0.ffn_gate.weight]: avg weights difference: 37.508761%\n",
            "[blk.0.ffn_up.weight]: avg weights difference: 39.061584%\n",
            "[blk.0.ffn_down.weight]: avg weights difference: 39.632648%\n",
            "[blk.0.attn_norm.weight]: avg weights difference: 3.997229%\n",
            "[blk.0.ffn_norm.weight]: avg weights difference: 5.686371%\n",
            "[blk.1.attn_q.weight]: avg weights difference: 47.318189%\n",
            "[blk.1.attn_k.weight]: avg weights difference: 44.031983%\n",
            "[blk.1.attn_v.weight]: avg weights difference: 36.303868%\n",
            "[blk.1.attn_output.weight]: avg weights difference: 40.389659%\n",
            "[blk.1.ffn_gate.weight]: avg weights difference: 38.658373%\n",
            "[blk.1.ffn_up.weight]: avg weights difference: 38.838259%\n",
            "[blk.1.ffn_down.weight]: avg weights difference: 37.840668%\n",
            "[blk.1.attn_norm.weight]: avg weights difference: 2.211642%\n",
            "[blk.1.ffn_norm.weight]: avg weights difference: 2.871902%\n",
            "[blk.2.attn_q.weight]: avg weights difference: 37.346753%\n",
            "[blk.2.attn_k.weight]: avg weights difference: 31.213910%\n",
            "[blk.2.attn_v.weight]: avg weights difference: 40.192856%\n",
            "[blk.2.attn_output.weight]: avg weights difference: 43.092701%\n",
            "[blk.2.ffn_gate.weight]: avg weights difference: 39.479107%\n",
            "[blk.2.ffn_up.weight]: avg weights difference: 39.526048%\n",
            "[blk.2.ffn_down.weight]: avg weights difference: 37.368479%\n",
            "[blk.2.attn_norm.weight]: avg weights difference: 2.445631%\n",
            "[blk.2.ffn_norm.weight]: avg weights difference: 2.347484%\n",
            "[blk.3.attn_q.weight]: avg weights difference: 38.638426%\n",
            "[blk.3.attn_k.weight]: avg weights difference: 31.908332%\n",
            "[blk.3.attn_v.weight]: avg weights difference: 32.967993%\n",
            "[blk.3.attn_output.weight]: avg weights difference: 37.274029%\n",
            "[blk.3.ffn_gate.weight]: avg weights difference: 38.899170%\n",
            "[blk.3.ffn_up.weight]: avg weights difference: 38.840248%\n",
            "[blk.3.ffn_down.weight]: avg weights difference: 37.489862%\n",
            "[blk.3.attn_norm.weight]: avg weights difference: 2.489361%\n",
            "[blk.3.ffn_norm.weight]: avg weights difference: 2.062806%\n",
            "[blk.4.attn_q.weight]: avg weights difference: 38.505135%\n",
            "[blk.4.attn_k.weight]: avg weights difference: 32.128014%\n",
            "[blk.4.attn_v.weight]: avg weights difference: 35.495024%\n",
            "[blk.4.attn_output.weight]: avg weights difference: 38.984788%\n",
            "[blk.4.ffn_gate.weight]: avg weights difference: 39.439534%\n",
            "[blk.4.ffn_up.weight]: avg weights difference: 38.947228%\n",
            "[blk.4.ffn_down.weight]: avg weights difference: 37.219814%\n",
            "[blk.4.attn_norm.weight]: avg weights difference: 2.662142%\n",
            "[blk.4.ffn_norm.weight]: avg weights difference: 1.997403%\n",
            "[blk.5.attn_q.weight]: avg weights difference: 37.021648%\n",
            "[blk.5.attn_k.weight]: avg weights difference: 30.047030%\n",
            "[blk.5.attn_v.weight]: avg weights difference: 33.831442%\n",
            "[blk.5.attn_output.weight]: avg weights difference: 36.671130%\n",
            "[blk.5.ffn_gate.weight]: avg weights difference: 38.709555%\n",
            "[blk.5.ffn_up.weight]: avg weights difference: 38.618665%\n",
            "[blk.5.ffn_down.weight]: avg weights difference: 36.862574%\n",
            "[blk.5.attn_norm.weight]: avg weights difference: 2.590642%\n",
            "[blk.5.ffn_norm.weight]: avg weights difference: 2.106001%\n",
            "[blk.6.attn_q.weight]: avg weights difference: 38.177590%\n",
            "[blk.6.attn_k.weight]: avg weights difference: 30.757326%\n",
            "[blk.6.attn_v.weight]: avg weights difference: 34.844417%\n",
            "[blk.6.attn_output.weight]: avg weights difference: 36.794619%\n",
            "[blk.6.ffn_gate.weight]: avg weights difference: 38.055215%\n",
            "[blk.6.ffn_up.weight]: avg weights difference: 38.183776%\n",
            "[blk.6.ffn_down.weight]: avg weights difference: 36.505638%\n",
            "[blk.6.attn_norm.weight]: avg weights difference: 2.735638%\n",
            "[blk.6.ffn_norm.weight]: avg weights difference: 2.089956%\n",
            "[blk.7.attn_q.weight]: avg weights difference: 37.813109%\n",
            "[blk.7.attn_k.weight]: avg weights difference: 30.317938%\n",
            "[blk.7.attn_v.weight]: avg weights difference: 31.793105%\n",
            "[blk.7.attn_output.weight]: avg weights difference: 33.939902%\n",
            "[blk.7.ffn_gate.weight]: avg weights difference: 37.719086%\n",
            "[blk.7.ffn_up.weight]: avg weights difference: 37.919256%\n",
            "[blk.7.ffn_down.weight]: avg weights difference: 36.205047%\n",
            "[blk.7.attn_norm.weight]: avg weights difference: 2.749280%\n",
            "[blk.7.ffn_norm.weight]: avg weights difference: 2.179731%\n",
            "[blk.8.attn_q.weight]: avg weights difference: 37.418863%\n",
            "[blk.8.attn_k.weight]: avg weights difference: 30.703674%\n",
            "[blk.8.attn_v.weight]: avg weights difference: 31.421675%\n",
            "[blk.8.attn_output.weight]: avg weights difference: 33.060101%\n",
            "[blk.8.ffn_gate.weight]: avg weights difference: 37.799261%\n",
            "[blk.8.ffn_up.weight]: avg weights difference: 37.859680%\n",
            "[blk.8.ffn_down.weight]: avg weights difference: 36.377553%\n",
            "[blk.8.attn_norm.weight]: avg weights difference: 2.681746%\n",
            "[blk.8.ffn_norm.weight]: avg weights difference: 2.241802%\n",
            "[blk.9.attn_q.weight]: avg weights difference: 37.823661%\n",
            "[blk.9.attn_k.weight]: avg weights difference: 30.615400%\n",
            "[blk.9.attn_v.weight]: avg weights difference: 33.012580%\n",
            "[blk.9.attn_output.weight]: avg weights difference: 33.388703%\n",
            "[blk.9.ffn_gate.weight]: avg weights difference: 38.211512%\n",
            "[blk.9.ffn_up.weight]: avg weights difference: 37.727899%\n",
            "[blk.9.ffn_down.weight]: avg weights difference: 36.666438%\n",
            "[blk.9.attn_norm.weight]: avg weights difference: 2.643698%\n",
            "[blk.9.ffn_norm.weight]: avg weights difference: 2.216395%\n",
            "[blk.10.attn_q.weight]: avg weights difference: 37.259335%\n",
            "[blk.10.attn_k.weight]: avg weights difference: 29.990339%\n",
            "[blk.10.attn_v.weight]: avg weights difference: 31.883167%\n",
            "[blk.10.attn_output.weight]: avg weights difference: 32.274127%\n",
            "[blk.10.ffn_gate.weight]: avg weights difference: 38.649153%\n",
            "[blk.10.ffn_up.weight]: avg weights difference: 37.996440%\n",
            "[blk.10.ffn_down.weight]: avg weights difference: 36.955804%\n",
            "[blk.10.attn_norm.weight]: avg weights difference: 2.558892%\n",
            "[blk.10.ffn_norm.weight]: avg weights difference: 2.241721%\n",
            "[blk.11.attn_q.weight]: avg weights difference: 40.012016%\n",
            "[blk.11.attn_k.weight]: avg weights difference: 32.131927%\n",
            "[blk.11.attn_v.weight]: avg weights difference: 29.644739%\n",
            "[blk.11.attn_output.weight]: avg weights difference: 33.145932%\n",
            "[blk.11.ffn_gate.weight]: avg weights difference: 38.862092%\n",
            "[blk.11.ffn_up.weight]: avg weights difference: 37.991157%\n",
            "[blk.11.ffn_down.weight]: avg weights difference: 36.996582%\n",
            "[blk.11.attn_norm.weight]: avg weights difference: 2.607966%\n",
            "[blk.11.ffn_norm.weight]: avg weights difference: 2.244699%\n",
            "[blk.12.attn_q.weight]: avg weights difference: 38.613124%\n",
            "[blk.12.attn_k.weight]: avg weights difference: 30.781870%\n",
            "[blk.12.attn_v.weight]: avg weights difference: 30.662009%\n",
            "[blk.12.attn_output.weight]: avg weights difference: 32.891754%\n",
            "[blk.12.ffn_gate.weight]: avg weights difference: 39.279775%\n",
            "[blk.12.ffn_up.weight]: avg weights difference: 37.981480%\n",
            "[blk.12.ffn_down.weight]: avg weights difference: 37.473172%\n",
            "[blk.12.attn_norm.weight]: avg weights difference: 2.628979%\n",
            "[blk.12.ffn_norm.weight]: avg weights difference: 2.275894%\n",
            "[blk.13.attn_q.weight]: avg weights difference: 38.039374%\n",
            "[blk.13.attn_k.weight]: avg weights difference: 30.561683%\n",
            "[blk.13.attn_v.weight]: avg weights difference: 28.995724%\n",
            "[blk.13.attn_output.weight]: avg weights difference: 31.145505%\n",
            "[blk.13.ffn_gate.weight]: avg weights difference: 39.723644%\n",
            "[blk.13.ffn_up.weight]: avg weights difference: 38.100503%\n",
            "[blk.13.ffn_down.weight]: avg weights difference: 38.059809%\n",
            "[blk.13.attn_norm.weight]: avg weights difference: 2.490819%\n",
            "[blk.13.ffn_norm.weight]: avg weights difference: 2.363987%\n",
            "[blk.14.attn_q.weight]: avg weights difference: 40.472187%\n",
            "[blk.14.attn_k.weight]: avg weights difference: 32.930238%\n",
            "[blk.14.attn_v.weight]: avg weights difference: 27.300996%\n",
            "[blk.14.attn_output.weight]: avg weights difference: 31.894562%\n",
            "[blk.14.ffn_gate.weight]: avg weights difference: 41.136303%\n",
            "[blk.14.ffn_up.weight]: avg weights difference: 39.108596%\n",
            "[blk.14.ffn_down.weight]: avg weights difference: 38.341792%\n",
            "[blk.14.attn_norm.weight]: avg weights difference: 2.538121%\n",
            "[blk.14.ffn_norm.weight]: avg weights difference: 2.551069%\n",
            "[blk.15.attn_q.weight]: avg weights difference: 40.611837%\n",
            "[blk.15.attn_k.weight]: avg weights difference: 34.254676%\n",
            "[blk.15.attn_v.weight]: avg weights difference: 27.622696%\n",
            "[blk.15.attn_output.weight]: avg weights difference: 33.154355%\n",
            "[blk.15.ffn_gate.weight]: avg weights difference: 41.763379%\n",
            "[blk.15.ffn_up.weight]: avg weights difference: 39.997022%\n",
            "[blk.15.ffn_down.weight]: avg weights difference: 39.382034%\n",
            "[blk.15.attn_norm.weight]: avg weights difference: 2.591605%\n",
            "[blk.15.ffn_norm.weight]: avg weights difference: 2.628742%\n",
            "[blk.16.attn_q.weight]: avg weights difference: 41.589593%\n",
            "[blk.16.attn_k.weight]: avg weights difference: 34.692658%\n",
            "[blk.16.attn_v.weight]: avg weights difference: 28.674807%\n",
            "[blk.16.attn_output.weight]: avg weights difference: 34.250061%\n",
            "[blk.16.ffn_gate.weight]: avg weights difference: 42.405602%\n",
            "[blk.16.ffn_up.weight]: avg weights difference: 40.948837%\n",
            "[blk.16.ffn_down.weight]: avg weights difference: 40.531679%\n",
            "[blk.16.attn_norm.weight]: avg weights difference: 2.539339%\n",
            "[blk.16.ffn_norm.weight]: avg weights difference: 2.665115%\n",
            "[blk.17.attn_q.weight]: avg weights difference: 43.977024%\n",
            "[blk.17.attn_k.weight]: avg weights difference: 37.305572%\n",
            "[blk.17.attn_v.weight]: avg weights difference: 30.896675%\n",
            "[blk.17.attn_output.weight]: avg weights difference: 36.561729%\n",
            "[blk.17.ffn_gate.weight]: avg weights difference: 43.448301%\n",
            "[blk.17.ffn_up.weight]: avg weights difference: 42.090316%\n",
            "[blk.17.ffn_down.weight]: avg weights difference: 42.240309%\n",
            "[blk.17.attn_norm.weight]: avg weights difference: 2.455910%\n",
            "[blk.17.ffn_norm.weight]: avg weights difference: 2.868740%\n",
            "[blk.18.attn_q.weight]: avg weights difference: 44.764107%\n",
            "[blk.18.attn_k.weight]: avg weights difference: 38.832177%\n",
            "[blk.18.attn_v.weight]: avg weights difference: 33.113803%\n",
            "[blk.18.attn_output.weight]: avg weights difference: 38.816583%\n",
            "[blk.18.ffn_gate.weight]: avg weights difference: 45.502698%\n",
            "[blk.18.ffn_up.weight]: avg weights difference: 44.017699%\n",
            "[blk.18.ffn_down.weight]: avg weights difference: 44.115881%\n",
            "[blk.18.attn_norm.weight]: avg weights difference: 2.416878%\n",
            "[blk.18.ffn_norm.weight]: avg weights difference: 2.927754%\n",
            "[blk.19.attn_q.weight]: avg weights difference: 47.196489%\n",
            "[blk.19.attn_k.weight]: avg weights difference: 41.841030%\n",
            "[blk.19.attn_v.weight]: avg weights difference: 34.385544%\n",
            "[blk.19.attn_output.weight]: avg weights difference: 40.929497%\n",
            "[blk.19.ffn_gate.weight]: avg weights difference: 47.539907%\n",
            "[blk.19.ffn_up.weight]: avg weights difference: 46.542849%\n",
            "[blk.19.ffn_down.weight]: avg weights difference: 46.713715%\n",
            "[blk.19.attn_norm.weight]: avg weights difference: 2.375080%\n",
            "[blk.19.ffn_norm.weight]: avg weights difference: 2.925147%\n",
            "[blk.20.attn_q.weight]: avg weights difference: 50.024704%\n",
            "[blk.20.attn_k.weight]: avg weights difference: 44.186161%\n",
            "[blk.20.attn_v.weight]: avg weights difference: 37.941084%\n",
            "[blk.20.attn_output.weight]: avg weights difference: 45.595754%\n",
            "[blk.20.ffn_gate.weight]: avg weights difference: 49.686502%\n",
            "[blk.20.ffn_up.weight]: avg weights difference: 49.797482%\n",
            "[blk.20.ffn_down.weight]: avg weights difference: 50.292769%\n",
            "[blk.20.attn_norm.weight]: avg weights difference: 2.328575%\n",
            "[blk.20.ffn_norm.weight]: avg weights difference: 2.893100%\n",
            "[blk.21.attn_q.weight]: avg weights difference: 51.864926%\n",
            "[blk.21.attn_k.weight]: avg weights difference: 45.568512%\n",
            "[blk.21.attn_v.weight]: avg weights difference: 41.739700%\n",
            "[blk.21.attn_output.weight]: avg weights difference: 49.506212%\n",
            "[blk.21.ffn_gate.weight]: avg weights difference: 50.911052%\n",
            "[blk.21.ffn_up.weight]: avg weights difference: 52.566557%\n",
            "[blk.21.ffn_down.weight]: avg weights difference: 53.212815%\n",
            "[blk.21.attn_norm.weight]: avg weights difference: 2.293635%\n",
            "[blk.21.ffn_norm.weight]: avg weights difference: 2.889721%\n",
            "[blk.22.attn_q.weight]: avg weights difference: 53.410648%\n",
            "[blk.22.attn_k.weight]: avg weights difference: 47.342402%\n",
            "[blk.22.attn_v.weight]: avg weights difference: 42.293365%\n",
            "[blk.22.attn_output.weight]: avg weights difference: 51.364368%\n",
            "[blk.22.ffn_gate.weight]: avg weights difference: 52.130977%\n",
            "[blk.22.ffn_up.weight]: avg weights difference: 54.536699%\n",
            "[blk.22.ffn_down.weight]: avg weights difference: 55.302310%\n",
            "[blk.22.attn_norm.weight]: avg weights difference: 2.390991%\n",
            "[blk.22.ffn_norm.weight]: avg weights difference: 3.016549%\n",
            "[blk.23.attn_q.weight]: avg weights difference: 53.041654%\n",
            "[blk.23.attn_k.weight]: avg weights difference: 48.415713%\n",
            "[blk.23.attn_v.weight]: avg weights difference: 41.050832%\n",
            "[blk.23.attn_output.weight]: avg weights difference: 51.755448%\n",
            "[blk.23.ffn_gate.weight]: avg weights difference: 52.869722%\n",
            "[blk.23.ffn_up.weight]: avg weights difference: 55.320554%\n",
            "[blk.23.ffn_down.weight]: avg weights difference: 56.109015%\n",
            "[blk.23.attn_norm.weight]: avg weights difference: 2.367006%\n",
            "[blk.23.ffn_norm.weight]: avg weights difference: 3.113265%\n",
            "[blk.24.attn_q.weight]: avg weights difference: 108.899523%\n",
            "[blk.24.attn_k.weight]: avg weights difference: 110.049306%\n",
            "[blk.24.attn_v.weight]: avg weights difference: 106.514180%\n",
            "[blk.24.attn_output.weight]: avg weights difference: 106.568760%\n",
            "[blk.24.ffn_gate.weight]: avg weights difference: 107.166285%\n",
            "[blk.24.ffn_up.weight]: avg weights difference: 108.004525%\n",
            "[blk.24.ffn_down.weight]: avg weights difference: 107.795442%\n",
            "[blk.24.attn_norm.weight]: avg weights difference: 48.379174%\n",
            "[blk.24.ffn_norm.weight]: avg weights difference: 61.560204%\n",
            "[blk.25.attn_q.weight]: avg weights difference: 109.019704%\n",
            "[blk.25.attn_k.weight]: avg weights difference: 110.332112%\n",
            "[blk.25.attn_v.weight]: avg weights difference: 106.817464%\n",
            "[blk.25.attn_output.weight]: avg weights difference: 106.669373%\n",
            "[blk.25.ffn_gate.weight]: avg weights difference: 107.154425%\n",
            "[blk.25.ffn_up.weight]: avg weights difference: 107.876515%\n",
            "[blk.25.ffn_down.weight]: avg weights difference: 107.653223%\n",
            "[blk.25.attn_norm.weight]: avg weights difference: 36.563442%\n",
            "[blk.25.ffn_norm.weight]: avg weights difference: 62.295531%\n",
            "[blk.26.attn_q.weight]: avg weights difference: 109.496764%\n",
            "[blk.26.attn_k.weight]: avg weights difference: 111.538304%\n",
            "[blk.26.attn_v.weight]: avg weights difference: 106.863084%\n",
            "[blk.26.attn_output.weight]: avg weights difference: 106.706427%\n",
            "[blk.26.ffn_gate.weight]: avg weights difference: 107.170340%\n",
            "[blk.26.ffn_up.weight]: avg weights difference: 107.759548%\n",
            "[blk.26.ffn_down.weight]: avg weights difference: 107.547046%\n",
            "[blk.26.attn_norm.weight]: avg weights difference: 39.477995%\n",
            "[blk.26.ffn_norm.weight]: avg weights difference: 62.659581%\n",
            "[blk.27.attn_q.weight]: avg weights difference: 109.029321%\n",
            "[blk.27.attn_k.weight]: avg weights difference: 110.000792%\n",
            "[blk.27.attn_v.weight]: avg weights difference: 106.843261%\n",
            "[blk.27.attn_output.weight]: avg weights difference: 106.985675%\n",
            "[blk.27.ffn_gate.weight]: avg weights difference: 107.161526%\n",
            "[blk.27.ffn_up.weight]: avg weights difference: 107.669111%\n",
            "[blk.27.ffn_down.weight]: avg weights difference: 107.433445%\n",
            "[blk.27.attn_norm.weight]: avg weights difference: 33.544700%\n",
            "[blk.27.ffn_norm.weight]: avg weights difference: 62.994368%\n",
            "[blk.28.attn_q.weight]: avg weights difference: 109.651855%\n",
            "[blk.28.attn_k.weight]: avg weights difference: 111.981668%\n",
            "[blk.28.attn_v.weight]: avg weights difference: 107.665774%\n",
            "[blk.28.attn_output.weight]: avg weights difference: 106.667741%\n",
            "[blk.28.ffn_gate.weight]: avg weights difference: 107.136942%\n",
            "[blk.28.ffn_up.weight]: avg weights difference: 107.488519%\n",
            "[blk.28.ffn_down.weight]: avg weights difference: 107.337726%\n",
            "[blk.28.attn_norm.weight]: avg weights difference: 26.051175%\n",
            "[blk.28.ffn_norm.weight]: avg weights difference: 60.396506%\n",
            "[blk.29.attn_q.weight]: avg weights difference: 110.061031%\n",
            "[blk.29.attn_k.weight]: avg weights difference: 112.580091%\n",
            "[blk.29.attn_v.weight]: avg weights difference: 107.837366%\n",
            "[blk.29.attn_output.weight]: avg weights difference: 106.839818%\n",
            "[blk.29.ffn_gate.weight]: avg weights difference: 107.168924%\n",
            "[blk.29.ffn_up.weight]: avg weights difference: 107.375079%\n",
            "[blk.29.ffn_down.weight]: avg weights difference: 107.331923%\n",
            "[blk.29.attn_norm.weight]: avg weights difference: 33.200632%\n",
            "[blk.29.ffn_norm.weight]: avg weights difference: 56.848111%\n",
            "[blk.30.attn_q.weight]: avg weights difference: 109.703995%\n",
            "[blk.30.attn_k.weight]: avg weights difference: 110.942919%\n",
            "[blk.30.attn_v.weight]: avg weights difference: 108.075344%\n",
            "[blk.30.attn_output.weight]: avg weights difference: 106.925215%\n",
            "[blk.30.ffn_gate.weight]: avg weights difference: 107.263311%\n",
            "[blk.30.ffn_up.weight]: avg weights difference: 107.374150%\n",
            "[blk.30.ffn_down.weight]: avg weights difference: 107.432925%\n",
            "[blk.30.attn_norm.weight]: avg weights difference: 27.909962%\n",
            "[blk.30.ffn_norm.weight]: avg weights difference: 55.151022%\n",
            "[blk.31.attn_q.weight]: avg weights difference: 109.676893%\n",
            "[blk.31.attn_k.weight]: avg weights difference: 111.353428%\n",
            "[blk.31.attn_v.weight]: avg weights difference: 108.417021%\n",
            "[blk.31.attn_output.weight]: avg weights difference: 107.072881%\n",
            "[blk.31.ffn_gate.weight]: avg weights difference: 107.160794%\n",
            "[blk.31.ffn_up.weight]: avg weights difference: 107.200618%\n",
            "[blk.31.ffn_down.weight]: avg weights difference: 108.043250%\n",
            "[blk.31.attn_norm.weight]: avg weights difference: 21.055964%\n",
            "[blk.31.ffn_norm.weight]: avg weights difference: 47.579968%\n",
            "[output_norm.weight]: avg weights difference: 2.362229%\n",
            "[output.weight]: avg weights difference: 37.724208%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools inspect-tensor file.gguf tensor.name [count]\n",
        "显示指定张量的所有权重值（如果未指定 count，则仅显示前 count 个）。这对于低级别的任务很有用，比如检查量化是否按预期工作，查看引入的误差，模型指纹等。\n",
        "\n"
      ],
      "metadata": {
        "id": "Rv_R7iWnZBqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo -e \"\\n----token_embd.weight----\\n\"\n",
        "!./gguf-tools inspect-tensor models/mistral-7b-instruct-v0.2.Q8_0.gguf token_embd.weight 50\n",
        "!echo -e \"\\n----blk.0.attn_q.weight----\\n\"\n",
        "!./gguf-tools inspect-tensor models/mistral-7b-instruct-v0.2.Q8_0.gguf blk.0.attn_q.weight 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uzegXfBZFwx",
        "outputId": "0b9131e2-9db4-4a58-f4b2-a4de974e5f74"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----token_embd.weight----\n",
            "\n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, \n",
            "\n",
            "----blk.0.attn_q.weight----\n",
            "\n",
            "0.000085, 0.000766, 0.000000, -0.000680, \n",
            "0.001616, -0.000085, 0.003828, 0.000255, \n",
            "0.002637, 0.000000, 0.000425, -0.000085, \n",
            "0.000000, 0.000170, 0.006039, -0.000595, \n",
            "0.000000, -0.002892, 0.000000, -0.006294, \n",
            "-0.004848, -0.002382, 0.010802, 0.002892, \n",
            "0.001191, 0.000000, -0.000255, 0.000000, \n",
            "0.000255, -0.003402, 0.000000, 0.007400, \n",
            "0.000000, 0.000000, 0.002815, 0.000088, \n",
            "0.000000, 0.000880, 0.000000, 0.000000, \n",
            "0.000000, 0.000088, 0.000176, -0.000880, \n",
            "0.001232, 0.000088, 0.000000, -0.000352, \n",
            "0.000176, -0.001496, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools split-mixtral 65230776370407150546470161412165 mixtral.gguf out.gguf\n",
        "从 Mixtral 7B MoE 中提取一个 7B 模型 out.gguf，使用指定的每层 MoE ID（在序列 652... 中有 32 个数字）。\n",
        "\n",
        "请注意，通过 split-mixtral 方式获得的模型实际上没有执行任何有用的工作。这只是一个实验和一个不太重要的任务，用来展示如何使用该库。很可能很快就会被移除，一旦我有更有趣和有用的示例要展示，比如模型合并。"
      ],
      "metadata": {
        "id": "aeJbhc2MZ_o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools split-mixtral 65230776370407150546470161412165 \\\n",
        "  models/mistral-7b-instruct-v0.2.Q8_0.gguf models/mistral-7b-instruct-v0.2.Q8_0_out.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFyQgUJyZ-vM",
        "outputId": "d7c6a7b8-197b-4bcc-f2eb-321492e145ee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying general.architecture\n",
            "Copying general.name\n",
            "Copying llama.context_length\n",
            "Copying llama.embedding_length\n",
            "Copying llama.block_count\n",
            "Copying llama.feed_forward_length\n",
            "Copying llama.rope.dimension_count\n",
            "Copying llama.attention.head_count\n",
            "Copying llama.attention.head_count_kv\n",
            "Copying llama.attention.layer_norm_rms_epsilon\n",
            "Copying llama.rope.freq_base\n",
            "Copying general.file_type\n",
            "Copying tokenizer.ggml.model\n",
            "Copying tokenizer.ggml.tokens\n",
            "Copying tokenizer.ggml.scores\n",
            "Copying tokenizer.ggml.token_type\n",
            "Copying tokenizer.ggml.bos_token_id\n",
            "Copying tokenizer.ggml.eos_token_id\n",
            "Copying tokenizer.ggml.unknown_token_id\n",
            "Copying tokenizer.ggml.padding_token_id\n",
            "Copying tokenizer.ggml.add_bos_token\n",
            "Copying tokenizer.ggml.add_eos_token\n",
            "Copying tokenizer.chat_template\n",
            "Copying general.quantization_version\n",
            "Skipping tensor blk.0.ffn_gate.weight\n",
            "Skipping tensor blk.0.ffn_up.weight\n",
            "Skipping tensor blk.0.ffn_down.weight\n",
            "Skipping tensor blk.1.ffn_gate.weight\n",
            "Skipping tensor blk.1.ffn_up.weight\n",
            "Skipping tensor blk.1.ffn_down.weight\n",
            "Skipping tensor blk.2.ffn_gate.weight\n",
            "Skipping tensor blk.2.ffn_up.weight\n",
            "Skipping tensor blk.2.ffn_down.weight\n",
            "Skipping tensor blk.3.ffn_gate.weight\n",
            "Skipping tensor blk.3.ffn_up.weight\n",
            "Skipping tensor blk.3.ffn_down.weight\n",
            "Skipping tensor blk.4.ffn_gate.weight\n",
            "Skipping tensor blk.4.ffn_up.weight\n",
            "Skipping tensor blk.4.ffn_down.weight\n",
            "Skipping tensor blk.5.ffn_gate.weight\n",
            "Skipping tensor blk.5.ffn_up.weight\n",
            "Skipping tensor blk.5.ffn_down.weight\n",
            "Skipping tensor blk.6.ffn_gate.weight\n",
            "Skipping tensor blk.6.ffn_up.weight\n",
            "Skipping tensor blk.6.ffn_down.weight\n",
            "Skipping tensor blk.7.ffn_gate.weight\n",
            "Skipping tensor blk.7.ffn_up.weight\n",
            "Skipping tensor blk.7.ffn_down.weight\n",
            "Skipping tensor blk.8.ffn_gate.weight\n",
            "Skipping tensor blk.8.ffn_up.weight\n",
            "Skipping tensor blk.8.ffn_down.weight\n",
            "Skipping tensor blk.9.ffn_gate.weight\n",
            "Skipping tensor blk.9.ffn_up.weight\n",
            "Skipping tensor blk.9.ffn_down.weight\n",
            "Skipping tensor blk.10.ffn_gate.weight\n",
            "Skipping tensor blk.10.ffn_up.weight\n",
            "Skipping tensor blk.10.ffn_down.weight\n",
            "Skipping tensor blk.11.ffn_gate.weight\n",
            "Skipping tensor blk.11.ffn_up.weight\n",
            "Skipping tensor blk.11.ffn_down.weight\n",
            "Skipping tensor blk.12.ffn_gate.weight\n",
            "Skipping tensor blk.12.ffn_up.weight\n",
            "Skipping tensor blk.12.ffn_down.weight\n",
            "Skipping tensor blk.13.ffn_gate.weight\n",
            "Skipping tensor blk.13.ffn_up.weight\n",
            "Skipping tensor blk.13.ffn_down.weight\n",
            "Skipping tensor blk.14.ffn_gate.weight\n",
            "Skipping tensor blk.14.ffn_up.weight\n",
            "Skipping tensor blk.14.ffn_down.weight\n",
            "Skipping tensor blk.15.ffn_gate.weight\n",
            "Skipping tensor blk.15.ffn_up.weight\n",
            "Skipping tensor blk.15.ffn_down.weight\n",
            "Skipping tensor blk.16.ffn_gate.weight\n",
            "Skipping tensor blk.16.ffn_up.weight\n",
            "Skipping tensor blk.16.ffn_down.weight\n",
            "Skipping tensor blk.17.ffn_gate.weight\n",
            "Skipping tensor blk.17.ffn_up.weight\n",
            "Skipping tensor blk.17.ffn_down.weight\n",
            "Skipping tensor blk.18.ffn_gate.weight\n",
            "Skipping tensor blk.18.ffn_up.weight\n",
            "Skipping tensor blk.18.ffn_down.weight\n",
            "Skipping tensor blk.19.ffn_gate.weight\n",
            "Skipping tensor blk.19.ffn_up.weight\n",
            "Skipping tensor blk.19.ffn_down.weight\n",
            "Skipping tensor blk.20.ffn_gate.weight\n",
            "Skipping tensor blk.20.ffn_up.weight\n",
            "Skipping tensor blk.20.ffn_down.weight\n",
            "Skipping tensor blk.21.ffn_gate.weight\n",
            "Skipping tensor blk.21.ffn_up.weight\n",
            "Skipping tensor blk.21.ffn_down.weight\n",
            "Skipping tensor blk.22.ffn_gate.weight\n",
            "Skipping tensor blk.22.ffn_up.weight\n",
            "Skipping tensor blk.22.ffn_down.weight\n",
            "Skipping tensor blk.23.ffn_gate.weight\n",
            "Skipping tensor blk.23.ffn_up.weight\n",
            "Skipping tensor blk.23.ffn_down.weight\n",
            "Skipping tensor blk.24.ffn_gate.weight\n",
            "Skipping tensor blk.24.ffn_up.weight\n",
            "Skipping tensor blk.24.ffn_down.weight\n",
            "Skipping tensor blk.25.ffn_gate.weight\n",
            "Skipping tensor blk.25.ffn_up.weight\n",
            "Skipping tensor blk.25.ffn_down.weight\n",
            "Skipping tensor blk.26.ffn_gate.weight\n",
            "Skipping tensor blk.26.ffn_up.weight\n",
            "Skipping tensor blk.26.ffn_down.weight\n",
            "Skipping tensor blk.27.ffn_gate.weight\n",
            "Skipping tensor blk.27.ffn_up.weight\n",
            "Skipping tensor blk.27.ffn_down.weight\n",
            "Skipping tensor blk.28.ffn_gate.weight\n",
            "Skipping tensor blk.28.ffn_up.weight\n",
            "Skipping tensor blk.28.ffn_down.weight\n",
            "Skipping tensor blk.29.ffn_gate.weight\n",
            "Skipping tensor blk.29.ffn_up.weight\n",
            "Skipping tensor blk.29.ffn_down.weight\n",
            "Skipping tensor blk.30.ffn_gate.weight\n",
            "Skipping tensor blk.30.ffn_up.weight\n",
            "Skipping tensor blk.30.ffn_down.weight\n",
            "Skipping tensor blk.31.ffn_gate.weight\n",
            "Skipping tensor blk.31.ffn_up.weight\n",
            "Skipping tensor blk.31.ffn_down.weight\n",
            "Output file: after writing tensors info, file size is: 729636\n",
            "Writing tensor token_embd.weight (weights from token_embd.weight)\n",
            "Writing tensor blk.0.attn_q.weight (weights from blk.0.attn_q.weight)\n",
            "Writing tensor blk.0.attn_k.weight (weights from blk.0.attn_k.weight)\n",
            "Writing tensor blk.0.attn_v.weight (weights from blk.0.attn_v.weight)\n",
            "Writing tensor blk.0.attn_output.weight (weights from blk.0.attn_output.weight)\n",
            "Writing tensor blk.0.attn_norm.weight (weights from blk.0.attn_norm.weight)\n",
            "Writing tensor blk.0.ffn_norm.weight (weights from blk.0.ffn_norm.weight)\n",
            "Writing tensor blk.1.attn_q.weight (weights from blk.1.attn_q.weight)\n",
            "Writing tensor blk.1.attn_k.weight (weights from blk.1.attn_k.weight)\n",
            "Writing tensor blk.1.attn_v.weight (weights from blk.1.attn_v.weight)\n",
            "Writing tensor blk.1.attn_output.weight (weights from blk.1.attn_output.weight)\n",
            "Writing tensor blk.1.attn_norm.weight (weights from blk.1.attn_norm.weight)\n",
            "Writing tensor blk.1.ffn_norm.weight (weights from blk.1.ffn_norm.weight)\n",
            "Writing tensor blk.2.attn_q.weight (weights from blk.2.attn_q.weight)\n",
            "Writing tensor blk.2.attn_k.weight (weights from blk.2.attn_k.weight)\n",
            "Writing tensor blk.2.attn_v.weight (weights from blk.2.attn_v.weight)\n",
            "Writing tensor blk.2.attn_output.weight (weights from blk.2.attn_output.weight)\n",
            "Writing tensor blk.2.attn_norm.weight (weights from blk.2.attn_norm.weight)\n",
            "Writing tensor blk.2.ffn_norm.weight (weights from blk.2.ffn_norm.weight)\n",
            "Writing tensor blk.3.attn_q.weight (weights from blk.3.attn_q.weight)\n",
            "Writing tensor blk.3.attn_k.weight (weights from blk.3.attn_k.weight)\n",
            "Writing tensor blk.3.attn_v.weight (weights from blk.3.attn_v.weight)\n",
            "Writing tensor blk.3.attn_output.weight (weights from blk.3.attn_output.weight)\n",
            "Writing tensor blk.3.attn_norm.weight (weights from blk.3.attn_norm.weight)\n",
            "Writing tensor blk.3.ffn_norm.weight (weights from blk.3.ffn_norm.weight)\n",
            "Writing tensor blk.4.attn_q.weight (weights from blk.4.attn_q.weight)\n",
            "Writing tensor blk.4.attn_k.weight (weights from blk.4.attn_k.weight)\n",
            "Writing tensor blk.4.attn_v.weight (weights from blk.4.attn_v.weight)\n",
            "Writing tensor blk.4.attn_output.weight (weights from blk.4.attn_output.weight)\n",
            "Writing tensor blk.4.attn_norm.weight (weights from blk.4.attn_norm.weight)\n",
            "Writing tensor blk.4.ffn_norm.weight (weights from blk.4.ffn_norm.weight)\n",
            "Writing tensor blk.5.attn_q.weight (weights from blk.5.attn_q.weight)\n",
            "Writing tensor blk.5.attn_k.weight (weights from blk.5.attn_k.weight)\n",
            "Writing tensor blk.5.attn_v.weight (weights from blk.5.attn_v.weight)\n",
            "Writing tensor blk.5.attn_output.weight (weights from blk.5.attn_output.weight)\n",
            "Writing tensor blk.5.attn_norm.weight (weights from blk.5.attn_norm.weight)\n",
            "Writing tensor blk.5.ffn_norm.weight (weights from blk.5.ffn_norm.weight)\n",
            "Writing tensor blk.6.attn_q.weight (weights from blk.6.attn_q.weight)\n",
            "Writing tensor blk.6.attn_k.weight (weights from blk.6.attn_k.weight)\n",
            "Writing tensor blk.6.attn_v.weight (weights from blk.6.attn_v.weight)\n",
            "Writing tensor blk.6.attn_output.weight (weights from blk.6.attn_output.weight)\n",
            "Writing tensor blk.6.attn_norm.weight (weights from blk.6.attn_norm.weight)\n",
            "Writing tensor blk.6.ffn_norm.weight (weights from blk.6.ffn_norm.weight)\n",
            "Writing tensor blk.7.attn_q.weight (weights from blk.7.attn_q.weight)\n",
            "Writing tensor blk.7.attn_k.weight (weights from blk.7.attn_k.weight)\n",
            "Writing tensor blk.7.attn_v.weight (weights from blk.7.attn_v.weight)\n",
            "Writing tensor blk.7.attn_output.weight (weights from blk.7.attn_output.weight)\n",
            "Writing tensor blk.7.attn_norm.weight (weights from blk.7.attn_norm.weight)\n",
            "Writing tensor blk.7.ffn_norm.weight (weights from blk.7.ffn_norm.weight)\n",
            "Writing tensor blk.8.attn_q.weight (weights from blk.8.attn_q.weight)\n",
            "Writing tensor blk.8.attn_k.weight (weights from blk.8.attn_k.weight)\n",
            "Writing tensor blk.8.attn_v.weight (weights from blk.8.attn_v.weight)\n",
            "Writing tensor blk.8.attn_output.weight (weights from blk.8.attn_output.weight)\n",
            "Writing tensor blk.8.attn_norm.weight (weights from blk.8.attn_norm.weight)\n",
            "Writing tensor blk.8.ffn_norm.weight (weights from blk.8.ffn_norm.weight)\n",
            "Writing tensor blk.9.attn_q.weight (weights from blk.9.attn_q.weight)\n",
            "Writing tensor blk.9.attn_k.weight (weights from blk.9.attn_k.weight)\n",
            "Writing tensor blk.9.attn_v.weight (weights from blk.9.attn_v.weight)\n",
            "Writing tensor blk.9.attn_output.weight (weights from blk.9.attn_output.weight)\n",
            "Writing tensor blk.9.attn_norm.weight (weights from blk.9.attn_norm.weight)\n",
            "Writing tensor blk.9.ffn_norm.weight (weights from blk.9.ffn_norm.weight)\n",
            "Writing tensor blk.10.attn_q.weight (weights from blk.10.attn_q.weight)\n",
            "Writing tensor blk.10.attn_k.weight (weights from blk.10.attn_k.weight)\n",
            "Writing tensor blk.10.attn_v.weight (weights from blk.10.attn_v.weight)\n",
            "Writing tensor blk.10.attn_output.weight (weights from blk.10.attn_output.weight)\n",
            "Writing tensor blk.10.attn_norm.weight (weights from blk.10.attn_norm.weight)\n",
            "Writing tensor blk.10.ffn_norm.weight (weights from blk.10.ffn_norm.weight)\n",
            "Writing tensor blk.11.attn_q.weight (weights from blk.11.attn_q.weight)\n",
            "Writing tensor blk.11.attn_k.weight (weights from blk.11.attn_k.weight)\n",
            "Writing tensor blk.11.attn_v.weight (weights from blk.11.attn_v.weight)\n",
            "Writing tensor blk.11.attn_output.weight (weights from blk.11.attn_output.weight)\n",
            "Writing tensor blk.11.attn_norm.weight (weights from blk.11.attn_norm.weight)\n",
            "Writing tensor blk.11.ffn_norm.weight (weights from blk.11.ffn_norm.weight)\n",
            "Writing tensor blk.12.attn_q.weight (weights from blk.12.attn_q.weight)\n",
            "Writing tensor blk.12.attn_k.weight (weights from blk.12.attn_k.weight)\n",
            "Writing tensor blk.12.attn_v.weight (weights from blk.12.attn_v.weight)\n",
            "Writing tensor blk.12.attn_output.weight (weights from blk.12.attn_output.weight)\n",
            "Writing tensor blk.12.attn_norm.weight (weights from blk.12.attn_norm.weight)\n",
            "Writing tensor blk.12.ffn_norm.weight (weights from blk.12.ffn_norm.weight)\n",
            "Writing tensor blk.13.attn_q.weight (weights from blk.13.attn_q.weight)\n",
            "Writing tensor blk.13.attn_k.weight (weights from blk.13.attn_k.weight)\n",
            "Writing tensor blk.13.attn_v.weight (weights from blk.13.attn_v.weight)\n",
            "Writing tensor blk.13.attn_output.weight (weights from blk.13.attn_output.weight)\n",
            "Writing tensor blk.13.attn_norm.weight (weights from blk.13.attn_norm.weight)\n",
            "Writing tensor blk.13.ffn_norm.weight (weights from blk.13.ffn_norm.weight)\n",
            "Writing tensor blk.14.attn_q.weight (weights from blk.14.attn_q.weight)\n",
            "Writing tensor blk.14.attn_k.weight (weights from blk.14.attn_k.weight)\n",
            "Writing tensor blk.14.attn_v.weight (weights from blk.14.attn_v.weight)\n",
            "Writing tensor blk.14.attn_output.weight (weights from blk.14.attn_output.weight)\n",
            "Writing tensor blk.14.attn_norm.weight (weights from blk.14.attn_norm.weight)\n",
            "Writing tensor blk.14.ffn_norm.weight (weights from blk.14.ffn_norm.weight)\n",
            "Writing tensor blk.15.attn_q.weight (weights from blk.15.attn_q.weight)\n",
            "Writing tensor blk.15.attn_k.weight (weights from blk.15.attn_k.weight)\n",
            "Writing tensor blk.15.attn_v.weight (weights from blk.15.attn_v.weight)\n",
            "Writing tensor blk.15.attn_output.weight (weights from blk.15.attn_output.weight)\n",
            "Writing tensor blk.15.attn_norm.weight (weights from blk.15.attn_norm.weight)\n",
            "Writing tensor blk.15.ffn_norm.weight (weights from blk.15.ffn_norm.weight)\n",
            "Writing tensor blk.16.attn_q.weight (weights from blk.16.attn_q.weight)\n",
            "Writing tensor blk.16.attn_k.weight (weights from blk.16.attn_k.weight)\n",
            "Writing tensor blk.16.attn_v.weight (weights from blk.16.attn_v.weight)\n",
            "Writing tensor blk.16.attn_output.weight (weights from blk.16.attn_output.weight)\n",
            "Writing tensor blk.16.attn_norm.weight (weights from blk.16.attn_norm.weight)\n",
            "Writing tensor blk.16.ffn_norm.weight (weights from blk.16.ffn_norm.weight)\n",
            "Writing tensor blk.17.attn_q.weight (weights from blk.17.attn_q.weight)\n",
            "Writing tensor blk.17.attn_k.weight (weights from blk.17.attn_k.weight)\n",
            "Writing tensor blk.17.attn_v.weight (weights from blk.17.attn_v.weight)\n",
            "Writing tensor blk.17.attn_output.weight (weights from blk.17.attn_output.weight)\n",
            "Writing tensor blk.17.attn_norm.weight (weights from blk.17.attn_norm.weight)\n",
            "Writing tensor blk.17.ffn_norm.weight (weights from blk.17.ffn_norm.weight)\n",
            "Writing tensor blk.18.attn_q.weight (weights from blk.18.attn_q.weight)\n",
            "Writing tensor blk.18.attn_k.weight (weights from blk.18.attn_k.weight)\n",
            "Writing tensor blk.18.attn_v.weight (weights from blk.18.attn_v.weight)\n",
            "Writing tensor blk.18.attn_output.weight (weights from blk.18.attn_output.weight)\n",
            "Writing tensor blk.18.attn_norm.weight (weights from blk.18.attn_norm.weight)\n",
            "Writing tensor blk.18.ffn_norm.weight (weights from blk.18.ffn_norm.weight)\n",
            "Writing tensor blk.19.attn_q.weight (weights from blk.19.attn_q.weight)\n",
            "Writing tensor blk.19.attn_k.weight (weights from blk.19.attn_k.weight)\n",
            "Writing tensor blk.19.attn_v.weight (weights from blk.19.attn_v.weight)\n",
            "Writing tensor blk.19.attn_output.weight (weights from blk.19.attn_output.weight)\n",
            "Writing tensor blk.19.attn_norm.weight (weights from blk.19.attn_norm.weight)\n",
            "Writing tensor blk.19.ffn_norm.weight (weights from blk.19.ffn_norm.weight)\n",
            "Writing tensor blk.20.attn_q.weight (weights from blk.20.attn_q.weight)\n",
            "Writing tensor blk.20.attn_k.weight (weights from blk.20.attn_k.weight)\n",
            "Writing tensor blk.20.attn_v.weight (weights from blk.20.attn_v.weight)\n",
            "Writing tensor blk.20.attn_output.weight (weights from blk.20.attn_output.weight)\n",
            "Writing tensor blk.20.attn_norm.weight (weights from blk.20.attn_norm.weight)\n",
            "Writing tensor blk.20.ffn_norm.weight (weights from blk.20.ffn_norm.weight)\n",
            "Writing tensor blk.21.attn_q.weight (weights from blk.21.attn_q.weight)\n",
            "Writing tensor blk.21.attn_k.weight (weights from blk.21.attn_k.weight)\n",
            "Writing tensor blk.21.attn_v.weight (weights from blk.21.attn_v.weight)\n",
            "Writing tensor blk.21.attn_output.weight (weights from blk.21.attn_output.weight)\n",
            "Writing tensor blk.21.attn_norm.weight (weights from blk.21.attn_norm.weight)\n",
            "Writing tensor blk.21.ffn_norm.weight (weights from blk.21.ffn_norm.weight)\n",
            "Writing tensor blk.22.attn_q.weight (weights from blk.22.attn_q.weight)\n",
            "Writing tensor blk.22.attn_k.weight (weights from blk.22.attn_k.weight)\n",
            "Writing tensor blk.22.attn_v.weight (weights from blk.22.attn_v.weight)\n",
            "Writing tensor blk.22.attn_output.weight (weights from blk.22.attn_output.weight)\n",
            "Writing tensor blk.22.attn_norm.weight (weights from blk.22.attn_norm.weight)\n",
            "Writing tensor blk.22.ffn_norm.weight (weights from blk.22.ffn_norm.weight)\n",
            "Writing tensor blk.23.attn_q.weight (weights from blk.23.attn_q.weight)\n",
            "Writing tensor blk.23.attn_k.weight (weights from blk.23.attn_k.weight)\n",
            "Writing tensor blk.23.attn_v.weight (weights from blk.23.attn_v.weight)\n",
            "Writing tensor blk.23.attn_output.weight (weights from blk.23.attn_output.weight)\n",
            "Writing tensor blk.23.attn_norm.weight (weights from blk.23.attn_norm.weight)\n",
            "Writing tensor blk.23.ffn_norm.weight (weights from blk.23.ffn_norm.weight)\n",
            "Writing tensor blk.24.attn_q.weight (weights from blk.24.attn_q.weight)\n",
            "Writing tensor blk.24.attn_k.weight (weights from blk.24.attn_k.weight)\n",
            "Writing tensor blk.24.attn_v.weight (weights from blk.24.attn_v.weight)\n",
            "Writing tensor blk.24.attn_output.weight (weights from blk.24.attn_output.weight)\n",
            "Writing tensor blk.24.attn_norm.weight (weights from blk.24.attn_norm.weight)\n",
            "Writing tensor blk.24.ffn_norm.weight (weights from blk.24.ffn_norm.weight)\n",
            "Writing tensor blk.25.attn_q.weight (weights from blk.25.attn_q.weight)\n",
            "Writing tensor blk.25.attn_k.weight (weights from blk.25.attn_k.weight)\n",
            "Writing tensor blk.25.attn_v.weight (weights from blk.25.attn_v.weight)\n",
            "Writing tensor blk.25.attn_output.weight (weights from blk.25.attn_output.weight)\n",
            "Writing tensor blk.25.attn_norm.weight (weights from blk.25.attn_norm.weight)\n",
            "Writing tensor blk.25.ffn_norm.weight (weights from blk.25.ffn_norm.weight)\n",
            "Writing tensor blk.26.attn_q.weight (weights from blk.26.attn_q.weight)\n",
            "Writing tensor blk.26.attn_k.weight (weights from blk.26.attn_k.weight)\n",
            "Writing tensor blk.26.attn_v.weight (weights from blk.26.attn_v.weight)\n",
            "Writing tensor blk.26.attn_output.weight (weights from blk.26.attn_output.weight)\n",
            "Writing tensor blk.26.attn_norm.weight (weights from blk.26.attn_norm.weight)\n",
            "Writing tensor blk.26.ffn_norm.weight (weights from blk.26.ffn_norm.weight)\n",
            "Writing tensor blk.27.attn_q.weight (weights from blk.27.attn_q.weight)\n",
            "Writing tensor blk.27.attn_k.weight (weights from blk.27.attn_k.weight)\n",
            "Writing tensor blk.27.attn_v.weight (weights from blk.27.attn_v.weight)\n",
            "Writing tensor blk.27.attn_output.weight (weights from blk.27.attn_output.weight)\n",
            "Writing tensor blk.27.attn_norm.weight (weights from blk.27.attn_norm.weight)\n",
            "Writing tensor blk.27.ffn_norm.weight (weights from blk.27.ffn_norm.weight)\n",
            "Writing tensor blk.28.attn_q.weight (weights from blk.28.attn_q.weight)\n",
            "Writing tensor blk.28.attn_k.weight (weights from blk.28.attn_k.weight)\n",
            "Writing tensor blk.28.attn_v.weight (weights from blk.28.attn_v.weight)\n",
            "Writing tensor blk.28.attn_output.weight (weights from blk.28.attn_output.weight)\n",
            "Writing tensor blk.28.attn_norm.weight (weights from blk.28.attn_norm.weight)\n",
            "Writing tensor blk.28.ffn_norm.weight (weights from blk.28.ffn_norm.weight)\n",
            "Writing tensor blk.29.attn_q.weight (weights from blk.29.attn_q.weight)\n",
            "Writing tensor blk.29.attn_k.weight (weights from blk.29.attn_k.weight)\n",
            "Writing tensor blk.29.attn_v.weight (weights from blk.29.attn_v.weight)\n",
            "Writing tensor blk.29.attn_output.weight (weights from blk.29.attn_output.weight)\n",
            "Writing tensor blk.29.attn_norm.weight (weights from blk.29.attn_norm.weight)\n",
            "Writing tensor blk.29.ffn_norm.weight (weights from blk.29.ffn_norm.weight)\n",
            "Writing tensor blk.30.attn_q.weight (weights from blk.30.attn_q.weight)\n",
            "Writing tensor blk.30.attn_k.weight (weights from blk.30.attn_k.weight)\n",
            "Writing tensor blk.30.attn_v.weight (weights from blk.30.attn_v.weight)\n",
            "Writing tensor blk.30.attn_output.weight (weights from blk.30.attn_output.weight)\n",
            "Writing tensor blk.30.attn_norm.weight (weights from blk.30.attn_norm.weight)\n",
            "Writing tensor blk.30.ffn_norm.weight (weights from blk.30.ffn_norm.weight)\n",
            "Writing tensor blk.31.attn_q.weight (weights from blk.31.attn_q.weight)\n",
            "Writing tensor blk.31.attn_k.weight (weights from blk.31.attn_k.weight)\n",
            "Writing tensor blk.31.attn_v.weight (weights from blk.31.attn_v.weight)\n",
            "Writing tensor blk.31.attn_output.weight (weights from blk.31.attn_output.weight)\n",
            "Writing tensor blk.31.attn_norm.weight (weights from blk.31.attn_norm.weight)\n",
            "Writing tensor blk.31.ffn_norm.weight (weights from blk.31.ffn_norm.weight)\n",
            "Writing tensor output_norm.weight (weights from output_norm.weight)\n",
            "Writing tensor output.weight (weights from output.weight)\n"
          ]
        }
      ]
    }
  ]
}