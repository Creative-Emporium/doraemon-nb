{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMCHDpRV1AUC88eajzwf4hC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/LLM_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://github.com/ggerganov/ggml/blob/master/docs/gguf.md gguf规范文档\n",
        "- https://github.com/weedge/ggml/blob/master/docs/gguf-cn.md gguf规范中文版\n",
        "- [用于量化 GGUF 模型的量化格式](https://github.com/ggerganov/ggml/blob/master/src/ggml-quants.h)。\n"
      ],
      "metadata": {
        "id": "pkuU6RdkGXF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GGUF 工具\n"
      ],
      "metadata": {
        "id": "ccyoyfJjNLj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H8kqOupGEdq",
        "outputId": "196d2318-1902-497e-f507-e9d6f385c3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gguf-tools'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 136 (delta 81), reused 102 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (136/136), 59.25 KiB | 4.94 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/antirez/gguf-tools.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gguf-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayvpmx1vOMcY",
        "outputId": "3559e923-c254-481b-98ef-9ee07f067671"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gguf-tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnZzswceOO3I",
        "outputId": "5ec8549e-db4d-4b69-c9b0-2a0d3f00055a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cc gguf-tools.c gguflib.c sds.c fp16.c \\\n",
            "\t-march=native -flto -ffast-math \\\n",
            "\t-g -ggdb -Wall -W -pedantic -O3 -o gguf-tools\n",
            "\u001b[01m\u001b[Kgguf-tools.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgguf_tools_show\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgguf-tools.c:169:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  169 |         printf(\"%s tensor %.*s @\u001b[01;35m\u001b[K%llu\u001b[m\u001b[K, %llu weights, %llu bytes\\n\",\n",
            "      |                                 \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                    \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                    \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "......\n",
            "  173 |             \u001b[32m\u001b[Ktensor.offset\u001b[m\u001b[K,\n",
            "      |             \u001b[32m\u001b[K~~~~~~~~~~~~~\u001b[m\u001b[K           \n",
            "      |                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguf-tools.c:169:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 6 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  169 |         printf(\"%s tensor %.*s @%llu, \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K weights, %llu bytes\\n\",\n",
            "      |                                       \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                          \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                       \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "......\n",
            "  174 |             \u001b[32m\u001b[Ktensor.num_weights\u001b[m\u001b[K,\n",
            "      |             \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K            \n",
            "      |                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguf-tools.c:169:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 7 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  169 |         printf(\"%s tensor %.*s @%llu, %llu weights, \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K bytes\\n\",\n",
            "      |                                                     \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                        \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                                     \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "......\n",
            "  175 |             \u001b[32m\u001b[Ktensor.bsize\u001b[m\u001b[K);\n",
            "      |             \u001b[32m\u001b[K~~~~~~~~~~~~\u001b[m\u001b[K                                \n",
            "      |                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguf-tools.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgguf_tools_split_mixtral\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgguf-tools.c:311:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  311 |     printf(\"Output file: after writing tensors info, file size is: \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K\\n\", \u001b[32m\u001b[Koutput->size\u001b[m\u001b[K);\n",
            "      |                                                                    \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K     \u001b[32m\u001b[K~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K           \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                                                    \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgguf_print_value_callback\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgguflib.c:362:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  362 |             printf(\"... \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K more items of %llu\", \u001b[32m\u001b[Karray_len-in_array+1\u001b[m\u001b[K,\n",
            "      |                         \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                      \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                            \u001b[01;35m\u001b[K|\u001b[m\u001b[K                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                            \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "      |                         \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:362:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  362 |             printf(\"... %llu more items of \u001b[01;35m\u001b[K%llu\u001b[m\u001b[K\", array_len-in_array+1,\n",
            "      |                                            \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                                            \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "  363 |                                                   \u001b[32m\u001b[Karray_len\u001b[m\u001b[K);\n",
            "      |                                                   \u001b[32m\u001b[K~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                   \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                   \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:395:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%llu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long unsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kuint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  395 |             printf(\"\u001b[01;35m\u001b[K%llu\u001b[m\u001b[K\", \u001b[32m\u001b[Kval->uint64\u001b[m\u001b[K); break;\n",
            "      |                     \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K   \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[Kuint64_t {aka long unsigned int}\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[Klong long unsigned int\u001b[m\u001b[K\n",
            "      |                     \u001b[32m\u001b[K%lu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgguflib.c:397:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat=\u0007-Wformat=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  397 |             printf(\"\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K\", \u001b[32m\u001b[Kval->int64\u001b[m\u001b[K); break;\n",
            "      |                     \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K   \u001b[32m\u001b[K~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[K|\u001b[m\u001b[K      \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "      |                        \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                     \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7qKB8JbRlsH",
        "outputId": "a3c8ab42-4c23-4476-caea-02698fefc595"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/phi-2-GGUF phi-2.Q8_0.gguf --local-dir ./models --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I6tuSujRoQv",
        "outputId": "6eda09ab-c5dd-4b70-bdcb-0ce1925bd2af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q8_0.gguf to /root/.cache/huggingface/hub/tmp8bsvbj_7\n",
            "phi-2.Q8_0.gguf: 100% 2.96G/2.96G [00:11<00:00, 250MB/s]\n",
            "./models/phi-2.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF --include='*Q4_K*gguf' --local-dir gguf-tools/models/ --local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "N-Ak8cPy6-zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools show file.gguf\n",
        "显示有关 GGUF 文件的详细信息。这将包括所有键值对，包括数组和详细的张量信息。张量的偏移量将相对于文件的开头（因此它们实际上是绝对偏移量），而不是像 GGUF 格式中的数据部分的开头。\n",
        "\n"
      ],
      "metadata": {
        "id": "UszNqIMVTqZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools show models/llama-2-7b-chat.Q4_K_M.gguf | head -100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFjqyRms9pTE",
        "outputId": "ec372ffd-4e5d-4467-df8c-ae277efea3dd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/llama-2-7b-chat.Q4_K_M.gguf (ver 2): 19 key-value pairs, 291 tensors\n",
            "general.architecture: [string] llama\n",
            "general.name: [string] LLaMA v2\n",
            "llama.context_length: [uint32] 4096\n",
            "llama.embedding_length: [uint32] 4096\n",
            "llama.block_count: [uint32] 32\n",
            "llama.feed_forward_length: [uint32] 11008\n",
            "llama.rope.dimension_count: [uint32] 128\n",
            "llama.attention.head_count: [uint32] 32\n",
            "llama.attention.head_count_kv: [uint32] 32\n",
            "llama.attention.layer_norm_rms_epsilon: [float32] 0.000001\n",
            "general.file_type: [uint32] 15\n",
            "tokenizer.ggml.model: [string] llama\n",
            "tokenizer.ggml.tokens: [array] [<unk>, <s>, </s>, <0x00>, <0x01>, <0x02>, <0x03>, <0x04>, <0x05>, <0x06>, <0x07>, <0x08>, <0x09>, <0x0A>, <0x0B>, <0x0C>, <0x0D>, <0x0E>, <0x0F>, <0x10>, <0x11>, <0x12>, <0x13>, <0x14>, <0x15>, <0x16>, <0x17>, <0x18>, <0x19>, <0x1A>, ... 31970 more items of 32000]\n",
            "tokenizer.ggml.scores: [array] [0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, ... 31970 more items of 32000]\n",
            "tokenizer.ggml.token_type: [array] [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... 31970 more items of 32000]\n",
            "tokenizer.ggml.bos_token_id: [uint32] 1\n",
            "tokenizer.ggml.eos_token_id: [uint32] 2\n",
            "tokenizer.ggml.unknown_token_id: [uint32] 0\n",
            "general.quantization_version: [uint32] 2\n",
            "q4_k tensor token_embd.weight @741056, 131072000 weights, 73728000 bytes\n",
            "f32 tensor blk.0.attn_norm.weight @74469056, 4096 weights, 16384 bytes\n",
            "q6_k tensor blk.0.ffn_down.weight @74485440, 45088768 weights, 36986880 bytes\n",
            "q4_k tensor blk.0.ffn_gate.weight @111472320, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.0.ffn_up.weight @136834752, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.0.ffn_norm.weight @162197184, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.0.attn_k.weight @162213568, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.0.attn_output.weight @171650752, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.0.attn_q.weight @181087936, 16777216 weights, 9437184 bytes\n",
            "q6_k tensor blk.0.attn_v.weight @190525120, 16777216 weights, 13762560 bytes\n",
            "f32 tensor blk.1.attn_norm.weight @204287680, 4096 weights, 16384 bytes\n",
            "q6_k tensor blk.1.ffn_down.weight @204304064, 45088768 weights, 36986880 bytes\n",
            "q4_k tensor blk.1.ffn_gate.weight @241290944, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.1.ffn_up.weight @266653376, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.1.ffn_norm.weight @292015808, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.1.attn_k.weight @292032192, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.1.attn_output.weight @301469376, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.1.attn_q.weight @310906560, 16777216 weights, 9437184 bytes\n",
            "q6_k tensor blk.1.attn_v.weight @320343744, 16777216 weights, 13762560 bytes\n",
            "f32 tensor blk.10.attn_norm.weight @334106304, 4096 weights, 16384 bytes\n",
            "q6_k tensor blk.10.ffn_down.weight @334122688, 45088768 weights, 36986880 bytes\n",
            "q4_k tensor blk.10.ffn_gate.weight @371109568, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.10.ffn_up.weight @396472000, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.10.ffn_norm.weight @421834432, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.10.attn_k.weight @421850816, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.10.attn_output.weight @431288000, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.10.attn_q.weight @440725184, 16777216 weights, 9437184 bytes\n",
            "q6_k tensor blk.10.attn_v.weight @450162368, 16777216 weights, 13762560 bytes\n",
            "f32 tensor blk.11.attn_norm.weight @463924928, 4096 weights, 16384 bytes\n",
            "q6_k tensor blk.11.ffn_down.weight @463941312, 45088768 weights, 36986880 bytes\n",
            "q4_k tensor blk.11.ffn_gate.weight @500928192, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.11.ffn_up.weight @526290624, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.11.ffn_norm.weight @551653056, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.11.attn_k.weight @551669440, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.11.attn_output.weight @561106624, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.11.attn_q.weight @570543808, 16777216 weights, 9437184 bytes\n",
            "q6_k tensor blk.11.attn_v.weight @579980992, 16777216 weights, 13762560 bytes\n",
            "f32 tensor blk.12.attn_norm.weight @593743552, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.12.ffn_down.weight @593759936, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.12.ffn_gate.weight @619122368, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.12.ffn_up.weight @644484800, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.12.ffn_norm.weight @669847232, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.12.attn_k.weight @669863616, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.12.attn_output.weight @679300800, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.12.attn_q.weight @688737984, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.12.attn_v.weight @698175168, 16777216 weights, 9437184 bytes\n",
            "f32 tensor blk.13.attn_norm.weight @707612352, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.13.ffn_down.weight @707628736, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.13.ffn_gate.weight @732991168, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.13.ffn_up.weight @758353600, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.13.ffn_norm.weight @783716032, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.13.attn_k.weight @783732416, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.13.attn_output.weight @793169600, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.13.attn_q.weight @802606784, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.13.attn_v.weight @812043968, 16777216 weights, 9437184 bytes\n",
            "f32 tensor blk.14.attn_norm.weight @821481152, 4096 weights, 16384 bytes\n",
            "q6_k tensor blk.14.ffn_down.weight @821497536, 45088768 weights, 36986880 bytes\n",
            "q4_k tensor blk.14.ffn_gate.weight @858484416, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.14.ffn_up.weight @883846848, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.14.ffn_norm.weight @909209280, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.14.attn_k.weight @909225664, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.14.attn_output.weight @918662848, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.14.attn_q.weight @928100032, 16777216 weights, 9437184 bytes\n",
            "q6_k tensor blk.14.attn_v.weight @937537216, 16777216 weights, 13762560 bytes\n",
            "f32 tensor blk.15.attn_norm.weight @951299776, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.15.ffn_down.weight @951316160, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.15.ffn_gate.weight @976678592, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.15.ffn_up.weight @1002041024, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.15.ffn_norm.weight @1027403456, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.15.attn_k.weight @1027419840, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.15.attn_output.weight @1036857024, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.15.attn_q.weight @1046294208, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.15.attn_v.weight @1055731392, 16777216 weights, 9437184 bytes\n",
            "f32 tensor blk.16.attn_norm.weight @1065168576, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.16.ffn_down.weight @1065184960, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.16.ffn_gate.weight @1090547392, 45088768 weights, 25362432 bytes\n",
            "q4_k tensor blk.16.ffn_up.weight @1115909824, 45088768 weights, 25362432 bytes\n",
            "f32 tensor blk.16.ffn_norm.weight @1141272256, 4096 weights, 16384 bytes\n",
            "q4_k tensor blk.16.attn_k.weight @1141288640, 16777216 weights, 9437184 bytes\n",
            "q4_k tensor blk.16.attn_output.weight @1150725824, 16777216 weights, 9437184 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools show models/phi-2.Q8_0.gguf | head -100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF6p-epxSEik",
        "outputId": "e3316433-3ed5-46cc-bdd2-d77b3d3c12c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/phi-2.Q8_0.gguf (ver 3): 20 key-value pairs, 325 tensors\n",
            "general.architecture: [string] phi2\n",
            "general.name: [string] Phi2\n",
            "phi2.context_length: [uint32] 2048\n",
            "phi2.embedding_length: [uint32] 2560\n",
            "phi2.feed_forward_length: [uint32] 10240\n",
            "phi2.block_count: [uint32] 32\n",
            "phi2.attention.head_count: [uint32] 32\n",
            "phi2.attention.head_count_kv: [uint32] 32\n",
            "phi2.attention.layer_norm_epsilon: [float32] 0.000010\n",
            "phi2.rope.dimension_count: [uint32] 32\n",
            "general.file_type: [uint32] 7\n",
            "tokenizer.ggml.add_bos_token: [bool] false\n",
            "tokenizer.ggml.model: [string] gpt2\n",
            "tokenizer.ggml.tokens: [array] [!, \", #, $, %, &, ', (, ), *, +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, <, =, >, ... 51170 more items of 51200]\n",
            "tokenizer.ggml.token_type: [array] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... 51170 more items of 51200]\n",
            "tokenizer.ggml.merges: [array] [Ġ t, Ġ a, h e, i n, r e, o n, Ġt he, e r, Ġ s, a t, Ġ w, Ġ o, e n, Ġ c, i t, i s, a n, o r, e s, Ġ b, e d, Ġ f, in g, Ġ p, o u, Ġa n, a l, a r, Ġt o, Ġ m, ... 49970 more items of 50000]\n",
            "tokenizer.ggml.bos_token_id: [uint32] 50256\n",
            "tokenizer.ggml.eos_token_id: [uint32] 50256\n",
            "tokenizer.ggml.unknown_token_id: [uint32] 50256\n",
            "general.quantization_version: [uint32] 2\n",
            "q8_0 tensor token_embd.weight @1806176, 131072000 weights, 139264000 bytes\n",
            "f32 tensor blk.0.attn_norm.bias @141070176, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.0.attn_norm.weight @141080416, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.0.attn_qkv.bias @141090656, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.0.attn_qkv.weight @141121376, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.0.attn_output.bias @162010976, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.0.attn_output.weight @162021216, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.0.ffn_up.bias @168984416, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.0.ffn_up.weight @169025376, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.0.ffn_down.bias @196878176, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.0.ffn_down.weight @196888416, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.1.attn_norm.bias @224741216, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.1.attn_norm.weight @224751456, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.1.attn_qkv.bias @224761696, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.1.attn_qkv.weight @224792416, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.1.attn_output.bias @245682016, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.1.attn_output.weight @245692256, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.1.ffn_up.bias @252655456, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.1.ffn_up.weight @252696416, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.1.ffn_down.bias @280549216, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.1.ffn_down.weight @280559456, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.10.attn_norm.bias @308412256, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.10.attn_norm.weight @308422496, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.10.attn_qkv.bias @308432736, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.10.attn_qkv.weight @308463456, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.10.attn_output.bias @329353056, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.10.attn_output.weight @329363296, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.10.ffn_up.bias @336326496, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.10.ffn_up.weight @336367456, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.10.ffn_down.bias @364220256, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.10.ffn_down.weight @364230496, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.11.attn_norm.bias @392083296, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.11.attn_norm.weight @392093536, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.11.attn_qkv.bias @392103776, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.11.attn_qkv.weight @392134496, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.11.attn_output.bias @413024096, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.11.attn_output.weight @413034336, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.11.ffn_up.bias @419997536, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.11.ffn_up.weight @420038496, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.11.ffn_down.bias @447891296, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.11.ffn_down.weight @447901536, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.12.attn_norm.bias @475754336, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.12.attn_norm.weight @475764576, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.12.attn_qkv.bias @475774816, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.12.attn_qkv.weight @475805536, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.12.attn_output.bias @496695136, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.12.attn_output.weight @496705376, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.12.ffn_up.bias @503668576, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.12.ffn_up.weight @503709536, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.12.ffn_down.bias @531562336, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.12.ffn_down.weight @531572576, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.13.attn_norm.bias @559425376, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.13.attn_norm.weight @559435616, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.13.attn_qkv.bias @559445856, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.13.attn_qkv.weight @559476576, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.13.attn_output.bias @580366176, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.13.attn_output.weight @580376416, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.13.ffn_up.bias @587339616, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.13.ffn_up.weight @587380576, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.13.ffn_down.bias @615233376, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.13.ffn_down.weight @615243616, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.14.attn_norm.bias @643096416, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.14.attn_norm.weight @643106656, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.14.attn_qkv.bias @643116896, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.14.attn_qkv.weight @643147616, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.14.attn_output.bias @664037216, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.14.attn_output.weight @664047456, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.14.ffn_up.bias @671010656, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.14.ffn_up.weight @671051616, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.14.ffn_down.bias @698904416, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.14.ffn_down.weight @698914656, 26214400 weights, 27852800 bytes\n",
            "f32 tensor blk.15.attn_norm.bias @726767456, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.15.attn_norm.weight @726777696, 2560 weights, 10240 bytes\n",
            "f32 tensor blk.15.attn_qkv.bias @726787936, 7680 weights, 30720 bytes\n",
            "q8_0 tensor blk.15.attn_qkv.weight @726818656, 19660800 weights, 20889600 bytes\n",
            "f32 tensor blk.15.attn_output.bias @747708256, 2560 weights, 10240 bytes\n",
            "q8_0 tensor blk.15.attn_output.weight @747718496, 6553600 weights, 6963200 bytes\n",
            "f32 tensor blk.15.ffn_up.bias @754681696, 10240 weights, 40960 bytes\n",
            "q8_0 tensor blk.15.ffn_up.weight @754722656, 26214400 weights, 27852800 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools compare file1.gguf file2.gguf\n",
        "此工具有助于了解两个 LLM（或其他以 GGUF 文件形式分发的模型）是否相关，例如一个是否是另一个的微调，或者两者是否都是从同一个父模型进行微调的。\n",
        "\n",
        "对于每个匹配的张量（名称和参数数量相同），该命令计算平均权重差异（以百分比表示，因此在 -N 到 +N 范围内的随机分布平均上与另一个随机分布相比较将达到 100% 差异）。这有助于查看模型是否是另一个模型的微调版本，它被微调了多少，哪些层在微调时被冻结等。请注意，由于量化，即使功能上等效的张量可能也存在一些小的平均差异。\n",
        "\n",
        "示例输出：\n"
      ],
      "metadata": {
        "id": "Pj3kCAfcTyD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q8_0.gguf \\\n",
        "  --local-dir ./models --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYp6BgRaTxgp",
        "outputId": "5440b844-d87d-47d7-b273-16030af95044"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf to /root/.cache/huggingface/hub/tmpexfcef09\n",
            "mistral-7b-instruct-v0.2.Q8_0.gguf: 100% 7.70G/7.70G [00:28<00:00, 273MB/s]\n",
            "./models/mistral-7b-instruct-v0.2.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf \\\n",
        "  --local-dir ./models --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2DuYxFgUQ9L",
        "outputId": "a94b4e9d-c454-4372-b1a8-73adf7bbffa6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF/resolve/main/solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf to /root/.cache/huggingface/hub/tmp6ymjwy_8\n",
            "(…)10.7b-instruct-v1.0-uncensored.Q8_0.gguf: 100% 11.4G/11.4G [03:57<00:00, 48.0MB/s]\n",
            "./models/solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools compare models/mistral-7b-instruct-v0.2.Q8_0.gguf models/solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPKCQjTSWKUv",
        "outputId": "96e28df7-1e9c-4ff0-fc40-d24601e33242"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[token_embd.weight]: avg weights difference: 44.539944%\n",
            "[blk.0.attn_q.weight]: avg weights difference: 48.717736%\n",
            "[blk.0.attn_k.weight]: avg weights difference: 56.201885%\n",
            "[blk.0.attn_v.weight]: avg weights difference: 47.087249%\n",
            "[blk.0.attn_output.weight]: avg weights difference: 47.663048%\n",
            "[blk.0.ffn_gate.weight]: avg weights difference: 37.508761%\n",
            "[blk.0.ffn_up.weight]: avg weights difference: 39.061584%\n",
            "[blk.0.ffn_down.weight]: avg weights difference: 39.632648%\n",
            "[blk.0.attn_norm.weight]: avg weights difference: 3.997229%\n",
            "[blk.0.ffn_norm.weight]: avg weights difference: 5.686371%\n",
            "[blk.1.attn_q.weight]: avg weights difference: 47.318189%\n",
            "[blk.1.attn_k.weight]: avg weights difference: 44.031983%\n",
            "[blk.1.attn_v.weight]: avg weights difference: 36.303868%\n",
            "[blk.1.attn_output.weight]: avg weights difference: 40.389659%\n",
            "[blk.1.ffn_gate.weight]: avg weights difference: 38.658373%\n",
            "[blk.1.ffn_up.weight]: avg weights difference: 38.838259%\n",
            "[blk.1.ffn_down.weight]: avg weights difference: 37.840668%\n",
            "[blk.1.attn_norm.weight]: avg weights difference: 2.211642%\n",
            "[blk.1.ffn_norm.weight]: avg weights difference: 2.871902%\n",
            "[blk.2.attn_q.weight]: avg weights difference: 37.346753%\n",
            "[blk.2.attn_k.weight]: avg weights difference: 31.213910%\n",
            "[blk.2.attn_v.weight]: avg weights difference: 40.192856%\n",
            "[blk.2.attn_output.weight]: avg weights difference: 43.092701%\n",
            "[blk.2.ffn_gate.weight]: avg weights difference: 39.479107%\n",
            "[blk.2.ffn_up.weight]: avg weights difference: 39.526048%\n",
            "[blk.2.ffn_down.weight]: avg weights difference: 37.368479%\n",
            "[blk.2.attn_norm.weight]: avg weights difference: 2.445631%\n",
            "[blk.2.ffn_norm.weight]: avg weights difference: 2.347484%\n",
            "[blk.3.attn_q.weight]: avg weights difference: 38.638426%\n",
            "[blk.3.attn_k.weight]: avg weights difference: 31.908332%\n",
            "[blk.3.attn_v.weight]: avg weights difference: 32.967993%\n",
            "[blk.3.attn_output.weight]: avg weights difference: 37.274029%\n",
            "[blk.3.ffn_gate.weight]: avg weights difference: 38.899170%\n",
            "[blk.3.ffn_up.weight]: avg weights difference: 38.840248%\n",
            "[blk.3.ffn_down.weight]: avg weights difference: 37.489862%\n",
            "[blk.3.attn_norm.weight]: avg weights difference: 2.489361%\n",
            "[blk.3.ffn_norm.weight]: avg weights difference: 2.062806%\n",
            "[blk.4.attn_q.weight]: avg weights difference: 38.505135%\n",
            "[blk.4.attn_k.weight]: avg weights difference: 32.128014%\n",
            "[blk.4.attn_v.weight]: avg weights difference: 35.495024%\n",
            "[blk.4.attn_output.weight]: avg weights difference: 38.984788%\n",
            "[blk.4.ffn_gate.weight]: avg weights difference: 39.439534%\n",
            "[blk.4.ffn_up.weight]: avg weights difference: 38.947228%\n",
            "[blk.4.ffn_down.weight]: avg weights difference: 37.219814%\n",
            "[blk.4.attn_norm.weight]: avg weights difference: 2.662142%\n",
            "[blk.4.ffn_norm.weight]: avg weights difference: 1.997403%\n",
            "[blk.5.attn_q.weight]: avg weights difference: 37.021648%\n",
            "[blk.5.attn_k.weight]: avg weights difference: 30.047030%\n",
            "[blk.5.attn_v.weight]: avg weights difference: 33.831442%\n",
            "[blk.5.attn_output.weight]: avg weights difference: 36.671130%\n",
            "[blk.5.ffn_gate.weight]: avg weights difference: 38.709555%\n",
            "[blk.5.ffn_up.weight]: avg weights difference: 38.618665%\n",
            "[blk.5.ffn_down.weight]: avg weights difference: 36.862574%\n",
            "[blk.5.attn_norm.weight]: avg weights difference: 2.590642%\n",
            "[blk.5.ffn_norm.weight]: avg weights difference: 2.106001%\n",
            "[blk.6.attn_q.weight]: avg weights difference: 38.177590%\n",
            "[blk.6.attn_k.weight]: avg weights difference: 30.757326%\n",
            "[blk.6.attn_v.weight]: avg weights difference: 34.844417%\n",
            "[blk.6.attn_output.weight]: avg weights difference: 36.794619%\n",
            "[blk.6.ffn_gate.weight]: avg weights difference: 38.055215%\n",
            "[blk.6.ffn_up.weight]: avg weights difference: 38.183776%\n",
            "[blk.6.ffn_down.weight]: avg weights difference: 36.505638%\n",
            "[blk.6.attn_norm.weight]: avg weights difference: 2.735638%\n",
            "[blk.6.ffn_norm.weight]: avg weights difference: 2.089956%\n",
            "[blk.7.attn_q.weight]: avg weights difference: 37.813109%\n",
            "[blk.7.attn_k.weight]: avg weights difference: 30.317938%\n",
            "[blk.7.attn_v.weight]: avg weights difference: 31.793105%\n",
            "[blk.7.attn_output.weight]: avg weights difference: 33.939902%\n",
            "[blk.7.ffn_gate.weight]: avg weights difference: 37.719086%\n",
            "[blk.7.ffn_up.weight]: avg weights difference: 37.919256%\n",
            "[blk.7.ffn_down.weight]: avg weights difference: 36.205047%\n",
            "[blk.7.attn_norm.weight]: avg weights difference: 2.749280%\n",
            "[blk.7.ffn_norm.weight]: avg weights difference: 2.179731%\n",
            "[blk.8.attn_q.weight]: avg weights difference: 37.418863%\n",
            "[blk.8.attn_k.weight]: avg weights difference: 30.703674%\n",
            "[blk.8.attn_v.weight]: avg weights difference: 31.421675%\n",
            "[blk.8.attn_output.weight]: avg weights difference: 33.060101%\n",
            "[blk.8.ffn_gate.weight]: avg weights difference: 37.799261%\n",
            "[blk.8.ffn_up.weight]: avg weights difference: 37.859680%\n",
            "[blk.8.ffn_down.weight]: avg weights difference: 36.377553%\n",
            "[blk.8.attn_norm.weight]: avg weights difference: 2.681746%\n",
            "[blk.8.ffn_norm.weight]: avg weights difference: 2.241802%\n",
            "[blk.9.attn_q.weight]: avg weights difference: 37.823661%\n",
            "[blk.9.attn_k.weight]: avg weights difference: 30.615400%\n",
            "[blk.9.attn_v.weight]: avg weights difference: 33.012580%\n",
            "[blk.9.attn_output.weight]: avg weights difference: 33.388703%\n",
            "[blk.9.ffn_gate.weight]: avg weights difference: 38.211512%\n",
            "[blk.9.ffn_up.weight]: avg weights difference: 37.727899%\n",
            "[blk.9.ffn_down.weight]: avg weights difference: 36.666438%\n",
            "[blk.9.attn_norm.weight]: avg weights difference: 2.643698%\n",
            "[blk.9.ffn_norm.weight]: avg weights difference: 2.216395%\n",
            "[blk.10.attn_q.weight]: avg weights difference: 37.259335%\n",
            "[blk.10.attn_k.weight]: avg weights difference: 29.990339%\n",
            "[blk.10.attn_v.weight]: avg weights difference: 31.883167%\n",
            "[blk.10.attn_output.weight]: avg weights difference: 32.274127%\n",
            "[blk.10.ffn_gate.weight]: avg weights difference: 38.649153%\n",
            "[blk.10.ffn_up.weight]: avg weights difference: 37.996440%\n",
            "[blk.10.ffn_down.weight]: avg weights difference: 36.955804%\n",
            "[blk.10.attn_norm.weight]: avg weights difference: 2.558892%\n",
            "[blk.10.ffn_norm.weight]: avg weights difference: 2.241721%\n",
            "[blk.11.attn_q.weight]: avg weights difference: 40.012016%\n",
            "[blk.11.attn_k.weight]: avg weights difference: 32.131927%\n",
            "[blk.11.attn_v.weight]: avg weights difference: 29.644739%\n",
            "[blk.11.attn_output.weight]: avg weights difference: 33.145932%\n",
            "[blk.11.ffn_gate.weight]: avg weights difference: 38.862092%\n",
            "[blk.11.ffn_up.weight]: avg weights difference: 37.991157%\n",
            "[blk.11.ffn_down.weight]: avg weights difference: 36.996582%\n",
            "[blk.11.attn_norm.weight]: avg weights difference: 2.607966%\n",
            "[blk.11.ffn_norm.weight]: avg weights difference: 2.244699%\n",
            "[blk.12.attn_q.weight]: avg weights difference: 38.613124%\n",
            "[blk.12.attn_k.weight]: avg weights difference: 30.781870%\n",
            "[blk.12.attn_v.weight]: avg weights difference: 30.662009%\n",
            "[blk.12.attn_output.weight]: avg weights difference: 32.891754%\n",
            "[blk.12.ffn_gate.weight]: avg weights difference: 39.279775%\n",
            "[blk.12.ffn_up.weight]: avg weights difference: 37.981480%\n",
            "[blk.12.ffn_down.weight]: avg weights difference: 37.473172%\n",
            "[blk.12.attn_norm.weight]: avg weights difference: 2.628979%\n",
            "[blk.12.ffn_norm.weight]: avg weights difference: 2.275894%\n",
            "[blk.13.attn_q.weight]: avg weights difference: 38.039374%\n",
            "[blk.13.attn_k.weight]: avg weights difference: 30.561683%\n",
            "[blk.13.attn_v.weight]: avg weights difference: 28.995724%\n",
            "[blk.13.attn_output.weight]: avg weights difference: 31.145505%\n",
            "[blk.13.ffn_gate.weight]: avg weights difference: 39.723644%\n",
            "[blk.13.ffn_up.weight]: avg weights difference: 38.100503%\n",
            "[blk.13.ffn_down.weight]: avg weights difference: 38.059809%\n",
            "[blk.13.attn_norm.weight]: avg weights difference: 2.490819%\n",
            "[blk.13.ffn_norm.weight]: avg weights difference: 2.363987%\n",
            "[blk.14.attn_q.weight]: avg weights difference: 40.472187%\n",
            "[blk.14.attn_k.weight]: avg weights difference: 32.930238%\n",
            "[blk.14.attn_v.weight]: avg weights difference: 27.300996%\n",
            "[blk.14.attn_output.weight]: avg weights difference: 31.894562%\n",
            "[blk.14.ffn_gate.weight]: avg weights difference: 41.136303%\n",
            "[blk.14.ffn_up.weight]: avg weights difference: 39.108596%\n",
            "[blk.14.ffn_down.weight]: avg weights difference: 38.341792%\n",
            "[blk.14.attn_norm.weight]: avg weights difference: 2.538121%\n",
            "[blk.14.ffn_norm.weight]: avg weights difference: 2.551069%\n",
            "[blk.15.attn_q.weight]: avg weights difference: 40.611837%\n",
            "[blk.15.attn_k.weight]: avg weights difference: 34.254676%\n",
            "[blk.15.attn_v.weight]: avg weights difference: 27.622696%\n",
            "[blk.15.attn_output.weight]: avg weights difference: 33.154355%\n",
            "[blk.15.ffn_gate.weight]: avg weights difference: 41.763379%\n",
            "[blk.15.ffn_up.weight]: avg weights difference: 39.997022%\n",
            "[blk.15.ffn_down.weight]: avg weights difference: 39.382034%\n",
            "[blk.15.attn_norm.weight]: avg weights difference: 2.591605%\n",
            "[blk.15.ffn_norm.weight]: avg weights difference: 2.628742%\n",
            "[blk.16.attn_q.weight]: avg weights difference: 41.589593%\n",
            "[blk.16.attn_k.weight]: avg weights difference: 34.692658%\n",
            "[blk.16.attn_v.weight]: avg weights difference: 28.674807%\n",
            "[blk.16.attn_output.weight]: avg weights difference: 34.250061%\n",
            "[blk.16.ffn_gate.weight]: avg weights difference: 42.405602%\n",
            "[blk.16.ffn_up.weight]: avg weights difference: 40.948837%\n",
            "[blk.16.ffn_down.weight]: avg weights difference: 40.531679%\n",
            "[blk.16.attn_norm.weight]: avg weights difference: 2.539339%\n",
            "[blk.16.ffn_norm.weight]: avg weights difference: 2.665115%\n",
            "[blk.17.attn_q.weight]: avg weights difference: 43.977024%\n",
            "[blk.17.attn_k.weight]: avg weights difference: 37.305572%\n",
            "[blk.17.attn_v.weight]: avg weights difference: 30.896675%\n",
            "[blk.17.attn_output.weight]: avg weights difference: 36.561729%\n",
            "[blk.17.ffn_gate.weight]: avg weights difference: 43.448301%\n",
            "[blk.17.ffn_up.weight]: avg weights difference: 42.090316%\n",
            "[blk.17.ffn_down.weight]: avg weights difference: 42.240309%\n",
            "[blk.17.attn_norm.weight]: avg weights difference: 2.455910%\n",
            "[blk.17.ffn_norm.weight]: avg weights difference: 2.868740%\n",
            "[blk.18.attn_q.weight]: avg weights difference: 44.764107%\n",
            "[blk.18.attn_k.weight]: avg weights difference: 38.832177%\n",
            "[blk.18.attn_v.weight]: avg weights difference: 33.113803%\n",
            "[blk.18.attn_output.weight]: avg weights difference: 38.816583%\n",
            "[blk.18.ffn_gate.weight]: avg weights difference: 45.502698%\n",
            "[blk.18.ffn_up.weight]: avg weights difference: 44.017699%\n",
            "[blk.18.ffn_down.weight]: avg weights difference: 44.115881%\n",
            "[blk.18.attn_norm.weight]: avg weights difference: 2.416878%\n",
            "[blk.18.ffn_norm.weight]: avg weights difference: 2.927754%\n",
            "[blk.19.attn_q.weight]: avg weights difference: 47.196489%\n",
            "[blk.19.attn_k.weight]: avg weights difference: 41.841030%\n",
            "[blk.19.attn_v.weight]: avg weights difference: 34.385544%\n",
            "[blk.19.attn_output.weight]: avg weights difference: 40.929497%\n",
            "[blk.19.ffn_gate.weight]: avg weights difference: 47.539907%\n",
            "[blk.19.ffn_up.weight]: avg weights difference: 46.542849%\n",
            "[blk.19.ffn_down.weight]: avg weights difference: 46.713715%\n",
            "[blk.19.attn_norm.weight]: avg weights difference: 2.375080%\n",
            "[blk.19.ffn_norm.weight]: avg weights difference: 2.925147%\n",
            "[blk.20.attn_q.weight]: avg weights difference: 50.024704%\n",
            "[blk.20.attn_k.weight]: avg weights difference: 44.186161%\n",
            "[blk.20.attn_v.weight]: avg weights difference: 37.941084%\n",
            "[blk.20.attn_output.weight]: avg weights difference: 45.595754%\n",
            "[blk.20.ffn_gate.weight]: avg weights difference: 49.686502%\n",
            "[blk.20.ffn_up.weight]: avg weights difference: 49.797482%\n",
            "[blk.20.ffn_down.weight]: avg weights difference: 50.292769%\n",
            "[blk.20.attn_norm.weight]: avg weights difference: 2.328575%\n",
            "[blk.20.ffn_norm.weight]: avg weights difference: 2.893100%\n",
            "[blk.21.attn_q.weight]: avg weights difference: 51.864926%\n",
            "[blk.21.attn_k.weight]: avg weights difference: 45.568512%\n",
            "[blk.21.attn_v.weight]: avg weights difference: 41.739700%\n",
            "[blk.21.attn_output.weight]: avg weights difference: 49.506212%\n",
            "[blk.21.ffn_gate.weight]: avg weights difference: 50.911052%\n",
            "[blk.21.ffn_up.weight]: avg weights difference: 52.566557%\n",
            "[blk.21.ffn_down.weight]: avg weights difference: 53.212815%\n",
            "[blk.21.attn_norm.weight]: avg weights difference: 2.293635%\n",
            "[blk.21.ffn_norm.weight]: avg weights difference: 2.889721%\n",
            "[blk.22.attn_q.weight]: avg weights difference: 53.410648%\n",
            "[blk.22.attn_k.weight]: avg weights difference: 47.342402%\n",
            "[blk.22.attn_v.weight]: avg weights difference: 42.293365%\n",
            "[blk.22.attn_output.weight]: avg weights difference: 51.364368%\n",
            "[blk.22.ffn_gate.weight]: avg weights difference: 52.130977%\n",
            "[blk.22.ffn_up.weight]: avg weights difference: 54.536699%\n",
            "[blk.22.ffn_down.weight]: avg weights difference: 55.302310%\n",
            "[blk.22.attn_norm.weight]: avg weights difference: 2.390991%\n",
            "[blk.22.ffn_norm.weight]: avg weights difference: 3.016549%\n",
            "[blk.23.attn_q.weight]: avg weights difference: 53.041654%\n",
            "[blk.23.attn_k.weight]: avg weights difference: 48.415713%\n",
            "[blk.23.attn_v.weight]: avg weights difference: 41.050832%\n",
            "[blk.23.attn_output.weight]: avg weights difference: 51.755448%\n",
            "[blk.23.ffn_gate.weight]: avg weights difference: 52.869722%\n",
            "[blk.23.ffn_up.weight]: avg weights difference: 55.320554%\n",
            "[blk.23.ffn_down.weight]: avg weights difference: 56.109015%\n",
            "[blk.23.attn_norm.weight]: avg weights difference: 2.367006%\n",
            "[blk.23.ffn_norm.weight]: avg weights difference: 3.113265%\n",
            "[blk.24.attn_q.weight]: avg weights difference: 108.899523%\n",
            "[blk.24.attn_k.weight]: avg weights difference: 110.049306%\n",
            "[blk.24.attn_v.weight]: avg weights difference: 106.514180%\n",
            "[blk.24.attn_output.weight]: avg weights difference: 106.568760%\n",
            "[blk.24.ffn_gate.weight]: avg weights difference: 107.166285%\n",
            "[blk.24.ffn_up.weight]: avg weights difference: 108.004525%\n",
            "[blk.24.ffn_down.weight]: avg weights difference: 107.795442%\n",
            "[blk.24.attn_norm.weight]: avg weights difference: 48.379174%\n",
            "[blk.24.ffn_norm.weight]: avg weights difference: 61.560204%\n",
            "[blk.25.attn_q.weight]: avg weights difference: 109.019704%\n",
            "[blk.25.attn_k.weight]: avg weights difference: 110.332112%\n",
            "[blk.25.attn_v.weight]: avg weights difference: 106.817464%\n",
            "[blk.25.attn_output.weight]: avg weights difference: 106.669373%\n",
            "[blk.25.ffn_gate.weight]: avg weights difference: 107.154425%\n",
            "[blk.25.ffn_up.weight]: avg weights difference: 107.876515%\n",
            "[blk.25.ffn_down.weight]: avg weights difference: 107.653223%\n",
            "[blk.25.attn_norm.weight]: avg weights difference: 36.563442%\n",
            "[blk.25.ffn_norm.weight]: avg weights difference: 62.295531%\n",
            "[blk.26.attn_q.weight]: avg weights difference: 109.496764%\n",
            "[blk.26.attn_k.weight]: avg weights difference: 111.538304%\n",
            "[blk.26.attn_v.weight]: avg weights difference: 106.863084%\n",
            "[blk.26.attn_output.weight]: avg weights difference: 106.706427%\n",
            "[blk.26.ffn_gate.weight]: avg weights difference: 107.170340%\n",
            "[blk.26.ffn_up.weight]: avg weights difference: 107.759548%\n",
            "[blk.26.ffn_down.weight]: avg weights difference: 107.547046%\n",
            "[blk.26.attn_norm.weight]: avg weights difference: 39.477995%\n",
            "[blk.26.ffn_norm.weight]: avg weights difference: 62.659581%\n",
            "[blk.27.attn_q.weight]: avg weights difference: 109.029321%\n",
            "[blk.27.attn_k.weight]: avg weights difference: 110.000792%\n",
            "[blk.27.attn_v.weight]: avg weights difference: 106.843261%\n",
            "[blk.27.attn_output.weight]: avg weights difference: 106.985675%\n",
            "[blk.27.ffn_gate.weight]: avg weights difference: 107.161526%\n",
            "[blk.27.ffn_up.weight]: avg weights difference: 107.669111%\n",
            "[blk.27.ffn_down.weight]: avg weights difference: 107.433445%\n",
            "[blk.27.attn_norm.weight]: avg weights difference: 33.544700%\n",
            "[blk.27.ffn_norm.weight]: avg weights difference: 62.994368%\n",
            "[blk.28.attn_q.weight]: avg weights difference: 109.651855%\n",
            "[blk.28.attn_k.weight]: avg weights difference: 111.981668%\n",
            "[blk.28.attn_v.weight]: avg weights difference: 107.665774%\n",
            "[blk.28.attn_output.weight]: avg weights difference: 106.667741%\n",
            "[blk.28.ffn_gate.weight]: avg weights difference: 107.136942%\n",
            "[blk.28.ffn_up.weight]: avg weights difference: 107.488519%\n",
            "[blk.28.ffn_down.weight]: avg weights difference: 107.337726%\n",
            "[blk.28.attn_norm.weight]: avg weights difference: 26.051175%\n",
            "[blk.28.ffn_norm.weight]: avg weights difference: 60.396506%\n",
            "[blk.29.attn_q.weight]: avg weights difference: 110.061031%\n",
            "[blk.29.attn_k.weight]: avg weights difference: 112.580091%\n",
            "[blk.29.attn_v.weight]: avg weights difference: 107.837366%\n",
            "[blk.29.attn_output.weight]: avg weights difference: 106.839818%\n",
            "[blk.29.ffn_gate.weight]: avg weights difference: 107.168924%\n",
            "[blk.29.ffn_up.weight]: avg weights difference: 107.375079%\n",
            "[blk.29.ffn_down.weight]: avg weights difference: 107.331923%\n",
            "[blk.29.attn_norm.weight]: avg weights difference: 33.200632%\n",
            "[blk.29.ffn_norm.weight]: avg weights difference: 56.848111%\n",
            "[blk.30.attn_q.weight]: avg weights difference: 109.703995%\n",
            "[blk.30.attn_k.weight]: avg weights difference: 110.942919%\n",
            "[blk.30.attn_v.weight]: avg weights difference: 108.075344%\n",
            "[blk.30.attn_output.weight]: avg weights difference: 106.925215%\n",
            "[blk.30.ffn_gate.weight]: avg weights difference: 107.263311%\n",
            "[blk.30.ffn_up.weight]: avg weights difference: 107.374150%\n",
            "[blk.30.ffn_down.weight]: avg weights difference: 107.432925%\n",
            "[blk.30.attn_norm.weight]: avg weights difference: 27.909962%\n",
            "[blk.30.ffn_norm.weight]: avg weights difference: 55.151022%\n",
            "[blk.31.attn_q.weight]: avg weights difference: 109.676893%\n",
            "[blk.31.attn_k.weight]: avg weights difference: 111.353428%\n",
            "[blk.31.attn_v.weight]: avg weights difference: 108.417021%\n",
            "[blk.31.attn_output.weight]: avg weights difference: 107.072881%\n",
            "[blk.31.ffn_gate.weight]: avg weights difference: 107.160794%\n",
            "[blk.31.ffn_up.weight]: avg weights difference: 107.200618%\n",
            "[blk.31.ffn_down.weight]: avg weights difference: 108.043250%\n",
            "[blk.31.attn_norm.weight]: avg weights difference: 21.055964%\n",
            "[blk.31.ffn_norm.weight]: avg weights difference: 47.579968%\n",
            "[output_norm.weight]: avg weights difference: 2.362229%\n",
            "[output.weight]: avg weights difference: 37.724208%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools compare models/llama-2-7b-chat.Q4_K_M.gguf models/llama-2-7b-chat.Q4_K_S.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8_3-w-J93Dh",
        "outputId": "f1586bcf-c5cc-4651-e83c-39b1d12d6277"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[token_embd.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.ffn_down.weight]: dequantization function missing...\n",
            "[blk.0.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.0.attn_v.weight]: dequantization function missing...\n",
            "[blk.1.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.1.ffn_down.weight]: dequantization function missing...\n",
            "[blk.1.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.1.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.1.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.1.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.1.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.1.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.1.attn_v.weight]: dequantization function missing...\n",
            "[blk.10.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.10.ffn_down.weight]: dequantization function missing...\n",
            "[blk.10.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.10.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.10.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.10.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.10.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.10.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.10.attn_v.weight]: dequantization function missing...\n",
            "[blk.11.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.11.ffn_down.weight]: dequantization function missing...\n",
            "[blk.11.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.11.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.11.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.11.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.11.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.11.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.11.attn_v.weight]: dequantization function missing...\n",
            "[blk.12.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.12.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.13.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.ffn_down.weight]: avg weights difference: 23.399939%\n",
            "[blk.14.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.14.attn_v.weight]: avg weights difference: 23.209264%\n",
            "[blk.15.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.15.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.16.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.ffn_down.weight]: avg weights difference: 23.222784%\n",
            "[blk.17.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.17.attn_v.weight]: avg weights difference: 23.256661%\n",
            "[blk.18.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.18.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.19.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.ffn_down.weight]: avg weights difference: 23.124140%\n",
            "[blk.2.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.2.attn_v.weight]: avg weights difference: 23.327378%\n",
            "[blk.20.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.20.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.21.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.ffn_down.weight]: avg weights difference: 23.051362%\n",
            "[blk.22.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.22.attn_v.weight]: avg weights difference: 23.135504%\n",
            "[blk.23.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.23.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.3.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.ffn_down.weight]: avg weights difference: 23.232585%\n",
            "[blk.4.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.4.attn_v.weight]: avg weights difference: 23.218996%\n",
            "[blk.5.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.5.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.6.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.ffn_down.weight]: avg weights difference: 23.259466%\n",
            "[blk.7.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.7.attn_v.weight]: avg weights difference: 23.353500%\n",
            "[blk.8.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.8.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.9.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[output.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.ffn_down.weight]: avg weights difference: 23.023198%\n",
            "[blk.24.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.24.attn_v.weight]: avg weights difference: 23.113835%\n",
            "[blk.25.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.25.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.ffn_down.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.26.attn_v.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.ffn_down.weight]: avg weights difference: 23.192564%\n",
            "[blk.27.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.27.attn_v.weight]: avg weights difference: 23.102388%\n",
            "[blk.28.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.28.ffn_down.weight]: avg weights difference: 23.280705%\n",
            "[blk.28.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.28.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.28.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.28.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.28.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.28.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.28.attn_v.weight]: avg weights difference: 23.136619%\n",
            "[blk.29.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.29.ffn_down.weight]: avg weights difference: 23.493435%\n",
            "[blk.29.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.29.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.29.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.29.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.29.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.29.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.29.attn_v.weight]: avg weights difference: 23.186790%\n",
            "[blk.30.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.30.ffn_down.weight]: avg weights difference: 24.020967%\n",
            "[blk.30.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.30.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.30.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.30.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.30.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.30.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.30.attn_v.weight]: avg weights difference: 23.151249%\n",
            "[blk.31.attn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.31.ffn_down.weight]: avg weights difference: 24.811953%\n",
            "[blk.31.ffn_gate.weight]: avg weights difference: 0.000000%\n",
            "[blk.31.ffn_up.weight]: avg weights difference: 0.000000%\n",
            "[blk.31.ffn_norm.weight]: avg weights difference: 0.000000%\n",
            "[blk.31.attn_k.weight]: avg weights difference: 0.000000%\n",
            "[blk.31.attn_output.weight]: avg weights difference: 0.000000%\n",
            "[blk.31.attn_q.weight]: avg weights difference: 0.000000%\n",
            "[blk.31.attn_v.weight]: avg weights difference: 23.378076%\n",
            "[output_norm.weight]: avg weights difference: 0.000000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools inspect-tensor file.gguf tensor.name [count]\n",
        "显示指定张量的所有权重值（如果未指定 count，则仅显示前 count 个）。这对于低级别的任务很有用，比如检查量化是否按预期工作，查看引入的误差，模型指纹等。\n",
        "\n"
      ],
      "metadata": {
        "id": "Rv_R7iWnZBqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo -e \"\\n----token_embd.weight----\\n\"\n",
        "!./gguf-tools inspect-tensor models/mistral-7b-instruct-v0.2.Q8_0.gguf token_embd.weight 50\n",
        "!echo -e \"\\n----blk.0.attn_q.weight----\\n\"\n",
        "!./gguf-tools inspect-tensor models/mistral-7b-instruct-v0.2.Q8_0.gguf blk.0.attn_q.weight 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uzegXfBZFwx",
        "outputId": "0b9131e2-9db4-4a58-f4b2-a4de974e5f74"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----token_embd.weight----\n",
            "\n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, 0.000000, 0.000000, \n",
            "0.000000, 0.000000, \n",
            "\n",
            "----blk.0.attn_q.weight----\n",
            "\n",
            "0.000085, 0.000766, 0.000000, -0.000680, \n",
            "0.001616, -0.000085, 0.003828, 0.000255, \n",
            "0.002637, 0.000000, 0.000425, -0.000085, \n",
            "0.000000, 0.000170, 0.006039, -0.000595, \n",
            "0.000000, -0.002892, 0.000000, -0.006294, \n",
            "-0.004848, -0.002382, 0.010802, 0.002892, \n",
            "0.001191, 0.000000, -0.000255, 0.000000, \n",
            "0.000255, -0.003402, 0.000000, 0.007400, \n",
            "0.000000, 0.000000, 0.002815, 0.000088, \n",
            "0.000000, 0.000880, 0.000000, 0.000000, \n",
            "0.000000, 0.000088, 0.000176, -0.000880, \n",
            "0.001232, 0.000088, 0.000000, -0.000352, \n",
            "0.000176, -0.001496, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gguf-tools split-mixtral 65230776370407150546470161412165 mixtral.gguf out.gguf\n",
        "从 Mixtral 7B MoE 中提取一个 7B 模型 out.gguf，使用指定的每层 MoE ID（在序列 652... 中有 32 个数字）。\n",
        "\n",
        "请注意，通过 split-mixtral 方式获得的模型实际上没有执行任何有用的工作。这只是一个实验和一个不太重要的任务，用来展示如何使用该库。很可能很快就会被移除，一旦我有更有趣和有用的示例要展示，比如模型合并。"
      ],
      "metadata": {
        "id": "aeJbhc2MZ_o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./gguf-tools split-mixtral 65230776370407150546470161412165 \\\n",
        "  models/mistral-7b-instruct-v0.2.Q8_0.gguf models/mistral-7b-instruct-v0.2.Q8_0_out.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFyQgUJyZ-vM",
        "outputId": "d7c6a7b8-197b-4bcc-f2eb-321492e145ee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying general.architecture\n",
            "Copying general.name\n",
            "Copying llama.context_length\n",
            "Copying llama.embedding_length\n",
            "Copying llama.block_count\n",
            "Copying llama.feed_forward_length\n",
            "Copying llama.rope.dimension_count\n",
            "Copying llama.attention.head_count\n",
            "Copying llama.attention.head_count_kv\n",
            "Copying llama.attention.layer_norm_rms_epsilon\n",
            "Copying llama.rope.freq_base\n",
            "Copying general.file_type\n",
            "Copying tokenizer.ggml.model\n",
            "Copying tokenizer.ggml.tokens\n",
            "Copying tokenizer.ggml.scores\n",
            "Copying tokenizer.ggml.token_type\n",
            "Copying tokenizer.ggml.bos_token_id\n",
            "Copying tokenizer.ggml.eos_token_id\n",
            "Copying tokenizer.ggml.unknown_token_id\n",
            "Copying tokenizer.ggml.padding_token_id\n",
            "Copying tokenizer.ggml.add_bos_token\n",
            "Copying tokenizer.ggml.add_eos_token\n",
            "Copying tokenizer.chat_template\n",
            "Copying general.quantization_version\n",
            "Skipping tensor blk.0.ffn_gate.weight\n",
            "Skipping tensor blk.0.ffn_up.weight\n",
            "Skipping tensor blk.0.ffn_down.weight\n",
            "Skipping tensor blk.1.ffn_gate.weight\n",
            "Skipping tensor blk.1.ffn_up.weight\n",
            "Skipping tensor blk.1.ffn_down.weight\n",
            "Skipping tensor blk.2.ffn_gate.weight\n",
            "Skipping tensor blk.2.ffn_up.weight\n",
            "Skipping tensor blk.2.ffn_down.weight\n",
            "Skipping tensor blk.3.ffn_gate.weight\n",
            "Skipping tensor blk.3.ffn_up.weight\n",
            "Skipping tensor blk.3.ffn_down.weight\n",
            "Skipping tensor blk.4.ffn_gate.weight\n",
            "Skipping tensor blk.4.ffn_up.weight\n",
            "Skipping tensor blk.4.ffn_down.weight\n",
            "Skipping tensor blk.5.ffn_gate.weight\n",
            "Skipping tensor blk.5.ffn_up.weight\n",
            "Skipping tensor blk.5.ffn_down.weight\n",
            "Skipping tensor blk.6.ffn_gate.weight\n",
            "Skipping tensor blk.6.ffn_up.weight\n",
            "Skipping tensor blk.6.ffn_down.weight\n",
            "Skipping tensor blk.7.ffn_gate.weight\n",
            "Skipping tensor blk.7.ffn_up.weight\n",
            "Skipping tensor blk.7.ffn_down.weight\n",
            "Skipping tensor blk.8.ffn_gate.weight\n",
            "Skipping tensor blk.8.ffn_up.weight\n",
            "Skipping tensor blk.8.ffn_down.weight\n",
            "Skipping tensor blk.9.ffn_gate.weight\n",
            "Skipping tensor blk.9.ffn_up.weight\n",
            "Skipping tensor blk.9.ffn_down.weight\n",
            "Skipping tensor blk.10.ffn_gate.weight\n",
            "Skipping tensor blk.10.ffn_up.weight\n",
            "Skipping tensor blk.10.ffn_down.weight\n",
            "Skipping tensor blk.11.ffn_gate.weight\n",
            "Skipping tensor blk.11.ffn_up.weight\n",
            "Skipping tensor blk.11.ffn_down.weight\n",
            "Skipping tensor blk.12.ffn_gate.weight\n",
            "Skipping tensor blk.12.ffn_up.weight\n",
            "Skipping tensor blk.12.ffn_down.weight\n",
            "Skipping tensor blk.13.ffn_gate.weight\n",
            "Skipping tensor blk.13.ffn_up.weight\n",
            "Skipping tensor blk.13.ffn_down.weight\n",
            "Skipping tensor blk.14.ffn_gate.weight\n",
            "Skipping tensor blk.14.ffn_up.weight\n",
            "Skipping tensor blk.14.ffn_down.weight\n",
            "Skipping tensor blk.15.ffn_gate.weight\n",
            "Skipping tensor blk.15.ffn_up.weight\n",
            "Skipping tensor blk.15.ffn_down.weight\n",
            "Skipping tensor blk.16.ffn_gate.weight\n",
            "Skipping tensor blk.16.ffn_up.weight\n",
            "Skipping tensor blk.16.ffn_down.weight\n",
            "Skipping tensor blk.17.ffn_gate.weight\n",
            "Skipping tensor blk.17.ffn_up.weight\n",
            "Skipping tensor blk.17.ffn_down.weight\n",
            "Skipping tensor blk.18.ffn_gate.weight\n",
            "Skipping tensor blk.18.ffn_up.weight\n",
            "Skipping tensor blk.18.ffn_down.weight\n",
            "Skipping tensor blk.19.ffn_gate.weight\n",
            "Skipping tensor blk.19.ffn_up.weight\n",
            "Skipping tensor blk.19.ffn_down.weight\n",
            "Skipping tensor blk.20.ffn_gate.weight\n",
            "Skipping tensor blk.20.ffn_up.weight\n",
            "Skipping tensor blk.20.ffn_down.weight\n",
            "Skipping tensor blk.21.ffn_gate.weight\n",
            "Skipping tensor blk.21.ffn_up.weight\n",
            "Skipping tensor blk.21.ffn_down.weight\n",
            "Skipping tensor blk.22.ffn_gate.weight\n",
            "Skipping tensor blk.22.ffn_up.weight\n",
            "Skipping tensor blk.22.ffn_down.weight\n",
            "Skipping tensor blk.23.ffn_gate.weight\n",
            "Skipping tensor blk.23.ffn_up.weight\n",
            "Skipping tensor blk.23.ffn_down.weight\n",
            "Skipping tensor blk.24.ffn_gate.weight\n",
            "Skipping tensor blk.24.ffn_up.weight\n",
            "Skipping tensor blk.24.ffn_down.weight\n",
            "Skipping tensor blk.25.ffn_gate.weight\n",
            "Skipping tensor blk.25.ffn_up.weight\n",
            "Skipping tensor blk.25.ffn_down.weight\n",
            "Skipping tensor blk.26.ffn_gate.weight\n",
            "Skipping tensor blk.26.ffn_up.weight\n",
            "Skipping tensor blk.26.ffn_down.weight\n",
            "Skipping tensor blk.27.ffn_gate.weight\n",
            "Skipping tensor blk.27.ffn_up.weight\n",
            "Skipping tensor blk.27.ffn_down.weight\n",
            "Skipping tensor blk.28.ffn_gate.weight\n",
            "Skipping tensor blk.28.ffn_up.weight\n",
            "Skipping tensor blk.28.ffn_down.weight\n",
            "Skipping tensor blk.29.ffn_gate.weight\n",
            "Skipping tensor blk.29.ffn_up.weight\n",
            "Skipping tensor blk.29.ffn_down.weight\n",
            "Skipping tensor blk.30.ffn_gate.weight\n",
            "Skipping tensor blk.30.ffn_up.weight\n",
            "Skipping tensor blk.30.ffn_down.weight\n",
            "Skipping tensor blk.31.ffn_gate.weight\n",
            "Skipping tensor blk.31.ffn_up.weight\n",
            "Skipping tensor blk.31.ffn_down.weight\n",
            "Output file: after writing tensors info, file size is: 729636\n",
            "Writing tensor token_embd.weight (weights from token_embd.weight)\n",
            "Writing tensor blk.0.attn_q.weight (weights from blk.0.attn_q.weight)\n",
            "Writing tensor blk.0.attn_k.weight (weights from blk.0.attn_k.weight)\n",
            "Writing tensor blk.0.attn_v.weight (weights from blk.0.attn_v.weight)\n",
            "Writing tensor blk.0.attn_output.weight (weights from blk.0.attn_output.weight)\n",
            "Writing tensor blk.0.attn_norm.weight (weights from blk.0.attn_norm.weight)\n",
            "Writing tensor blk.0.ffn_norm.weight (weights from blk.0.ffn_norm.weight)\n",
            "Writing tensor blk.1.attn_q.weight (weights from blk.1.attn_q.weight)\n",
            "Writing tensor blk.1.attn_k.weight (weights from blk.1.attn_k.weight)\n",
            "Writing tensor blk.1.attn_v.weight (weights from blk.1.attn_v.weight)\n",
            "Writing tensor blk.1.attn_output.weight (weights from blk.1.attn_output.weight)\n",
            "Writing tensor blk.1.attn_norm.weight (weights from blk.1.attn_norm.weight)\n",
            "Writing tensor blk.1.ffn_norm.weight (weights from blk.1.ffn_norm.weight)\n",
            "Writing tensor blk.2.attn_q.weight (weights from blk.2.attn_q.weight)\n",
            "Writing tensor blk.2.attn_k.weight (weights from blk.2.attn_k.weight)\n",
            "Writing tensor blk.2.attn_v.weight (weights from blk.2.attn_v.weight)\n",
            "Writing tensor blk.2.attn_output.weight (weights from blk.2.attn_output.weight)\n",
            "Writing tensor blk.2.attn_norm.weight (weights from blk.2.attn_norm.weight)\n",
            "Writing tensor blk.2.ffn_norm.weight (weights from blk.2.ffn_norm.weight)\n",
            "Writing tensor blk.3.attn_q.weight (weights from blk.3.attn_q.weight)\n",
            "Writing tensor blk.3.attn_k.weight (weights from blk.3.attn_k.weight)\n",
            "Writing tensor blk.3.attn_v.weight (weights from blk.3.attn_v.weight)\n",
            "Writing tensor blk.3.attn_output.weight (weights from blk.3.attn_output.weight)\n",
            "Writing tensor blk.3.attn_norm.weight (weights from blk.3.attn_norm.weight)\n",
            "Writing tensor blk.3.ffn_norm.weight (weights from blk.3.ffn_norm.weight)\n",
            "Writing tensor blk.4.attn_q.weight (weights from blk.4.attn_q.weight)\n",
            "Writing tensor blk.4.attn_k.weight (weights from blk.4.attn_k.weight)\n",
            "Writing tensor blk.4.attn_v.weight (weights from blk.4.attn_v.weight)\n",
            "Writing tensor blk.4.attn_output.weight (weights from blk.4.attn_output.weight)\n",
            "Writing tensor blk.4.attn_norm.weight (weights from blk.4.attn_norm.weight)\n",
            "Writing tensor blk.4.ffn_norm.weight (weights from blk.4.ffn_norm.weight)\n",
            "Writing tensor blk.5.attn_q.weight (weights from blk.5.attn_q.weight)\n",
            "Writing tensor blk.5.attn_k.weight (weights from blk.5.attn_k.weight)\n",
            "Writing tensor blk.5.attn_v.weight (weights from blk.5.attn_v.weight)\n",
            "Writing tensor blk.5.attn_output.weight (weights from blk.5.attn_output.weight)\n",
            "Writing tensor blk.5.attn_norm.weight (weights from blk.5.attn_norm.weight)\n",
            "Writing tensor blk.5.ffn_norm.weight (weights from blk.5.ffn_norm.weight)\n",
            "Writing tensor blk.6.attn_q.weight (weights from blk.6.attn_q.weight)\n",
            "Writing tensor blk.6.attn_k.weight (weights from blk.6.attn_k.weight)\n",
            "Writing tensor blk.6.attn_v.weight (weights from blk.6.attn_v.weight)\n",
            "Writing tensor blk.6.attn_output.weight (weights from blk.6.attn_output.weight)\n",
            "Writing tensor blk.6.attn_norm.weight (weights from blk.6.attn_norm.weight)\n",
            "Writing tensor blk.6.ffn_norm.weight (weights from blk.6.ffn_norm.weight)\n",
            "Writing tensor blk.7.attn_q.weight (weights from blk.7.attn_q.weight)\n",
            "Writing tensor blk.7.attn_k.weight (weights from blk.7.attn_k.weight)\n",
            "Writing tensor blk.7.attn_v.weight (weights from blk.7.attn_v.weight)\n",
            "Writing tensor blk.7.attn_output.weight (weights from blk.7.attn_output.weight)\n",
            "Writing tensor blk.7.attn_norm.weight (weights from blk.7.attn_norm.weight)\n",
            "Writing tensor blk.7.ffn_norm.weight (weights from blk.7.ffn_norm.weight)\n",
            "Writing tensor blk.8.attn_q.weight (weights from blk.8.attn_q.weight)\n",
            "Writing tensor blk.8.attn_k.weight (weights from blk.8.attn_k.weight)\n",
            "Writing tensor blk.8.attn_v.weight (weights from blk.8.attn_v.weight)\n",
            "Writing tensor blk.8.attn_output.weight (weights from blk.8.attn_output.weight)\n",
            "Writing tensor blk.8.attn_norm.weight (weights from blk.8.attn_norm.weight)\n",
            "Writing tensor blk.8.ffn_norm.weight (weights from blk.8.ffn_norm.weight)\n",
            "Writing tensor blk.9.attn_q.weight (weights from blk.9.attn_q.weight)\n",
            "Writing tensor blk.9.attn_k.weight (weights from blk.9.attn_k.weight)\n",
            "Writing tensor blk.9.attn_v.weight (weights from blk.9.attn_v.weight)\n",
            "Writing tensor blk.9.attn_output.weight (weights from blk.9.attn_output.weight)\n",
            "Writing tensor blk.9.attn_norm.weight (weights from blk.9.attn_norm.weight)\n",
            "Writing tensor blk.9.ffn_norm.weight (weights from blk.9.ffn_norm.weight)\n",
            "Writing tensor blk.10.attn_q.weight (weights from blk.10.attn_q.weight)\n",
            "Writing tensor blk.10.attn_k.weight (weights from blk.10.attn_k.weight)\n",
            "Writing tensor blk.10.attn_v.weight (weights from blk.10.attn_v.weight)\n",
            "Writing tensor blk.10.attn_output.weight (weights from blk.10.attn_output.weight)\n",
            "Writing tensor blk.10.attn_norm.weight (weights from blk.10.attn_norm.weight)\n",
            "Writing tensor blk.10.ffn_norm.weight (weights from blk.10.ffn_norm.weight)\n",
            "Writing tensor blk.11.attn_q.weight (weights from blk.11.attn_q.weight)\n",
            "Writing tensor blk.11.attn_k.weight (weights from blk.11.attn_k.weight)\n",
            "Writing tensor blk.11.attn_v.weight (weights from blk.11.attn_v.weight)\n",
            "Writing tensor blk.11.attn_output.weight (weights from blk.11.attn_output.weight)\n",
            "Writing tensor blk.11.attn_norm.weight (weights from blk.11.attn_norm.weight)\n",
            "Writing tensor blk.11.ffn_norm.weight (weights from blk.11.ffn_norm.weight)\n",
            "Writing tensor blk.12.attn_q.weight (weights from blk.12.attn_q.weight)\n",
            "Writing tensor blk.12.attn_k.weight (weights from blk.12.attn_k.weight)\n",
            "Writing tensor blk.12.attn_v.weight (weights from blk.12.attn_v.weight)\n",
            "Writing tensor blk.12.attn_output.weight (weights from blk.12.attn_output.weight)\n",
            "Writing tensor blk.12.attn_norm.weight (weights from blk.12.attn_norm.weight)\n",
            "Writing tensor blk.12.ffn_norm.weight (weights from blk.12.ffn_norm.weight)\n",
            "Writing tensor blk.13.attn_q.weight (weights from blk.13.attn_q.weight)\n",
            "Writing tensor blk.13.attn_k.weight (weights from blk.13.attn_k.weight)\n",
            "Writing tensor blk.13.attn_v.weight (weights from blk.13.attn_v.weight)\n",
            "Writing tensor blk.13.attn_output.weight (weights from blk.13.attn_output.weight)\n",
            "Writing tensor blk.13.attn_norm.weight (weights from blk.13.attn_norm.weight)\n",
            "Writing tensor blk.13.ffn_norm.weight (weights from blk.13.ffn_norm.weight)\n",
            "Writing tensor blk.14.attn_q.weight (weights from blk.14.attn_q.weight)\n",
            "Writing tensor blk.14.attn_k.weight (weights from blk.14.attn_k.weight)\n",
            "Writing tensor blk.14.attn_v.weight (weights from blk.14.attn_v.weight)\n",
            "Writing tensor blk.14.attn_output.weight (weights from blk.14.attn_output.weight)\n",
            "Writing tensor blk.14.attn_norm.weight (weights from blk.14.attn_norm.weight)\n",
            "Writing tensor blk.14.ffn_norm.weight (weights from blk.14.ffn_norm.weight)\n",
            "Writing tensor blk.15.attn_q.weight (weights from blk.15.attn_q.weight)\n",
            "Writing tensor blk.15.attn_k.weight (weights from blk.15.attn_k.weight)\n",
            "Writing tensor blk.15.attn_v.weight (weights from blk.15.attn_v.weight)\n",
            "Writing tensor blk.15.attn_output.weight (weights from blk.15.attn_output.weight)\n",
            "Writing tensor blk.15.attn_norm.weight (weights from blk.15.attn_norm.weight)\n",
            "Writing tensor blk.15.ffn_norm.weight (weights from blk.15.ffn_norm.weight)\n",
            "Writing tensor blk.16.attn_q.weight (weights from blk.16.attn_q.weight)\n",
            "Writing tensor blk.16.attn_k.weight (weights from blk.16.attn_k.weight)\n",
            "Writing tensor blk.16.attn_v.weight (weights from blk.16.attn_v.weight)\n",
            "Writing tensor blk.16.attn_output.weight (weights from blk.16.attn_output.weight)\n",
            "Writing tensor blk.16.attn_norm.weight (weights from blk.16.attn_norm.weight)\n",
            "Writing tensor blk.16.ffn_norm.weight (weights from blk.16.ffn_norm.weight)\n",
            "Writing tensor blk.17.attn_q.weight (weights from blk.17.attn_q.weight)\n",
            "Writing tensor blk.17.attn_k.weight (weights from blk.17.attn_k.weight)\n",
            "Writing tensor blk.17.attn_v.weight (weights from blk.17.attn_v.weight)\n",
            "Writing tensor blk.17.attn_output.weight (weights from blk.17.attn_output.weight)\n",
            "Writing tensor blk.17.attn_norm.weight (weights from blk.17.attn_norm.weight)\n",
            "Writing tensor blk.17.ffn_norm.weight (weights from blk.17.ffn_norm.weight)\n",
            "Writing tensor blk.18.attn_q.weight (weights from blk.18.attn_q.weight)\n",
            "Writing tensor blk.18.attn_k.weight (weights from blk.18.attn_k.weight)\n",
            "Writing tensor blk.18.attn_v.weight (weights from blk.18.attn_v.weight)\n",
            "Writing tensor blk.18.attn_output.weight (weights from blk.18.attn_output.weight)\n",
            "Writing tensor blk.18.attn_norm.weight (weights from blk.18.attn_norm.weight)\n",
            "Writing tensor blk.18.ffn_norm.weight (weights from blk.18.ffn_norm.weight)\n",
            "Writing tensor blk.19.attn_q.weight (weights from blk.19.attn_q.weight)\n",
            "Writing tensor blk.19.attn_k.weight (weights from blk.19.attn_k.weight)\n",
            "Writing tensor blk.19.attn_v.weight (weights from blk.19.attn_v.weight)\n",
            "Writing tensor blk.19.attn_output.weight (weights from blk.19.attn_output.weight)\n",
            "Writing tensor blk.19.attn_norm.weight (weights from blk.19.attn_norm.weight)\n",
            "Writing tensor blk.19.ffn_norm.weight (weights from blk.19.ffn_norm.weight)\n",
            "Writing tensor blk.20.attn_q.weight (weights from blk.20.attn_q.weight)\n",
            "Writing tensor blk.20.attn_k.weight (weights from blk.20.attn_k.weight)\n",
            "Writing tensor blk.20.attn_v.weight (weights from blk.20.attn_v.weight)\n",
            "Writing tensor blk.20.attn_output.weight (weights from blk.20.attn_output.weight)\n",
            "Writing tensor blk.20.attn_norm.weight (weights from blk.20.attn_norm.weight)\n",
            "Writing tensor blk.20.ffn_norm.weight (weights from blk.20.ffn_norm.weight)\n",
            "Writing tensor blk.21.attn_q.weight (weights from blk.21.attn_q.weight)\n",
            "Writing tensor blk.21.attn_k.weight (weights from blk.21.attn_k.weight)\n",
            "Writing tensor blk.21.attn_v.weight (weights from blk.21.attn_v.weight)\n",
            "Writing tensor blk.21.attn_output.weight (weights from blk.21.attn_output.weight)\n",
            "Writing tensor blk.21.attn_norm.weight (weights from blk.21.attn_norm.weight)\n",
            "Writing tensor blk.21.ffn_norm.weight (weights from blk.21.ffn_norm.weight)\n",
            "Writing tensor blk.22.attn_q.weight (weights from blk.22.attn_q.weight)\n",
            "Writing tensor blk.22.attn_k.weight (weights from blk.22.attn_k.weight)\n",
            "Writing tensor blk.22.attn_v.weight (weights from blk.22.attn_v.weight)\n",
            "Writing tensor blk.22.attn_output.weight (weights from blk.22.attn_output.weight)\n",
            "Writing tensor blk.22.attn_norm.weight (weights from blk.22.attn_norm.weight)\n",
            "Writing tensor blk.22.ffn_norm.weight (weights from blk.22.ffn_norm.weight)\n",
            "Writing tensor blk.23.attn_q.weight (weights from blk.23.attn_q.weight)\n",
            "Writing tensor blk.23.attn_k.weight (weights from blk.23.attn_k.weight)\n",
            "Writing tensor blk.23.attn_v.weight (weights from blk.23.attn_v.weight)\n",
            "Writing tensor blk.23.attn_output.weight (weights from blk.23.attn_output.weight)\n",
            "Writing tensor blk.23.attn_norm.weight (weights from blk.23.attn_norm.weight)\n",
            "Writing tensor blk.23.ffn_norm.weight (weights from blk.23.ffn_norm.weight)\n",
            "Writing tensor blk.24.attn_q.weight (weights from blk.24.attn_q.weight)\n",
            "Writing tensor blk.24.attn_k.weight (weights from blk.24.attn_k.weight)\n",
            "Writing tensor blk.24.attn_v.weight (weights from blk.24.attn_v.weight)\n",
            "Writing tensor blk.24.attn_output.weight (weights from blk.24.attn_output.weight)\n",
            "Writing tensor blk.24.attn_norm.weight (weights from blk.24.attn_norm.weight)\n",
            "Writing tensor blk.24.ffn_norm.weight (weights from blk.24.ffn_norm.weight)\n",
            "Writing tensor blk.25.attn_q.weight (weights from blk.25.attn_q.weight)\n",
            "Writing tensor blk.25.attn_k.weight (weights from blk.25.attn_k.weight)\n",
            "Writing tensor blk.25.attn_v.weight (weights from blk.25.attn_v.weight)\n",
            "Writing tensor blk.25.attn_output.weight (weights from blk.25.attn_output.weight)\n",
            "Writing tensor blk.25.attn_norm.weight (weights from blk.25.attn_norm.weight)\n",
            "Writing tensor blk.25.ffn_norm.weight (weights from blk.25.ffn_norm.weight)\n",
            "Writing tensor blk.26.attn_q.weight (weights from blk.26.attn_q.weight)\n",
            "Writing tensor blk.26.attn_k.weight (weights from blk.26.attn_k.weight)\n",
            "Writing tensor blk.26.attn_v.weight (weights from blk.26.attn_v.weight)\n",
            "Writing tensor blk.26.attn_output.weight (weights from blk.26.attn_output.weight)\n",
            "Writing tensor blk.26.attn_norm.weight (weights from blk.26.attn_norm.weight)\n",
            "Writing tensor blk.26.ffn_norm.weight (weights from blk.26.ffn_norm.weight)\n",
            "Writing tensor blk.27.attn_q.weight (weights from blk.27.attn_q.weight)\n",
            "Writing tensor blk.27.attn_k.weight (weights from blk.27.attn_k.weight)\n",
            "Writing tensor blk.27.attn_v.weight (weights from blk.27.attn_v.weight)\n",
            "Writing tensor blk.27.attn_output.weight (weights from blk.27.attn_output.weight)\n",
            "Writing tensor blk.27.attn_norm.weight (weights from blk.27.attn_norm.weight)\n",
            "Writing tensor blk.27.ffn_norm.weight (weights from blk.27.ffn_norm.weight)\n",
            "Writing tensor blk.28.attn_q.weight (weights from blk.28.attn_q.weight)\n",
            "Writing tensor blk.28.attn_k.weight (weights from blk.28.attn_k.weight)\n",
            "Writing tensor blk.28.attn_v.weight (weights from blk.28.attn_v.weight)\n",
            "Writing tensor blk.28.attn_output.weight (weights from blk.28.attn_output.weight)\n",
            "Writing tensor blk.28.attn_norm.weight (weights from blk.28.attn_norm.weight)\n",
            "Writing tensor blk.28.ffn_norm.weight (weights from blk.28.ffn_norm.weight)\n",
            "Writing tensor blk.29.attn_q.weight (weights from blk.29.attn_q.weight)\n",
            "Writing tensor blk.29.attn_k.weight (weights from blk.29.attn_k.weight)\n",
            "Writing tensor blk.29.attn_v.weight (weights from blk.29.attn_v.weight)\n",
            "Writing tensor blk.29.attn_output.weight (weights from blk.29.attn_output.weight)\n",
            "Writing tensor blk.29.attn_norm.weight (weights from blk.29.attn_norm.weight)\n",
            "Writing tensor blk.29.ffn_norm.weight (weights from blk.29.ffn_norm.weight)\n",
            "Writing tensor blk.30.attn_q.weight (weights from blk.30.attn_q.weight)\n",
            "Writing tensor blk.30.attn_k.weight (weights from blk.30.attn_k.weight)\n",
            "Writing tensor blk.30.attn_v.weight (weights from blk.30.attn_v.weight)\n",
            "Writing tensor blk.30.attn_output.weight (weights from blk.30.attn_output.weight)\n",
            "Writing tensor blk.30.attn_norm.weight (weights from blk.30.attn_norm.weight)\n",
            "Writing tensor blk.30.ffn_norm.weight (weights from blk.30.ffn_norm.weight)\n",
            "Writing tensor blk.31.attn_q.weight (weights from blk.31.attn_q.weight)\n",
            "Writing tensor blk.31.attn_k.weight (weights from blk.31.attn_k.weight)\n",
            "Writing tensor blk.31.attn_v.weight (weights from blk.31.attn_v.weight)\n",
            "Writing tensor blk.31.attn_output.weight (weights from blk.31.attn_output.weight)\n",
            "Writing tensor blk.31.attn_norm.weight (weights from blk.31.attn_norm.weight)\n",
            "Writing tensor blk.31.ffn_norm.weight (weights from blk.31.ffn_norm.weight)\n",
            "Writing tensor output_norm.weight (weights from output_norm.weight)\n",
            "Writing tensor output.weight (weights from output.weight)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ghl models/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnE1T542b2P9",
        "outputId": "9c799711-dd97-460a-eb15-1714c9a356f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 23G\n",
            "-rw-r--r-- 1 root 7.2G Jan  4 02:33 mistral-7b-instruct-v0.2.Q8_0.gguf\n",
            "-rw-r--r-- 1 root 1.6G Jan  4 03:03 mistral-7b-instruct-v0.2.Q8_0_out.gguf\n",
            "-rw-r--r-- 1 root 2.8G Jan  4 02:23 phi-2.Q8_0.gguf\n",
            "-rw-r--r-- 1 root  11G Jan  4 02:43 solar-10.7b-instruct-v1.0-uncensored.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLama.cpp"
      ],
      "metadata": {
        "id": "usifu6d0b_5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**作者定义的项目起源hack**: (https://github.com/ggerganov/llama.cpp/issues/33)\n",
        "\n",
        "这是关于实现（也称为“hack”）流程的简要摘要，如果有人感兴趣的话，可能对移植其他模型会有帮助：\n",
        "\n",
        "- 从 `ggml` 仓库的 [GPT-J](https://github.com/ggerganov/ggml/tree/master/examples/gpt-j) 示例开始\n",
        "- 使用了 `ggml` 的 [4-bit 分支](https://github.com/ggerganov/ggml/pull/27)，因为它具有我们想要的初始量化支持\n",
        "- LLaMA 模型与 GPT-J 有非常相似的架构。它使用相同的位置编码（RoPE），类似的激活函数（SiLU 而不是 GELU）。主要区别在于：\n",
        "  - 没有偏置张量\n",
        "  - 一些新的归一化层\n",
        "  - 前馈部分的额外张量\n",
        "  - 操作顺序稍有不同\n",
        "  - 看起来上下文大小并非固定？（如果我正确理解了代码）\n",
        "- 所有这些都是微不足道的更改，只需查看原始的 Python LLaMA 代码即可应用到 GPT-J 示例上\n",
        "- 修改了 Python 转换脚本，以读取 7B 模型的 `.pth` 文件，并像通常一样将其转储为 `ggml` 格式\n",
        "- 分词器显然更加复杂和有问题，但进行了快速的修改，至少部分支持它\n",
        "- 这足以使 LLaMA-7B 正常运行。后来，通过查找到的一些来自社区的[参考资料](https://github.com/ggerganov/llama.cpp/issues/1)，使其余的模型也得到了支持\n",
        "\n",
        "这是在 `ggml` 仓库中的 LLaMA WIP 分支，后来迁移到了 `llama.cpp`：\n",
        "https://github.com/ggerganov/ggml/tree/llama\n",
        "\n",
        "在这个过程中，甚至不需要运行原始的 Python 代码。缺点是我没有机会比较推理不同阶段的输出，所以我对这个实现的正确性有些怀疑。然而，从生成的输出来看，我猜应该是正确的。"
      ],
      "metadata": {
        "id": "q9EimOkUdwq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph1PA7bFcRXr",
        "outputId": "29d65daf-a553-4b50-bc6f-28bcfcdd418d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9M6jO81cVc5",
        "outputId": "6c606f80-f411-42b2-faf5-4bed1570c003"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 15047, done.\u001b[K\n",
            "remote: Counting objects: 100% (1943/1943), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 15047 (delta 1881), reused 1891 (delta 1858), pack-reused 13104\u001b[K\n",
            "Receiving objects: 100% (15047/15047), 18.04 MiB | 24.83 MiB/s, done.\n",
            "Resolving deltas: 100% (10435/10435), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build"
      ],
      "metadata": {
        "id": "inEojitsg12S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOsClnT_cYwk",
        "outputId": "f11fb145-8ed9-450b-bda3-7fd2b9bb8770"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6T3QEPjcbNS",
        "outputId": "7afa7697-9aa2-49a3-d2e4-520c658760f9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server    -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLAS 构建\n",
        "使用 BLAS 支持构建程序可能会在批处理大小大于 32（默认为 512）时提高提示处理的性能。对于仅基于 CPU 的 BLAS 实现的支持不会影响正常的生成性能。对于涉及 GPU 的 BLAS 实现（例如 cuBLAS、hipBLAS 和 CLBlast），我们可能会看到生成性能的提升。目前有几种不同的 BLAS 实现可用于构建和使用：\n",
        "### OpenBLAS"
      ],
      "metadata": {
        "id": "VoGwZGZfgbVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean\n",
        "!make LLAMA_OPENBLAS=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjqDWotLfXF7",
        "outputId": "587bef35-a6d5-4a88-c9be-58690f928415"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
            "removed 'build-info.o'\n",
            "removed 'common.o'\n",
            "removed 'console.o'\n",
            "removed 'ggml-alloc.o'\n",
            "removed 'ggml-backend.o'\n",
            "removed 'ggml-quants.o'\n",
            "removed 'ggml.o'\n",
            "removed 'grammar-parser.o'\n",
            "removed 'llama.o'\n",
            "removed 'sampling.o'\n",
            "removed 'train.o'\n",
            "removed 'tests/test-c.o'\n",
            "removed 'benchmark-matmult'\n",
            "removed 'common/build-info.cpp'\n",
            "removed 'main'\n",
            "removed 'quantize'\n",
            "removed 'quantize-stats'\n",
            "removed 'perplexity'\n",
            "removed 'embedding'\n",
            "removed 'vdot'\n",
            "removed 'q8dot'\n",
            "removed 'train-text-from-scratch'\n",
            "removed 'convert-llama2c-to-ggml'\n",
            "removed 'simple'\n",
            "removed 'batched'\n",
            "removed 'batched-bench'\n",
            "removed 'save-load-state'\n",
            "removed 'server'\n",
            "removed 'gguf'\n",
            "removed 'llama-bench'\n",
            "removed 'libllava.a'\n",
            "removed 'llava-cli'\n",
            "removed 'baby-llama'\n",
            "removed 'beam-search'\n",
            "removed 'speculative'\n",
            "removed 'infill'\n",
            "removed 'tokenize'\n",
            "removed 'parallel'\n",
            "removed 'finetune'\n",
            "removed 'export-lora'\n",
            "removed 'lookahead'\n",
            "removed 'lookup'\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:   -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas  -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLIS 构建\n",
        "\n",
        "https://github.com/ggerganov/llama.cpp/blob/master/docs/BLIS.md\n"
      ],
      "metadata": {
        "id": "U8TFtOqWg8Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KDoHfJoiOq8",
        "outputId": "9c4e477a-bc63-48da-d824-fb175ce791cb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile BLIS:\n",
        "%%bash\n",
        "git clone https://github.com/flame/blis\n",
        "cd blis\n",
        "./configure --enable-cblas -t openmp,pthreads auto\n",
        "# will install to /usr/local/ by default.\n",
        "make -j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsVZXtqxiVwT",
        "outputId": "3456c95a-d072-4ff6-fc71-5ed1dc8e9d42"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configure: detected Linux kernel version 6.1.58+.\n",
            "configure: python interpreter search list is: python python3 python2.\n",
            "configure: found 'python'.\n",
            "configure: using 'python' as python interpreter.\n",
            "configure: found python version 3.10.12 (maj: 3, min: 10, rev: 12).\n",
            "configure: python 3.10.12 appears to be supported.\n",
            "configure: C compiler search list is: gcc clang cc.\n",
            "configure: found 'gcc'.\n",
            "configure: using 'gcc' as C compiler.\n",
            "configure: found gcc version 11.4.0 (maj: 11, min: 4, rev: 0).\n",
            "configure: checking for blacklisted configurations due to gcc 11.4.0.\n",
            "configure: checking gcc 11.4.0 against known consequential version ranges.\n",
            "configure: found assembler ('as') version 2.38 (maj: 2, min: 38, rev: ).\n",
            "configure: checking for blacklisted configurations due to as 2.38.\n",
            "configure: C++ compiler search list is: g++ clang++ c++.\n",
            "configure: found 'g++'.\n",
            "configure: using 'g++' as C++ compiler.\n",
            "configure: Fortran compiler search list is: gfortran ifort.\n",
            "configure: found 'gfortran'.\n",
            "configure: using 'gfortran' as Fortran compiler.\n",
            "configure: library archiver search list is: ar.\n",
            "configure: found 'ar'.\n",
            "configure: using 'ar' as library archiver.\n",
            "configure: archive indexer search list is: ranlib.\n",
            "configure: found 'ranlib'.\n",
            "configure: using 'ranlib' as archive indexer.\n",
            "configure: reading configuration registry...done.\n",
            "configure: determining default version string.\n",
            "configure: found '.git' directory; assuming git clone.\n",
            "configure: executing: git describe --tags.\n",
            "configure: got back 0.9.0-134-ga72e4569.\n",
            "configure: truncating to 0.9.0-134.\n",
            "configure: starting configuration of BLIS 0.9.0-134.\n",
            "configure: configuring with official version string.\n",
            "configure: found shared library .so version '4.0.0'.\n",
            "configure:   .so major version: 4\n",
            "configure:   .so minor.build version: 0.0\n",
            "configure: automatic configuration requested.\n",
            "configure: hardware detection driver returned 'haswell'.\n",
            "configure: checking configuration against contents of 'config_registry'.\n",
            "configure: configuration 'haswell' is registered.\n",
            "configure: 'haswell' is defined as having the following sub-configurations:\n",
            "configure:    haswell\n",
            "configure: which collectively require the following kernels:\n",
            "configure:    haswell zen\n",
            "configure: checking sub-configurations:\n",
            "configure:   'haswell' is registered...and exists.\n",
            "configure: checking sub-configurations' requisite kernels:\n",
            "configure:   'haswell' kernels...exist.\n",
            "configure:   'zen' kernels...exist.\n",
            "configure: no install prefix option given; defaulting to '/usr/local'.\n",
            "configure: no install exec_prefix option given; defaulting to PREFIX.\n",
            "configure: no install libdir option given; defaulting to EXECPREFIX/lib.\n",
            "configure: no install includedir option given; defaulting to PREFIX/include.\n",
            "configure: no install sharedir option given; defaulting to PREFIX/share.\n",
            "configure: final installation directories:\n",
            "configure:   prefix:      /usr/local\n",
            "configure:   exec_prefix: /usr/local\n",
            "configure:   libdir:      /usr/local/lib\n",
            "configure:   includedir:  /usr/local/include\n",
            "configure:   sharedir:    /usr/local/share\n",
            "configure: NOTE: the variables above can be overridden when running make.\n",
            "configure: no preset CFLAGS detected.\n",
            "configure: no preset LDFLAGS detected.\n",
            "configure: disabling verbose make output. (enable with 'make V=1'.)\n",
            "configure: disabling ARG_MAX hack.\n",
            "configure: debug symbols disabled.\n",
            "configure: AddressSanitizer support disabled.\n",
            "configure: building BLIS as both static and shared libraries.\n",
            "configure: exporting only public symbols within shared library.\n",
            "configure: enabling operating system support.\n",
            "configure: enabling thread-local storage (TLS) support.\n",
            "configure: enabling support for threading via OpenMP.\n",
            "configure: enabling support for threading via pthreads.\n",
            "configure: enabling support for single-threading.\n",
            "configure: threading will default to OpenMP.\n",
            "configure: requesting slab work partitioning in jr and/or ir loops.\n",
            "configure: internal memory pools for packing blocks are enabled.\n",
            "configure: internal memory pools for small blocks are enabled.\n",
            "configure: memory tracing output is disabled.\n",
            "configure: libmemkind not found; disabling.\n",
            "configure: compiler appears to support #pragma omp simd.\n",
            "configure: the BLAS compatibility layer is enabled.\n",
            "configure: the CBLAS compatibility layer is enabled.\n",
            "configure: mixed datatype support is enabled.\n",
            "configure: mixed datatype optimizations requiring extra memory are enabled.\n",
            "configure: sup (skinny/unpacked) matrix handling is enabled.\n",
            "configure: trsm diagonal element pre-inversion is enabled.\n",
            "configure: the BLIS API integer size is automatically determined.\n",
            "configure: the BLAS/CBLAS API integer size is 32-bit.\n",
            "configure: AMD-specific framework files will not be considered.\n",
            "configure: configuring with no addons.\n",
            "configure: configuring for conventional gemm implementation.\n",
            "configure: configuring complex return type as \"gnu\".\n",
            "configure: creating ./config.mk from ./build/config.mk.in\n",
            "configure: creating ./bli_config.h from ./build/bli_config.h.in\n",
            "configure: creating ./bli_addon.h from ./build/bli_addon.h.in\n",
            "configure: creating ./obj/haswell\n",
            "configure: creating ./obj/haswell/config/haswell\n",
            "configure: creating ./obj/haswell/kernels/haswell\n",
            "configure: creating ./obj/haswell/kernels/zen\n",
            "configure: creating ./obj/haswell/ref_kernels/haswell\n",
            "configure: creating ./obj/haswell/frame\n",
            "configure: creating ./obj/haswell/blastest\n",
            "configure: creating ./obj/haswell/testsuite\n",
            "configure: creating ./lib/haswell\n",
            "configure: creating ./include/haswell\n",
            "configure: mirroring ./config/haswell to ./obj/haswell/config/haswell\n",
            "configure: mirroring ./kernels/haswell to ./obj/haswell/kernels/haswell\n",
            "configure: mirroring ./kernels/zen to ./obj/haswell/kernels/zen\n",
            "configure: mirroring ./ref_kernels to ./obj/haswell/ref_kernels\n",
            "configure: mirroring ./ref_kernels to ./obj/haswell/ref_kernels/haswell\n",
            "configure: mirroring ./frame to ./obj/haswell/frame\n",
            "configure: creating makefile fragments in ./obj/haswell/config/haswell\n",
            "configure: creating makefile fragments in ./obj/haswell/kernels/haswell\n",
            "configure: creating makefile fragments in ./obj/haswell/kernels/zen\n",
            "configure: creating makefile fragments in ./obj/haswell/ref_kernels\n",
            "configure: creating makefile fragments in ./obj/haswell/frame\n",
            "configure: configured to build within top-level directory of source distribution.\n",
            "Generating monolithic blis.h............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "Generated include/haswell/blis.h\n",
            "Compiling obj/haswell/config/haswell/bli_cntx_init_haswell.o ('haswell' CFLAGS for config code)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_c3xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_c8xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_d6xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_d8xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_s16xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_s6xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_z3xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/1m/bli_packm_haswell_asm_z4xk.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/bli_gemm_haswell_asm_d6x8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/bli_gemm_haswell_asm_d8x6.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/bli_gemmtrsm_l_haswell_asm_d6x8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/bli_gemmtrsm_u_haswell_asm_d6x8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rd_haswell_asm_d6x8m.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rd_haswell_asm_d6x8n.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rd_haswell_asm_s6x16m.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rd_haswell_asm_s6x16n.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rv_haswell_asm_d6x8m.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rv_haswell_asm_d6x8n.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rv_haswell_asm_s6x16m.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/bli_gemmsup_rv_haswell_asm_s6x16n.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rd_haswell_asm_dMx1.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rd_haswell_asm_dMx2.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rd_haswell_asm_dMx4.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rd_haswell_asm_dMx8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_r_haswell_ref_dMx1.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rv_haswell_asm_dMx2.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rv_haswell_asm_dMx4.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rv_haswell_asm_dMx6.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/d6x8/bli_gemmsup_rv_haswell_asm_dMx8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rd_haswell_asm_sMx12.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rd_haswell_asm_sMx16.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rd_haswell_asm_sMx1.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rd_haswell_asm_sMx2.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rd_haswell_asm_sMx4.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rd_haswell_asm_sMx8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_r_haswell_ref_sMx1.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rv_haswell_asm_sMx12.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rv_haswell_asm_sMx16.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rv_haswell_asm_sMx2.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rv_haswell_asm_sMx4.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rv_haswell_asm_sMx6.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/haswell/3/sup/s6x16/bli_gemmsup_rv_haswell_asm_sMx8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_amaxv_zen_int.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_axpyv_zen_int10.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_axpyv_zen_int.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_copyv_zen_int.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_dotv_zen_int10.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_dotv_zen_int.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_dotxv_zen_int.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_scalv_zen_int10.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_scalv_zen_int.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_setv_zen_int.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1/bli_swapv_zen_int8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1f/bli_axpyf_zen_int_4.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1f/bli_axpyf_zen_int_5.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1f/bli_axpyf_zen_int_8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/1f/bli_dotxf_zen_int_8.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/3/bli_gemm_small.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/3/bli_gemmt_small.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/kernels/zen/3/bli_trsm_small.o ('haswell' CFLAGS for kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/bli_cntx_haswell_ref.o ('haswell' CFLAGS for ref. kernel init)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_addv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_amaxv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_axpbyv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_axpyv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_copyv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_dotv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_dotxv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_invertv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_invscalv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_scal2v_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_scalv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_setv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_subv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_swapv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1/bli_xpbyv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1f/bli_axpy2v_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1f/bli_axpyf_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1f/bli_dotaxpyv_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1f/bli_dotxaxpyf_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1f/bli_dotxf_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1m/bli_packm_cxc_diag_1er_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1m/bli_packm_cxc_diag_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1m/bli_packm_cxk_1er_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1m/bli_packm_cxk_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/1m/bli_unpackm_cxk_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/3/bli_gemm_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/3/bli_gemmsup_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/3/bli_gemmtrsm_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/3/bli_trsm_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/ind/bli_gemm1m_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/ind/bli_gemmtrsm1m_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/ref_kernels/haswell/ind/bli_trsm1m_haswell_ref.o ('haswell' CFLAGS for ref. kernels)\n",
            "Compiling obj/haswell/frame/0/bli_l0_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/0/bli_l0_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/0/bli_l0_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/0/bli_l0_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/0/copysc/bli_copysc.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_oapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_oapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_tapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1/bli_l1v_tapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_oapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_oapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_tapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1d/bli_l1d_tapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_oapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_oapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_tapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1f/bli_l1f_tapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_oapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_oapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_tapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_tapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/bli_l1m_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_alloc.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_blk_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_cntl.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_init.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_int.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_part.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_scalar.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_struc_cxk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/packm/bli_packm_struc_cxk_md.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/unpackm/bli_unpackm_blk_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/unpackm/bli_unpackm_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/unpackm/bli_unpackm_cntl.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/1m/unpackm/bli_unpackm_int.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_oapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_oapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_tapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/bli_l2_tapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/gemv/bli_gemv_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/gemv/bli_gemv_unb_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/gemv/bli_gemv_unf_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/gemv/bli_gemv_unf_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/gemv/bli_gemv_var_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/ger/bli_ger_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/ger/bli_ger_unb_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/ger/bli_ger_var_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unb_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unb_var3.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unb_var4.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unf_var1a.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unf_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unf_var3a.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_unf_var3.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/hemv/bli_hemv_var_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her/bli_her_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her/bli_her_unb_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her/bli_her_var_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her2/bli_her2_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her2/bli_her2_unb_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her2/bli_her2_unb_var3.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her2/bli_her2_unb_var4.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her2/bli_her2_unf_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her2/bli_her2_unf_var4.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/her2/bli_her2_var_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trmv/bli_trmv_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trmv/bli_trmv_unb_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trmv/bli_trmv_unf_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trmv/bli_trmv_unf_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trmv/bli_trmv_var_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trsv/bli_trsv_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trsv/bli_trsv_unb_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trsv/bli_trsv_unf_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trsv/bli_trsv_unf_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/2/trsv/bli_trsv_var_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_blocksize.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_cntl.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_decor.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_direct.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_ind.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_int.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_oapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_packab.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_prune.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_schema.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup_decor.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup_int.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup_packm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup_packm_var.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup_ref.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup_var12.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_sup_var1n2m.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_tapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_thrinfo.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_ukr_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_ukr_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/bli_l3_ukr_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_blk_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_blk_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_blk_var3.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_cntl.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_front.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_md.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemm/bli_gemm_md_c2r_ref.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemmt/bli_gemmt_front.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemmt/bli_gemmt_l_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemmt/bli_gemmt_l_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemmt/bli_gemmt_u_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemmt/bli_gemmt_u_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemmt/bli_gemmt_x_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/gemmt/bli_gemmt_x_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/hemm/bli_hemm_front.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/symm/bli_symm_front.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_front.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_ll_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_ll_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_lu_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_lu_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_rl_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_rl_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_ru_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_ru_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_xx_ker_var2b.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm/bli_trmm_xx_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trmm3/bli_trmm3_front.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_blk_var1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_blk_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_blk_var3.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_cntl.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_front.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_ll_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_lu_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_rl_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_ru_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/3/trsm/bli_trsm_xx_ker_var2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_apool.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_arch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_array.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_blksz.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_clock.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_cntl.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_cntx.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_const.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_cpuid.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_env.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_error.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_func.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_getopt.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_gks.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_ind.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_info.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_init.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_machval.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_malloc.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_mbool.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_memsys.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_obj.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_pack.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_obj_scalar.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_param_map.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_part.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_pba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_pool.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_prune.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_query.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_rntm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_sba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_setgetijm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_setgetijv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_setri.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_string.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/bli_winsys.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/cast/bli_castm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/cast/bli_castnzm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/cast/bli_castv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/check/bli_obj_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/check/bli_part_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/noopt/bli_dlamch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/noopt/bli_lsame.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/noopt/bli_slamch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/proj/bli_projm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/base/proj/bli_projv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_amax.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_asum.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_axpy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_copy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_dot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_gemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_gemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_ger.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_hemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_hemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_her2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_her2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_her.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_herk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_nrm2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_scal.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_swap.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_symm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_symv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_syr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_syr2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_syr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_syrk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_trmm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_trmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_trsm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/bla_trsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/blis/thread/b77_thread.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/f77_sub/f77_amax_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/f77_sub/f77_asum_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/f77_sub/f77_dot_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/f77_sub/f77_nrm2_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_caxpy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ccopy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cdotc_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cdotu_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cgbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cgemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cgemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cgerc.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cgeru.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_chbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_chemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_chemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cher2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cher2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cher.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cherk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_chpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_chpr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_chpr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cscal.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_csscal.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_cswap.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_csymm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_csyr2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_csyrk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctbsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctpsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctrmm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctrmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctrsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ctrsm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dasum.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_daxpy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dcopy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ddot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dgbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dgemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dgemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dger.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dnrm2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_drot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_drotg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_drotm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_drotmg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dscal.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsdot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dspmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dspr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dspr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dswap.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsymm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsymv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsyr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsyr2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsyr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dsyrk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtbsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtpsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtrmm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtrmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtrsm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dtrsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dzasum.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_dznrm2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_globals.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_icamax.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_idamax.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_isamax.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_izamax.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sasum.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_saxpy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_scasum.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_scnrm2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_scopy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sdot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sdsdot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sgbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sgemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sgemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sger.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_snrm2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_srot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_srotg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_srotm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_srotmg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ssbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sscal.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sspmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sspr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sspr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_sswap.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ssymm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ssymv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ssyr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ssyr2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ssyr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ssyrk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_stbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_stbsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_stpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_stpsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_strmm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_strmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_strsm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_strsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_xerbla.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zaxpy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zcopy.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zdotc_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zdotu_sub.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zdscal.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zgbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zgemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zgemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zgerc.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zgeru.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zhbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zhemm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zhemv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zher2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zher2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zher.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zherk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zhpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zhpr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zhpr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zscal.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zswap.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zsymm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zsyr2k.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_zsyrk.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztbsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztpsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztrmm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztrmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztrsm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/cblas_ztrsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_caxpby.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_cgemm3m.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_cgemm_batch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_cgemmt.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_daxpby.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_dgemm_batch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_dgemmt.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_saxpby.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_sgemm_batch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_sgemmt.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_zaxpby.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_zgemm3m.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_zgemm_batch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/cblas/src/extra/cblas_zgemmt.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/extra/bla_axpby.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/extra/bla_gemm3m.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/extra/bla_gemm_batch.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/extra/bla_gemmt.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_cabs1.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_gbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_hbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_hpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_hpr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_hpr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_lsame.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_rot.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_rotg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_rotm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_rotmg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_sbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_spmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_spr2.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_spr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_tbmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_tbsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_tpmv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_tpsv.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_xerbla_array.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/bla_xerbla.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_c_abs.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_c_div.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_d_abs.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_d_cnjg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_d_imag.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_d_sign.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_f__cabs.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_r_abs.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_r_cnjg.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_r_imag.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_r_sign.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_z_abs.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/compat/f2c/util/bla_z_div.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_pthread.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thrcomm.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thrcomm_openmp.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thrcomm_pthreads.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thrcomm_single.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thread.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thread_openmp.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thread_pthreads.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thread_range.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thread_range_slab_rr.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thread_range_tlb.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thread_single.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/thread/bli_thrinfo.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_check.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_fpa.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_oapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_oapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_oapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_tapi_ba.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_tapi.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_tapi_ex.o ('haswell' CFLAGS for framework code)\n",
            "Compiling obj/haswell/frame/util/bli_util_unb_var1.o ('haswell' CFLAGS for framework code)\n",
            "Archiving lib/haswell/libblis.a\n",
            "Dynamically linking lib/haswell/libblis.so\n",
            "Creating symlink lib/haswell/libblis.so.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'blis'...\n",
            "In file included from ref_kernels/3/bli_gemm_ref.c:35:\n",
            "ref_kernels/3/bli_gemm_ref.c: In function ‘bli_cgemm_haswell_ref’:\n",
            "./frame/include//bli_complex_macro_defs.h:133:30: warning: ‘ab[<unknown>].real’ may be used uninitialized [-Wmaybe-uninitialized]\n",
            "  133 | #define bli_cimag( x )  ( (x).imag )\n",
            "./frame/include/level0/ri//bli_xpbyris.h:48:38: note: in definition of macro ‘bli_cxxpbyris’\n",
            "   48 |         const __typeof__(yi) yt_i = (xi) + (bi) * (yr) + (br) * (yi); \\\n",
            "      |                                      ^~\n",
            "./frame/include/level0//bli_xpbys.h:106:49: note: in expansion of macro ‘bli_creal’\n",
            "  106 | #define bli_zccxpbys( x, b, y )  bli_cxxpbyris( bli_zreal(x), bli_zimag(x), bli_creal(b), bli_cimag(b), bli_creal(y), bli_cimag(y) )\n",
            "      |                                                 ^~~~~~~~~\n",
            "./frame/include/level0//bli_xpbys.h:187:32: note: in expansion of macro ‘bli_cccxpbys’\n",
            "  187 | #define bli_zxpbys( x, b, y )  bli_zzzxpbys( x, b, y )\n",
            "      |                                ^~~~~~~~~~~~\n",
            "./frame/include//bli_macro_defs.h:52:36: note: in expansion of macro ‘bli_cxpbys’\n",
            "   52 | #define PASTEMAC(ch,op)            PASTEMAC_(ch,op)\n",
            "      |                                    ^~~~\n",
            "./frame/include//bli_macro_defs.h:53:36: note: in expansion of macro ‘PASTEMAC_’\n",
            "   53 | \n",
            "      |                                    ^        \n",
            "ref_kernels/3/bli_gemm_ref.c:290:25: note: in expansion of macro ‘PASTEMAC’\n",
            "  290 |                         PASTEMAC(ch,xpbys) \\\n",
            "      |                         ^~~~~~~~\n",
            "./frame/include//bli_gentfunc_macro_defs.h:153:1: note: in expansion of macro ‘GENTFUNC’\n",
            "  153 | GENTFUNC( dcomplex, z, __VA_ARGS__ )\n",
            "      | ^~~~~~~~\n",
            "ref_kernels/3/bli_gemm_ref.c:300:1: note: in expansion of macro ‘INSERT_GENTFUNC_BASIC’\n",
            "  300 | INSERT_GENTFUNC_BASIC( gemm, BLIS_CNAME_INFIX, BLIS_REF_SUFFIX )\n",
            "      | ^~~~~~~~~~~~~~~~~~~~~\n",
            "ref_kernels/3/bli_gemm_ref.c:197:21: note: ‘ab’ declared here\n",
            "  197 |               ctype ab[ BLIS_STACK_BUF_MAX_SIZE \\\n",
            "      |                     ^~\n",
            "./frame/include//bli_gentfunc_macro_defs.h:153:1: note: in expansion of macro ‘GENTFUNC’\n",
            "  153 | GENTFUNC( dcomplex, z, __VA_ARGS__ )\n",
            "      | ^~~~~~~~\n",
            "ref_kernels/3/bli_gemm_ref.c:300:1: note: in expansion of macro ‘INSERT_GENTFUNC_BASIC’\n",
            "  300 | INSERT_GENTFUNC_BASIC( gemm, BLIS_CNAME_INFIX, BLIS_REF_SUFFIX )\n",
            "      | ^~~~~~~~~~~~~~~~~~~~~\n",
            "In file included from ref_kernels/3/bli_gemm_ref.c:35:\n",
            "./frame/include//bli_complex_macro_defs.h:134:30: warning: ‘ab[<unknown>].imag’ may be used uninitialized [-Wmaybe-uninitialized]\n",
            "  134 | #define bli_zreal( x )  ( (x).real )\n",
            "./frame/include/level0/ri//bli_xpbyris.h:49:38: note: in definition of macro ‘bli_cxxpbyris’\n",
            "   49 |         (yr) = yt_r; \\\n",
            "      |                                      ^ \n",
            "./frame/include/level0//bli_xpbys.h:106:63: note: in expansion of macro ‘bli_cimag’\n",
            "  106 | #define bli_zccxpbys( x, b, y )  bli_cxxpbyris( bli_zreal(x), bli_zimag(x), bli_creal(b), bli_cimag(b), bli_creal(y), bli_cimag(y) )\n",
            "      |                                                               ^~~~~~~~~\n",
            "./frame/include/level0//bli_xpbys.h:187:32: note: in expansion of macro ‘bli_cccxpbys’\n",
            "  187 | #define bli_zxpbys( x, b, y )  bli_zzzxpbys( x, b, y )\n",
            "      |                                ^~~~~~~~~~~~\n",
            "./frame/include//bli_macro_defs.h:52:36: note: in expansion of macro ‘bli_cxpbys’\n",
            "   52 | #define PASTEMAC(ch,op)            PASTEMAC_(ch,op)\n",
            "      |                                    ^~~~\n",
            "./frame/include//bli_macro_defs.h:53:36: note: in expansion of macro ‘PASTEMAC_’\n",
            "   53 | \n",
            "      |                                    ^        \n",
            "ref_kernels/3/bli_gemm_ref.c:290:25: note: in expansion of macro ‘PASTEMAC’\n",
            "  290 |                         PASTEMAC(ch,xpbys) \\\n",
            "      |                         ^~~~~~~~\n",
            "./frame/include//bli_gentfunc_macro_defs.h:153:1: note: in expansion of macro ‘GENTFUNC’\n",
            "  153 | GENTFUNC( dcomplex, z, __VA_ARGS__ )\n",
            "      | ^~~~~~~~\n",
            "ref_kernels/3/bli_gemm_ref.c:300:1: note: in expansion of macro ‘INSERT_GENTFUNC_BASIC’\n",
            "  300 | INSERT_GENTFUNC_BASIC( gemm, BLIS_CNAME_INFIX, BLIS_REF_SUFFIX )\n",
            "      | ^~~~~~~~~~~~~~~~~~~~~\n",
            "ref_kernels/3/bli_gemm_ref.c:197:21: note: ‘ab’ declared here\n",
            "  197 |               ctype ab[ BLIS_STACK_BUF_MAX_SIZE \\\n",
            "      |                     ^~\n",
            "./frame/include//bli_gentfunc_macro_defs.h:153:1: note: in expansion of macro ‘GENTFUNC’\n",
            "  153 | GENTFUNC( dcomplex, z, __VA_ARGS__ )\n",
            "      | ^~~~~~~~\n",
            "ref_kernels/3/bli_gemm_ref.c:300:1: note: in expansion of macro ‘INSERT_GENTFUNC_BASIC’\n",
            "  300 | INSERT_GENTFUNC_BASIC( gemm, BLIS_CNAME_INFIX, BLIS_REF_SUFFIX )\n",
            "      | ^~~~~~~~~~~~~~~~~~~~~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd blis && sudo make install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF4VTY71i0dR",
        "outputId": "a990eb7a-7892-434e-e767-83406a31f2ec"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libblis.a into /usr/local/lib/\n",
            "Installing libblis.so.4.0.0 into /usr/local/lib/\n",
            "Installing symlink libblis.so into /usr/local/lib/\n",
            "Installing symlink libblis.so.4 into /usr/local/lib/\n",
            "Generating monolithic cblas.h........\n",
            "Generated include/haswell/cblas.h\n",
            "Installing blis.h cblas.h into /usr/local/include/blis/\n",
            "Installing blis.h helper header into /usr/local/include/\n",
            "Installing cblas.h helper header into /usr/local/include/\n",
            "Installing config.mk common.mk into /usr/local/share/blis/\n",
            "Installing config/haswell/make_defs.mk into /usr/local/share/blis/config/haswell\n",
            "Installing blis.pc into /usr/local/share/pkgconfig/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make clean\n",
        "!cd llama.cpp && make LLAMA_BLIS=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HkB9n19i875",
        "outputId": "f821943a-7036-4fe7-9e29-6a4d57f66374"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
            "removed 'build-info.o'\n",
            "removed 'common.o'\n",
            "removed 'console.o'\n",
            "removed 'ggml-alloc.o'\n",
            "removed 'ggml-backend.o'\n",
            "removed 'ggml-quants.o'\n",
            "removed 'ggml.o'\n",
            "removed 'grammar-parser.o'\n",
            "removed 'llama.o'\n",
            "removed 'sampling.o'\n",
            "removed 'train.o'\n",
            "removed 'tests/test-c.o'\n",
            "removed 'benchmark-matmult'\n",
            "removed 'common/build-info.cpp'\n",
            "removed 'main'\n",
            "removed 'quantize'\n",
            "removed 'quantize-stats'\n",
            "removed 'perplexity'\n",
            "removed 'embedding'\n",
            "removed 'vdot'\n",
            "removed 'q8dot'\n",
            "removed 'train-text-from-scratch'\n",
            "removed 'convert-llama2c-to-ggml'\n",
            "removed 'simple'\n",
            "removed 'batched'\n",
            "removed 'batched-bench'\n",
            "removed 'save-load-state'\n",
            "removed 'server'\n",
            "removed 'gguf'\n",
            "removed 'llama-bench'\n",
            "removed 'libllava.a'\n",
            "removed 'llava-cli'\n",
            "removed 'baby-llama'\n",
            "removed 'beam-search'\n",
            "removed 'speculative'\n",
            "removed 'infill'\n",
            "removed 'tokenize'\n",
            "removed 'parallel'\n",
            "removed 'finetune'\n",
            "removed 'export-lora'\n",
            "removed 'lookahead'\n",
            "removed 'lookup'\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:   -lblis -L/usr/local/lib \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lblis -L/usr/local/lib \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lblis -L/usr/local/lib   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lblis -L/usr/local/lib  -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lblis -L/usr/local/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup -lblis -L/usr/local/lib \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/local/include/blis -I/usr/include/blis  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intel oneMKL\n",
        "use MKL (Math Kernel Library), a Basic Linear Algebra Subprograms (BLAS) Library\n",
        "https://www.intel.com/content/www/us/en/docs/onemkl/get-started-guide/2024-0/overview.html\n",
        "\n",
        "compiler:\n",
        "https://www.intel.com/content/www/us/en/docs/dpcpp-cpp-compiler/get-started-guide/2024-0/get-started-on-linux.html\n",
        "\n"
      ],
      "metadata": {
        "id": "_7LRQxffj0xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIkxdIgIj_Wp",
        "outputId": "78c60fed-fe4a-4092-a5f4-8eb32a909c96"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装oneApi\n",
        "%%bash\n",
        "wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB \\\n",
        "    | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null\n",
        "echo \"deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main\" | sudo tee /etc/apt/sources.list.d/oneAPI.list\n",
        "apt-get update && \\\n",
        "    apt-get install intel-oneapi-mkl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rypssWjkmVT",
        "outputId": "c384d942-5fa5-45d6-f32e-2d29e14bbe72"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://apt.repos.intel.com/oneapi all InRelease [4,455 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://apt.repos.intel.com/oneapi all/main amd64 Packages [455 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:11 https://apt.repos.intel.com/oneapi all/main all Packages [129 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n",
            "Hit:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:16 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,257 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,158 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n",
            "Fetched 9,531 kB in 2s (4,845 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  intel-oneapi-common-licensing-2024.0 intel-oneapi-common-oneapi-vars-2024.0\n",
            "  intel-oneapi-common-vars intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0\n",
            "  intel-oneapi-compiler-shared-runtime-2024.0 intel-oneapi-mkl-2024.0\n",
            "  intel-oneapi-mkl-common-2024.0 intel-oneapi-openmp-2024.0 intel-oneapi-openmp-common-2024.0\n",
            "  intel-oneapi-tbb-2021.11 intel-oneapi-tbb-common-2021.11 intel-oneapi-tcm-1.0\n",
            "The following NEW packages will be installed:\n",
            "  intel-oneapi-common-licensing-2024.0 intel-oneapi-common-oneapi-vars-2024.0\n",
            "  intel-oneapi-common-vars intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0\n",
            "  intel-oneapi-compiler-shared-runtime-2024.0 intel-oneapi-mkl intel-oneapi-mkl-2024.0\n",
            "  intel-oneapi-mkl-common-2024.0 intel-oneapi-openmp-2024.0 intel-oneapi-openmp-common-2024.0\n",
            "  intel-oneapi-tbb-2021.11 intel-oneapi-tbb-common-2021.11 intel-oneapi-tcm-1.0\n",
            "0 upgraded, 13 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 379 MB of archives.\n",
            "After this operation, 2,494 MB of additional disk space will be used.\n",
            "Get:1 https://apt.repos.intel.com/oneapi all/main all intel-oneapi-common-licensing-2024.0 all 2024.0.0-49406 [30.7 kB]\n",
            "Get:2 https://apt.repos.intel.com/oneapi all/main all intel-oneapi-common-oneapi-vars-2024.0 all 2024.0.0-49406 [10.4 kB]\n",
            "Get:3 https://apt.repos.intel.com/oneapi all/main all intel-oneapi-common-vars all 2024.0.0-49406 [12.2 kB]\n",
            "Get:4 https://apt.repos.intel.com/oneapi all/main all intel-oneapi-openmp-common-2024.0 all 2024.0.2-49895 [20.0 kB]\n",
            "Get:5 https://apt.repos.intel.com/oneapi all/main amd64 intel-oneapi-tcm-1.0 amd64 1.0.0-435 [1,819 kB]\n",
            "Get:6 https://apt.repos.intel.com/oneapi all/main amd64 intel-oneapi-openmp-2024.0 amd64 2024.0.2-49895 [67.0 MB]\n",
            "Get:7 https://apt.repos.intel.com/oneapi all/main amd64 intel-oneapi-compiler-shared-runtime-2024.0 amd64 2024.0.2-49895 [142 MB]\n",
            "Get:8 https://apt.repos.intel.com/oneapi all/main all intel-oneapi-tbb-common-2021.11 all 2021.11.0-49513 [21.0 kB]\n",
            "Get:9 https://apt.repos.intel.com/oneapi all/main amd64 intel-oneapi-tbb-2021.11 amd64 2021.11.0-49513 [968 kB]\n",
            "Get:10 https://apt.repos.intel.com/oneapi all/main amd64 intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0 amd64 2024.0.2-49895 [3,002 kB]\n",
            "Get:11 https://apt.repos.intel.com/oneapi all/main all intel-oneapi-mkl-common-2024.0 all 2024.0.0-49656 [21.9 kB]\n",
            "Get:12 https://apt.repos.intel.com/oneapi all/main amd64 intel-oneapi-mkl-2024.0 amd64 2024.0.0-49656 [164 MB]\n",
            "Get:13 https://apt.repos.intel.com/oneapi all/main amd64 intel-oneapi-mkl amd64 2024.0.0-49656 [2,036 B]\n",
            "Fetched 379 MB in 5s (77.9 MB/s)\n",
            "Selecting previously unselected package intel-oneapi-common-licensing-2024.0.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 121654 files and directories currently installed.)\r\n",
            "Preparing to unpack .../00-intel-oneapi-common-licensing-2024.0_2024.0.0-49406_all.deb ...\r\n",
            "Unpacking intel-oneapi-common-licensing-2024.0 (2024.0.0-49406) ...\r\n",
            "Selecting previously unselected package intel-oneapi-common-oneapi-vars-2024.0.\r\n",
            "Preparing to unpack .../01-intel-oneapi-common-oneapi-vars-2024.0_2024.0.0-49406_all.deb ...\r\n",
            "Unpacking intel-oneapi-common-oneapi-vars-2024.0 (2024.0.0-49406) ...\r\n",
            "Selecting previously unselected package intel-oneapi-common-vars.\r\n",
            "Preparing to unpack .../02-intel-oneapi-common-vars_2024.0.0-49406_all.deb ...\r\n",
            "Unpacking intel-oneapi-common-vars (2024.0.0-49406) ...\r\n",
            "Selecting previously unselected package intel-oneapi-openmp-common-2024.0.\r\n",
            "Preparing to unpack .../03-intel-oneapi-openmp-common-2024.0_2024.0.2-49895_all.deb ...\r\n",
            "Unpacking intel-oneapi-openmp-common-2024.0 (2024.0.2-49895) ...\r\n",
            "Selecting previously unselected package intel-oneapi-tcm-1.0.\r\n",
            "Preparing to unpack .../04-intel-oneapi-tcm-1.0_1.0.0-435_amd64.deb ...\r\n",
            "Unpacking intel-oneapi-tcm-1.0 (1.0.0-435) ...\r\n",
            "Selecting previously unselected package intel-oneapi-openmp-2024.0.\r\n",
            "Preparing to unpack .../05-intel-oneapi-openmp-2024.0_2024.0.2-49895_amd64.deb ...\r\n",
            "Unpacking intel-oneapi-openmp-2024.0 (2024.0.2-49895) ...\r\n",
            "Selecting previously unselected package intel-oneapi-compiler-shared-runtime-2024.0.\r\n",
            "Preparing to unpack .../06-intel-oneapi-compiler-shared-runtime-2024.0_2024.0.2-49895_amd64.deb ...\r\n",
            "Unpacking intel-oneapi-compiler-shared-runtime-2024.0 (2024.0.2-49895) ...\r\n",
            "Selecting previously unselected package intel-oneapi-tbb-common-2021.11.\r\n",
            "Preparing to unpack .../07-intel-oneapi-tbb-common-2021.11_2021.11.0-49513_all.deb ...\r\n",
            "Unpacking intel-oneapi-tbb-common-2021.11 (2021.11.0-49513) ...\r\n",
            "Selecting previously unselected package intel-oneapi-tbb-2021.11.\r\n",
            "Preparing to unpack .../08-intel-oneapi-tbb-2021.11_2021.11.0-49513_amd64.deb ...\r\n",
            "Unpacking intel-oneapi-tbb-2021.11 (2021.11.0-49513) ...\r\n",
            "Selecting previously unselected package intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0.\r\n",
            "Preparing to unpack .../09-intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0_2024.0.2-49895_amd64.deb ...\r\n",
            "Unpacking intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0 (2024.0.2-49895) ...\r\n",
            "Selecting previously unselected package intel-oneapi-mkl-common-2024.0.\r\n",
            "Preparing to unpack .../10-intel-oneapi-mkl-common-2024.0_2024.0.0-49656_all.deb ...\r\n",
            "Unpacking intel-oneapi-mkl-common-2024.0 (2024.0.0-49656) ...\r\n",
            "Selecting previously unselected package intel-oneapi-mkl-2024.0.\r\n",
            "Preparing to unpack .../11-intel-oneapi-mkl-2024.0_2024.0.0-49656_amd64.deb ...\r\n",
            "Unpacking intel-oneapi-mkl-2024.0 (2024.0.0-49656) ...\r\n",
            "Selecting previously unselected package intel-oneapi-mkl.\r\n",
            "Preparing to unpack .../12-intel-oneapi-mkl_2024.0.0-49656_amd64.deb ...\r\n",
            "Unpacking intel-oneapi-mkl (2024.0.0-49656) ...\r\n",
            "Setting up intel-oneapi-common-oneapi-vars-2024.0 (2024.0.0-49406) ...\r\n",
            "Setting up intel-oneapi-common-licensing-2024.0 (2024.0.0-49406) ...\r\n",
            "Setting up intel-oneapi-common-vars (2024.0.0-49406) ...\r\n",
            "Setting up intel-oneapi-mkl-common-2024.0 (2024.0.0-49656) ...\r\n",
            "Setting up intel-oneapi-tcm-1.0 (1.0.0-435) ...\r\n",
            "Setting up intel-oneapi-openmp-common-2024.0 (2024.0.2-49895) ...\r\n",
            "Setting up intel-oneapi-tbb-common-2021.11 (2021.11.0-49513) ...\r\n",
            "Setting up intel-oneapi-openmp-2024.0 (2024.0.2-49895) ...\r\n",
            "Setting up intel-oneapi-tbb-2021.11 (2021.11.0-49513) ...\r\n",
            "Setting up intel-oneapi-compiler-shared-runtime-2024.0 (2024.0.2-49895) ...\r\n",
            "Setting up intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0 (2024.0.2-49895) ...\r\n",
            "Setting up intel-oneapi-mkl-2024.0 (2024.0.0-49656) ...\r\n",
            "Setting up intel-oneapi-mkl (2024.0.0-49656) ...\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2024-01-04 03:45:23--  https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\n",
            "Resolving apt.repos.intel.com (apt.repos.intel.com)... 23.38.16.172, 2a02:26f0:b200:38d::4b23, 2a02:26f0:b200:38e::4b23\n",
            "Connecting to apt.repos.intel.com (apt.repos.intel.com)|23.38.16.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4738 (4.6K) [application/vnd.exstream-package]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "     0K ....                                                  100% 2.11G=0s\n",
            "\n",
            "2024-01-04 03:45:23 (2.11 GB/s) - written to stdout [4738/4738]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source /opt/intel/oneapi/mkl/latest/env/vars.sh && echo $MKLROOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByqQRpAokqWx",
        "outputId": "2d326c92-a2c7-4e19-ff10-1f30e8a7f903"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/intel/oneapi/mkl/2024.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tips:\n",
        "download offline pkg to install (apt install have not include files :{ ), have include files, use ldconfig to linker for #include<mkl.h>\n",
        "\n",
        "GettingStarted link: https://www.intel.com/content/www/us/en/docs/onemkl/get-started-guide/current/overview.html"
      ],
      "metadata": {
        "id": "1M6nKmGzWLHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt autoremove intel-oneapi-mkl\n",
        "#!wget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/adb8a02c-4ee7-4882-97d6-a524150da358/l_onemkl_p_2023.2.0.49497_offline.sh\n",
        "!wget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/163da6e4-56eb-4948-aba3-debcec61c064/l_BaseKit_p_2024.0.1.46_offline.sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnQ9QU67rqnj",
        "outputId": "98008810-8789-46a0-fffc-7ef813872430"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  intel-oneapi-common-licensing-2024.0 intel-oneapi-common-oneapi-vars-2024.0\n",
            "  intel-oneapi-common-vars intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0\n",
            "  intel-oneapi-compiler-shared-runtime-2024.0 intel-oneapi-mkl intel-oneapi-mkl-2024.0\n",
            "  intel-oneapi-mkl-common-2024.0 intel-oneapi-openmp-2024.0 intel-oneapi-openmp-common-2024.0\n",
            "  intel-oneapi-tbb-2021.11 intel-oneapi-tbb-common-2021.11 intel-oneapi-tcm-1.0\n",
            "0 upgraded, 0 newly installed, 13 to remove and 30 not upgraded.\n",
            "After this operation, 2,494 MB disk space will be freed.\n",
            "(Reading database ... 122084 files and directories currently installed.)\n",
            "Removing intel-oneapi-mkl (2024.0.0-49656) ...\n",
            "Removing intel-oneapi-mkl-2024.0 (2024.0.0-49656) ...\n",
            "Removing intel-oneapi-compiler-dpcpp-cpp-runtime-2024.0 (2024.0.2-49895) ...\n",
            "Removing intel-oneapi-compiler-shared-runtime-2024.0 (2024.0.2-49895) ...\n",
            "Removing intel-oneapi-openmp-2024.0 (2024.0.2-49895) ...\n",
            "Removing intel-oneapi-openmp-common-2024.0 (2024.0.2-49895) ...\n",
            "Removing intel-oneapi-mkl-common-2024.0 (2024.0.0-49656) ...\n",
            "Removing intel-oneapi-tbb-2021.11 (2021.11.0-49513) ...\n",
            "Removing intel-oneapi-tbb-common-2021.11 (2021.11.0-49513) ...\n",
            "Removing intel-oneapi-tcm-1.0 (1.0.0-435) ...\n",
            "Removing intel-oneapi-common-vars (2024.0.0-49406) ...\n",
            "Removing intel-oneapi-common-licensing-2024.0 (2024.0.0-49406) ...\n",
            "Removing intel-oneapi-common-oneapi-vars-2024.0 (2024.0.0-49406) ...\n",
            "--2024-01-04 04:05:51--  https://registrationcenter-download.intel.com/akdlm/IRC_NAS/163da6e4-56eb-4948-aba3-debcec61c064/l_BaseKit_p_2024.0.1.46_offline.sh\n",
            "Resolving registrationcenter-download.intel.com (registrationcenter-download.intel.com)... 104.109.143.20, 104.109.143.18\n",
            "Connecting to registrationcenter-download.intel.com (registrationcenter-download.intel.com)|104.109.143.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2808922690 (2.6G) [application/octet-stream]\n",
            "Saving to: ‘l_BaseKit_p_2024.0.1.46_offline.sh’\n",
            "\n",
            "l_BaseKit_p_2024.0. 100%[===================>]   2.62G  40.2MB/s    in 66s     \n",
            "\n",
            "2024-01-04 04:06:57 (40.6 MB/s) - ‘l_BaseKit_p_2024.0.1.46_offline.sh’ saved [2808922690/2808922690]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# oneApi 组件\n",
        "# https://www.intel.com/content/www/us/en/docs/oneapi/installation-guide-linux/2024-0/install-with-command-line.html\n",
        "!sh l_BaseKit_p_2024.0.1.46_offline.sh -a --list-components"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzDuwjHKthDk",
        "outputId": "07cd75ba-f485-4bc5-a6f0-85a1c2162a31"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract l_BaseKit_p_2024.0.1.46_offline to /content/l_BaseKit_p_2024.0.1.46_offline...\n",
            "[#################################################################################################]\n",
            "Extract l_BaseKit_p_2024.0.1.46_offline completed!\n",
            "Checking system requirements...\n",
            "Done.\n",
            "Wait while the installer is preparing...\n",
            "Done.\n",
            "Launching the installer...\n",
            "ID                                  Version         Installed Name                                                  \n",
            "====================================================================================================================\n",
            "intel.oneapi.lin.dpcpp-ct           2024.0.0+49391  false     Intel® DPC++ Compatibility Tool                       \n",
            "intel.oneapi.lin.dpcpp_dbg          2024.0.0+49348  false     Intel® Distribution for GDB*                          \n",
            "intel.oneapi.lin.dpl                2022.3.0+49384  false     Intel® oneAPI DPC++ Library                           \n",
            "intel.oneapi.lin.tbb.devel          2021.11.0+49525 false     Intel® oneAPI Threading Building Blocks               \n",
            "intel.oneapi.lin.ccl.devel          2021.11.2+5     false     Intel® oneAPI Collective Communications Library       \n",
            "intel.oneapi.lin.dpcpp-cpp-compiler 2024.0.2+49895  false     Intel® oneAPI DPC++/C++ Compiler                      \n",
            " └─intel.oneapi.lin.dpl             2022.3.0+49384  false      └─Intel® oneAPI DPC++ Library                        \n",
            "intel.oneapi.lin.dal.devel          2024.0.1+25     false     Intel® oneAPI Data Analytics Library                  \n",
            "intel.oneapi.lin.ipp.devel          2021.10.0+668   false     Intel® Integrated Performance Primitives              \n",
            "intel.oneapi.lin.ippcp.devel        2021.9.1+5      false     Intel® Integrated Performance Primitives Cryptography \n",
            "intel.oneapi.lin.mkl.devel          2024.0.0+49671  false     Intel® oneAPI Math Kernel Library                     \n",
            "intel.oneapi.lin.advisor            2024.0.0+49520  false     Intel® Advisor                                        \n",
            "intel.oneapi.lin.vtune              2024.0.0+49501  false     Intel® VTune(TM) Profiler                             \n",
            "intel.oneapi.lin.dnnl               2024.0.0+49546  false     Intel® oneAPI Deep Neural Network Library             \n",
            " └─intel.oneapi.lin.tbb.devel       2021.11.0+49525 false      └─Intel® oneAPI Threading Building Blocks            \n",
            "Remove extracted files: /content/l_BaseKit_p_2024.0.1.46_offline...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 卸载oneapi\n",
        "!sh l_BaseKit_p_2024.0.1.46_offline.sh -a --action remove --eula accept -s\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dsDYE-cu9Rd",
        "outputId": "9c825fa1-3616-469a-f86e-34ca2f29ee81"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract l_BaseKit_p_2024.0.1.46_offline to /content/l_BaseKit_p_2024.0.1.46_offline...\n",
            "[#################################################################################################]\n",
            "Extract l_BaseKit_p_2024.0.1.46_offline completed!\n",
            "Checking system requirements...\n",
            "Done.\n",
            "Wait while the installer is preparing...\n",
            "Done.\n",
            "Launching the installer...\n",
            "Start installation flow...\n",
            "Log files: /tmp/unknown_user/intel_oneapi_installer/2024.01.04.04.35.42.059\n",
            "Removal has successfully completed\n",
            "Remove extracted files: /content/l_BaseKit_p_2024.0.1.46_offline...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装 MKL\n",
        "#!sh l_onemkl_p_2023.2.0.49497_offline.sh -a --components intel.oneapi.lin.mkl.devel --action install --eula accept -s\n",
        "!sh l_BaseKit_p_2024.0.1.46_offline.sh -a --action install --eula accept -s \\\n",
        "  --components intel.oneapi.lin.mkl.devel:intel.oneapi.lin.dpcpp-cpp-compiler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHM-7IXuTJ8y",
        "outputId": "8a47d1fe-f202-4de2-8202-a4261098dc54"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract l_BaseKit_p_2024.0.1.46_offline to /content/l_BaseKit_p_2024.0.1.46_offline...\n",
            "[#################################################################################################]\n",
            "Extract l_BaseKit_p_2024.0.1.46_offline completed!\n",
            "Checking system requirements...\n",
            "Done.\n",
            "Wait while the installer is preparing...\n",
            "Done.\n",
            "Launching the installer...\n",
            "Start installation flow...\n",
            "Installed Location: /opt/intel/oneapi\n",
            "Log files: /tmp/unknown_user/intel_oneapi_installer/2024.01.04.04.42.06.327\n",
            "Installation has successfully completed\n",
            "Remove extracted files: /content/l_BaseKit_p_2024.0.1.46_offline...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"/opt/intel/oneapi/mkl/latest/lib/intel64\" > /etc/ld.so.conf.d/mkl.conf && ldconfig && touch /etc/profile.d/intel.sh \\\n",
        "    && echo \".  /opt/intel/oneapi/mkl/latest/env/vars.sh\" >> /etc/profile.d/intel.sh && . /etc/profile.d/intel.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYucU8SiUlSx",
        "outputId": "143a3fbb-715f-407e-c6fe-859c8d8477d4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /opt/intel/oneapi/mkl/latest/lib/intel64/libmkl_sycl.so is not an ELF file - it has the wrong magic bytes at the start.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/opt/intel/oneapi/2024.0/bin/icx --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2jWXeLrzVr4",
        "outputId": "307ba0dc-a46c-45f4-918f-c46679f02af9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intel(R) oneAPI DPC++/C++ Compiler 2024.0.2 (2024.0.2.20231213)\n",
            "Target: x86_64-unknown-linux-gnu\n",
            "Thread model: posix\n",
            "InstalledDir: /opt/intel/oneapi/compiler/2024.0/bin/compiler\n",
            "Configuration file: /opt/intel/oneapi/compiler/2024.0/bin/compiler/../icx.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/opt/intel/oneapi/2024.0/bin/icpx --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsVGpHy7y_Te",
        "outputId": "cdd17660-7263-410f-a8a5-3175554241b5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intel(R) oneAPI DPC++/C++ Compiler 2024.0.2 (2024.0.2.20231213)\n",
            "Target: x86_64-unknown-linux-gnu\n",
            "Thread model: posix\n",
            "InstalledDir: /opt/intel/oneapi/compiler/2024.0/bin/compiler\n",
            "Configuration file: /opt/intel/oneapi/compiler/2024.0/bin/compiler/../icpx.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build llama.cpp with oneMKL\n",
        "\n",
        "使用 oneAPI 编译器构建将使不支持 avx512 和 avx512_vnni 的英特尔处理器可以使用 avx_vnni 指令集。\n",
        "\n",
        "请查看 [在英特尔 CPU 上优化和运行 LLaMA2](https://www.intel.com/content/www/us/en/content-details/791610/optimizing-and-running-llama2-on-intel-cpu.html) 获取更多信息。"
      ],
      "metadata": {
        "id": "PWWBesM9mHTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!source /opt/intel/oneapi/mkl/latest/env/vars.sh && echo $MKLROOT && echo $CPATH\n",
        "!cd /content/llama.cpp && rm -rf build && mkdir build && cd build \\\n",
        "  && cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=Intel10_64lp \\\n",
        "    -DCMAKE_C_COMPILER=/opt/intel/oneapi/2024.0/bin/icx \\\n",
        "    -DCMAKE_CXX_COMPILER=/opt/intel/oneapi/2024.0/bin/icpx -DLLAMA_NATIVE=ON \\\n",
        "  && cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVI8Wk9VlDPv",
        "outputId": "948d9feb-04b5-4a80-bd99-2d1866162ff4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/intel/oneapi/mkl/2024.0\n",
            "/opt/intel/oneapi/mkl/2024.0/include:\n",
            "-- The C compiler identification is IntelLLVM 2024.0.2\n",
            "-- The CXX compiler identification is IntelLLVM 2024.0.2\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /opt/intel/oneapi/2024.0/bin/icx - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /opt/intel/oneapi/2024.0/bin/icpx - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Looking for sgemm_\n",
            "-- Looking for sgemm_ - found\n",
            "-- Found BLAS: /usr/lib/x86_64-linux-gnu/libmkl_intel_lp64.so;/usr/lib/x86_64-linux-gnu/libmkl_intel_thread.so;/usr/lib/x86_64-linux-gnu/libmkl_core.so;/usr/local/lib/libiomp5.so;-lm;-ldl  \n",
            "-- BLAS found, Libraries: /usr/lib/x86_64-linux-gnu/libmkl_intel_lp64.so;/usr/lib/x86_64-linux-gnu/libmkl_intel_thread.so;/usr/lib/x86_64-linux-gnu/libmkl_core.so;/usr/local/lib/libiomp5.so;-lm;-ldl\n",
            "-- Found PkgConfig: /usr/bin/pkg-config (found version \"0.29.2\") \n",
            "-- Checking for module 'mkl-sdl'\n",
            "--   Found mkl-sdl, version 2024\n",
            "-- BLAS found, Includes: /opt/intel//oneapi//mkl/2024.0/lib/pkgconfig/../../include\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done (1.8s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/ggml.c:11177:23: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 11177 |             if (wp[i] == -INFINITY) {\u001b[0m\n",
            "       | \u001b[0;1;32m                ~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/ggml.c:12849:42: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 12849 |                         } else if (SS[j] == -INFINITY) {\u001b[0m\n",
            "       | \u001b[0;1;32m                                   ~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/ggml.c:13054:35: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 13054 |                         if (SS[j] == -INFINITY) {\u001b[0m\n",
            "       | \u001b[0;1;32m                            ~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/ggml.c:13502:50: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 13502 |                                 } else if (SR[j] == -INFINITY) {\u001b[0m\n",
            "       | \u001b[0;1;32m                                           ~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/ggml.c:14251:27: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 14251 |                 if (s0[i] == -INFINITY) {\u001b[0m\n",
            "       | \u001b[0;1;32m                    ~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/ggml.c:14365:27: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 14365 |                 if (s0[i] == -INFINITY) {\u001b[0m\n",
            "       | \u001b[0;1;32m                    ~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m6 warnings generated.\n",
            "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] Built target ggml\n",
            "[  5%] \u001b[32m\u001b[1mLinking C static library libggml_static.a\u001b[0m\n",
            "[  5%] Built target ggml_static\n",
            "[  6%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/llama.cpp:147:9: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  147 |     if (std::isinf(a) || std::isinf(b)) {\u001b[0m\n",
            "      | \u001b[0;1;32m        ^~~~~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/llama.cpp:147:26: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with infinity\n",
            "      in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  147 |     if (std::isinf(a) || std::isinf(b)) {\u001b[0m\n",
            "      | \u001b[0;1;32m                         ^~~~~~~~~~~~~\n",
            "\u001b[0m2 warnings generated.\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
            "[  7%] Built target llama\n",
            "[  8%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  9%] Built target build_info\n",
            "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/common/common.cpp:1454:98: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with\n",
            "      infinity in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 1454 |   ...= logit_bias_eos != sparams.logit_bias.end() && logit_bias_eos->second == -INFINITY;\u001b[0m\n",
            "      | \u001b[0;1;32m                                                     ~~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m1 warning generated.\n",
            "[ 11%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/common/sampling.cpp:277:63: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison\n",
            "      with infinity in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  277 |         bool is_valid = single_token_data_array.data[0].logit != -INFINITY;\u001b[0m\n",
            "      | \u001b[0;1;32m                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m1 warning generated.\n",
            "[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/common/train.cpp:1445:13: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with\n",
            "      NaN in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 1445 |         if (std::isnan(opt->loss_before) || std::isnan(opt->loss_after)) impr_plot = 0;\u001b[0m\n",
            "      | \u001b[0;1;32m            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/common/train.cpp:1445:45: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with\n",
            "      NaN in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 1445 |         if (std::isnan(opt->loss_before) || std::isnan(opt->loss_after)) impr_plot = 0;\u001b[0m\n",
            "      | \u001b[0;1;32m                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\u001b[0m2 warnings generated.\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 15%] Built target common\n",
            "[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 17%] Built target test-quantize-fns\n",
            "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 19%] Built target test-quantize-perf\n",
            "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 21%] Built target test-sampling\n",
            "[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-llama\u001b[0m\n",
            "[ 23%] Built target test-tokenizer-0-llama\n",
            "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-falcon\u001b[0m\n",
            "[ 25%] Built target test-tokenizer-0-falcon\n",
            "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-llama\u001b[0m\n",
            "[ 27%] Built target test-tokenizer-1-llama\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 29%] Built target test-tokenizer-1-bpe\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 31%] Built target test-grammar-parser\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "In file included from /content/llama.cpp/tests/test-llama-grammar.cpp:5:\n",
            "\u001b[1m/content/llama.cpp/./llama.cpp:147:9: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with\n",
            "      infinity in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  147 |     if (std::isinf(a) || std::isinf(b)) {\u001b[0m\n",
            "      | \u001b[0;1;32m        ^~~~~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/./llama.cpp:147:26: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit comparison with\n",
            "      infinity in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  147 |     if (std::isinf(a) || std::isinf(b)) {\u001b[0m\n",
            "      | \u001b[0;1;32m                         ^~~~~~~~~~~~~\n",
            "\u001b[0m2 warnings generated.\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 33%] Built target test-llama-grammar\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
            "[ 35%] Built target test-grad0\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/tests/test-backend-ops.cpp:239:12: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit\n",
            "      comparison with infinity in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  239 |     return std::isinf(f) || f == FLT_MAX || f == -FLT_MAX;\u001b[0m\n",
            "      | \u001b[0;1;32m           ^~~~~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/tests/test-backend-ops.cpp:424:21: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit\n",
            "      comparison with NaN in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  424 |                 if (std::isnan(f1[i]) || std::isnan(f2[i])) {\u001b[0m\n",
            "      | \u001b[0;1;32m                    ^~~~~~~~~~~~~~~~~\n",
            "\u001b[0m\u001b[1m/content/llama.cpp/tests/test-backend-ops.cpp:424:42: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit\n",
            "      comparison with NaN in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            "  424 |                 if (std::isnan(f1[i]) || std::isnan(f2[i])) {\u001b[0m\n",
            "      | \u001b[0;1;32m                                         ^~~~~~~~~~~~~~~~~\n",
            "\u001b[0m3 warnings generated.\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 37%] Built target test-backend-ops\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 39%] Built target test-rope\n",
            "[ 40%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-c\u001b[0m\n",
            "[ 41%] Built target test-c\n",
            "[ 42%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/examples/baby-llama/baby-llama.cpp:1271:30: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m\n",
            "      explicit comparison with infinity in fast floating point mode\n",
            "      [-Wtautological-constant-compare]\u001b[0m\n",
            " 1271 |             float p = (logit == -INFINITY) ? 0 : expf(logit - max_logit);\u001b[0m\n",
            "      | \u001b[0;1;32m                       ~~~~~ ^  ~~~~~~~~~\n",
            "\u001b[0m1 warning generated.\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
            "[ 43%] Built target baby-llama\n",
            "[ 44%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched\u001b[0m\n",
            "[ 45%] Built target batched\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched-bench\u001b[0m\n",
            "[ 47%] Built target batched-bench\n",
            "[ 48%] \u001b[32mBuilding CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/beam-search\u001b[0m\n",
            "[ 50%] Built target beam-search\n",
            "[ 51%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
            "[ 52%] Built target benchmark\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/convert-llama2c-to-ggml\u001b[0m\n",
            "[ 54%] Built target convert-llama2c-to-ggml\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
            "[ 56%] Built target embedding\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/finetune\u001b[0m\n",
            "[ 58%] Built target finetune\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/infill\u001b[0m\n",
            "[ 60%] Built target infill\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 62%] Built target llama-bench\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 64%] Built target llava\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 65%] Built target llava_static\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llava-cli\u001b[0m\n",
            "[ 67%] Built target llava-cli\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
            "[ 69%] Built target main\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/tokenize\u001b[0m\n",
            "[ 71%] Built target tokenize\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/parallel\u001b[0m\n",
            "[ 73%] Built target parallel\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
            "[ 75%] Built target perplexity\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
            "[ 77%] Built target quantize\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
            "[ 79%] Built target quantize-stats\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
            "[ 81%] Built target save-load-state\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/simple\u001b[0m\n",
            "[ 83%] Built target simple\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/speculative\u001b[0m\n",
            "[ 85%] Built target speculative\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookahead\u001b[0m\n",
            "[ 87%] Built target lookahead\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookup\u001b[0m\n",
            "[ 89%] Built target lookup\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
            "[ 91%] Built target train-text-from-scratch\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/server.dir/server.cpp.o\u001b[0m\n",
            "\u001b[1m/content/llama.cpp/examples/server/server.cpp:1215:60: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mexplicit\n",
            "      comparison with infinity in fast floating point mode [-Wtautological-constant-compare]\u001b[0m\n",
            " 1215 |                                 eos_bias->second < 0.0f && std::isinf(eos_bias->second);\u001b[0m\n",
            "      | \u001b[0;1;32m                                                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\u001b[0m1 warning generated.\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/server\u001b[0m\n",
            "[ 93%] Built target server\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/export-lora\u001b[0m\n",
            "[ 95%] Built target export-lora\n",
            "[ 96%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
            "[ 97%] Built target vdot\n",
            "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
            "[100%] Built target q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!lscpu | grep avx\n",
        "!lscpu | grep avx2\n",
        "# no avx_vnni\n",
        "!lscpu | grep avx_vnni"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B97xUN5-rg7e",
        "outputId": "ec3a941c-40af-47cd-85ce-e9668ccefdd7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference"
      ],
      "metadata": {
        "id": "LVpvgKwc25E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeG59POk3gqx",
        "outputId": "e83756a3-d4dd-4aaa-e1e4-19f9c156bc2c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llama.cpp/build/bin/main -m gguf-tools/models/phi-2.Q8_0.gguf --color \\\n",
        "  -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 \\\n",
        "  --multiline-input \\\n",
        "  -p \"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n{prompt}[/INST]\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRSm3foL1PMm",
        "outputId": "dc8c40f8-2128-4fc3-d64e-d788171e05eb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 1761 (cb1e281)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1704344933\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from gguf-tools/models/phi-2.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
            "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
            "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  195 tensors\n",
            "llama_model_loader: - type q8_0:  130 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 910/51200 vs 944/51200 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 51200\n",
            "llm_load_print_meta: n_merges         = 50000\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2560\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 32\n",
            "llm_load_print_meta: n_embd_head_k    = 80\n",
            "llm_load_print_meta: n_embd_head_v    = 80\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 10240\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 2.78 B\n",
            "llm_load_print_meta: model size       = 2.75 GiB (8.51 BPW) \n",
            "llm_load_print_meta: general.name     = Phi2\n",
            "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_tensors: ggml ctx size       =    0.12 MiB\n",
            "llm_load_tensors: system memory used  = 2819.40 MiB\n",
            ".............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
            "llama_build_graph: non-view tensors processed: 774/774\n",
            "llama_new_context_with_model: compute buffer total size = 297.19 MiB\n",
            "main: warning: model was trained on only 2048 context tokens (4096 specified)\n",
            "\n",
            "system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.700\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
            "generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n{prompt}[/INST]\u001b[0m\")\n",
            "if input(\"Do you want another prompt? (y/n)\").lower() != \"y\":\n",
            "break\n",
            "\n",
            "\n",
            "# --------------------------------------------------\n",
            "if __name__ == '__main__':\n",
            "main() [end of text]\n",
            "\n",
            "llama_print_timings:        load time =   21094.05 ms\n",
            "llama_print_timings:      sample time =      24.36 ms /    49 runs   (    0.50 ms per token,  2011.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12901.13 ms /   124 tokens (  104.04 ms per token,     9.61 tokens per second)\n",
            "llama_print_timings:        eval time =    6071.08 ms /    48 runs   (  126.48 ms per token,     7.91 tokens per second)\n",
            "llama_print_timings:       total time =   19021.04 ms\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llama.cpp/build/bin/main -m gguf-tools/models/llama-2-7b-chat.Q4_K_M.gguf --color \\\n",
        "  -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 \\\n",
        "  --multiline-input \\\n",
        "  -i -ins\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re69_Ht_29uI",
        "outputId": "e70d7926-d78f-4b8c-aa49-0829445a6ceb"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 1761 (cb1e281)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1704345853\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from gguf-tools/models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
            "llm_load_tensors: system memory used  = 3891.35 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_build_graph: non-view tensors processed: 676/676\n",
            "llama_new_context_with_model: compute buffer total size = 291.19 MiB\n",
            "\n",
            "system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "main: interactive mode on.\n",
            "Reverse prompt: '### Instruction:\n",
            "\n",
            "'\n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.700\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
            "generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - To return control to LLaMa, end your input with '\\'.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            "\n",
            "\u001b[33m\u001b[0m\n",
            "> \u001b[1m\u001b[32mhi\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mHi! How are you today?\n",
            "\n",
            "> \u001b[1m\u001b[32mwhat's you name?\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mMy name is John.\n",
            "\n",
            "> \u001b[1m\u001b[32mcan u speeak chinese ?\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mI apologize, but I don't speak Chinese. I am a machine learning model trained on text data, and while I can generate responses to various prompts, I do not have the ability to speak or understand any language other than English. Is there anything else I can help you with?\n",
            "\n",
            "> \u001b[1m\u001b[32mok, can u explain io_uring?\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mI'm not familiar with the term \"io_uring\". Could you please provide more context or information about what this term refers to? I want to make sure I give you an accurate and helpful response.\n",
            "\n",
            "> \u001b[1m\u001b[32mio_uring in linux kernel\n",
            "\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mIn the Linux kernel, io_uring is a high-performance I/O scheduling mechanism that provides efficient and scalable I/O processing. It was introduced in the Linux kernel version 3.10 as an alternative to the traditional I/O scheduler, which had limitations in handling high-bandwidth and low-latency I/O workloads.\n",
            "io_uring is designed to provide a more efficient and flexible I/O scheduling mechanism that can handle a wide range of I/O workloads, including those with varying I/O patterns and priorities. It uses a ring buffer data structure to store I/O requests, which allows for more efficient handling of concurrent I/O operations.\n",
            "io_uring provides several benefits over traditional I/O schedulers, including:\n",
            "* Better performance: io_uring can handle high-bandwidth and low-latency I/O workloads more efficiently than traditional I/O schedulers.\n",
            "* Scalability: io_uring can handle a large number of I/O operations concurrently without significant performance degradation.\n",
            "* Flexibility: io_uring provides fine-grained control over I/O operations, allowing for customized I/O processing based on the specific needs of the system and application.\n",
            "Overall, io_uring is a powerful tool for optimizing I/O performance in Linux systems, particularly those with high-bandwidth and low-latency I/O workloads.\n",
            "\n",
            "> \u001b[1m\u001b[32mcan u write  code about io_uring?\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mCertainly! Here is an example of how to use the io_uring API in Linux to perform I/O operations efficiently:\n",
            "```c\n",
            "#include <linux/io_uring.h>\n",
            "int main() {\n",
            "    // Create a ring buffer for storing I/O requests\n",
            "    struct io_uring ring;\n",
            "    int err = io_uring_init(&ring, 1024);\n",
            "    if (err) {\n",
            "        printk(KERN_ERR \"io_uring_init failed: %d\\n\", err);\n",
            "        return -ENOMEM;\n",
            "    }\n",
            "    // Submit I/O requests to the ring buffer\n",
            "    struct iovec vec[3];\n",
            "    vec[0].iov_base = \"Hello, world!\";\n",
            "    vec[0].iov_len = strlen(\"Hello, world!\");\n",
            "    vec[1].iov_base = &my_number;\n",
            "    vec[1].iov_len = sizeof(int);\n",
            "    vec[2].iov_base = NULL;\n",
            "    err = io_uring_submit(&ring, 3, vec, IO_URING_NOWAIT);\n",
            "    if (err) {\n",
            "        printk(KERN_ERR \"io_uring_submit failed: %d\\n\", err);\n",
            "        return -ENOMEM;\n",
            "    }\n",
            "    // Wait for the I/O operations to complete\n",
            "    int result;\n",
            "    do {\n",
            "        result = io_uring_poll(&ring, 1, 100);\n",
            "        if (result > 0) {\n",
            "            printk(KERN_INFO \"I/O operation completed successfully\\n\");\n",
            "        } else if (result == -ETIMEDOUT) {\n",
            "            printk(KERN_INFO \"I/O operation timed out\\n\");\n",
            "    } else {\n",
            "        printk(KERN_ERR \"io_uring_poll failed: %d\\n\", result);\n",
            "\n",
            "    }\n",
            "\n",
            "return 0;\n",
            "}\n",
            "```\n",
            "This code creates a ring buffer for storing I/O requests, submits three I/O operations to the ring buffer using the `io_uring_submit()` function, and then waits for the operations to complete using the `io_uring_poll()` function. The `IO_URING_NOWAIT` flag is used with `io_uring_submit()` to indicate that the function should not block if there are no available resources.\n",
            "Note: This code is just an example and may not work as-is on your system. You will need to modify it to suit your specific use case and requirements.\n",
            "I hope this helps! Let me know if you have any questions.\n",
            "\n",
            "> \u001b[1m\u001b[32meq\\\u001b[33m\b\\\b \b\n",
            "\u001b[0mThank\n",
            "\n",
            "llama_print_timings:        load time =    3615.77 ms\n",
            "llama_print_timings:      sample time =     342.96 ms /  1032 runs   (    0.33 ms per token,  3009.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20331.92 ms /   161 tokens (  126.29 ms per token,     7.92 tokens per second)\n",
            "llama_print_timings:        eval time =  220160.49 ms /  1032 runs   (  213.33 ms per token,     4.69 tokens per second)\n",
            "llama_print_timings:       total time =  531075.72 ms\n"
          ]
        }
      ]
    }
  ]
}