{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqC7Xqn4U1Dj0VKUoY37Q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/deepspeed_MII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "legacy\n",
        "- https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy\n",
        "- https://towardsdatascience.com/deepspeed-deep-dive-model-implementations-for-inference-mii-b02aa5d5e7f7\n",
        "- https://www.deepspeed.ai/2022/10/10/mii.html\n",
        "- [**DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale**](https://arxiv.org/abs/2207.00032)\n",
        "\n",
        "fastgen\n",
        "- https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/README.md\n",
        "- https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/chinese/README.md\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8SApMofRDid_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nki3tY8wCeSg",
        "outputId": "f6202767-2ecf-4564-ca2e-3bf54a2356f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed-mii\n",
            "  Downloading deepspeed_mii-0.1.2-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Flask-RESTful (from deepspeed-mii)\n",
            "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (9.4.0)\n",
            "Requirement already satisfied: Werkzeug in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (3.0.1)\n",
            "Collecting asyncio (from deepspeed-mii)\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed-kernels (from deepspeed-mii)\n",
            "  Downloading deepspeed_kernels-0.0.1.dev1698255861-py3-none-manylinux1_x86_64.whl (44.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed>=0.12.4 (from deepspeed-mii)\n",
            "  Downloading deepspeed-0.12.4.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (1.59.3)\n",
            "Collecting grpcio-tools (from deepspeed-mii)\n",
            "  Downloading grpcio_tools-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (1.10.13)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (0.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from deepspeed-mii) (4.35.2)\n",
            "Collecting ujson (from deepspeed-mii)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zmq (from deepspeed-mii)\n",
            "  Downloading zmq-0.0.0.zip (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson (from deepspeed>=0.12.4->deepspeed-mii)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed>=0.12.4->deepspeed-mii)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (9.0.0)\n",
            "Collecting pynvml (from deepspeed>=0.12.4->deepspeed-mii)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.12.4->deepspeed-mii) (4.66.1)\n",
            "Requirement already satisfied: cmake>=3.24 in /usr/local/lib/python3.10/dist-packages (from deepspeed-kernels->deepspeed-mii) (3.27.9)\n",
            "Collecting aniso8601>=0.82 (from Flask-RESTful->deepspeed-mii)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from Flask-RESTful->deepspeed-mii) (2.2.5)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from Flask-RESTful->deepspeed-mii) (1.16.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from Flask-RESTful->deepspeed-mii) (2023.3.post1)\n",
            "Collecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools->deepspeed-mii)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio (from deepspeed-mii)\n",
            "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools->deepspeed-mii) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed-mii) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed-mii) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->deepspeed-mii) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug->deepspeed-mii) (2.1.3)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from zmq->deepspeed-mii) (23.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->Flask-RESTful->deepspeed-mii) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->Flask-RESTful->deepspeed-mii) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepspeed-mii) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed-mii) (1.3.0)\n",
            "Building wheels for collected packages: deepspeed, zmq\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.12.4-py3-none-any.whl size=1290644 sha256=5f01d0ad84aba0493b6c70f79c6323ff9a10033e083a1523f2b62501672962d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/0c/52/f464610477b069120f740202a9d84a27f9d7235cbf035c4b75\n",
            "  Building wheel for zmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zmq: filename=zmq-0.0.0-py3-none-any.whl size=1264 sha256=7f905ffe0cd2a9cc29ecd3c3fd5b3da8f49c1f471e601d375f537d99b7a95ec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/c5/fe/d853f71843cae26c123d37a7a5934baac20fc66f35a913951d\n",
            "Successfully built deepspeed zmq\n",
            "Installing collected packages: ninja, hjson, asyncio, aniso8601, zmq, ujson, pynvml, protobuf, grpcio, deepspeed-kernels, grpcio-tools, Flask-RESTful, deepspeed, deepspeed-mii\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.59.3\n",
            "    Uninstalling grpcio-1.59.3:\n",
            "      Successfully uninstalled grpcio-1.59.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Flask-RESTful-0.3.10 aniso8601-9.0.1 asyncio-3.4.3 deepspeed-0.12.4 deepspeed-kernels-0.0.1.dev1698255861 deepspeed-mii-0.1.2 grpcio-1.60.0 grpcio-tools-1.60.0 hjson-3.1.0 ninja-1.11.1.1 protobuf-4.25.1 pynvml-11.5.0 ujson-5.9.0 zmq-0.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "asyncio"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install deepspeed-mii"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ds_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLdzy63DJ2u-",
        "outputId": "e38ff93a-89af-4c58-f82e-8cca9b8d1245"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:10:55,461] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "async_io ............... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "fused_adam ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_lion ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "evoformer_attn ......... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "fused_lamb ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lion ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "inference_core_ops ..... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "cutlass_ops ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "ragged_device_ops ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "ragged_ops ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/usr/local/lib/python3.10/dist-packages/torch']\n",
            "torch version .................... 2.1.0+cu118\n",
            "deepspeed install path ........... ['/usr/local/lib/python3.10/dist-packages/deepspeed']\n",
            "deepspeed info ................... 0.12.4, unknown, unknown\n",
            "torch cuda version ............... 11.8\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.1, cuda 11.8\n",
            "shared memory (/dev/shm) size .... 24.62 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/DeepSpeed-MII"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shaTH9qEYZsr",
        "outputId": "a8648ee6-2f84-4da3-ee00-e56bf953f244"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepSpeed-MII'...\n",
            "remote: Enumerating objects: 3197, done.\u001b[K\n",
            "remote: Counting objects: 100% (1806/1806), done.\u001b[K\n",
            "remote: Compressing objects: 100% (677/677), done.\u001b[K\n",
            "remote: Total 3197 (delta 1384), reused 1326 (delta 1104), pack-reused 1391\u001b[K\n",
            "Receiving objects: 100% (3197/3197), 6.35 MiB | 1.30 MiB/s, done.\n",
            "Resolving deltas: 100% (2079/2079), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSpeed-Legacy\n",
        "\n",
        "ds mii 以前的组件\n",
        "\n",
        "https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "深度学习 (DL) 开源社区在过去几个月中出现了巨大的增长。极其强大的文本生成模型（如 Bloom 176B）或图像生成模型（如稳定扩散）现在可供通过 Hugging Face 等平台访问少量甚至单个 GPU 的任何人使用。尽管开源使人工智能功能的访问变得民主化，但其应用仍然受到两个关键因素的限制：推理延迟和成本。\n",
        "\n",
        "深度学习模型推理的系统优化已经取得了重大进展，可以大大减少延迟和成本，但这些进展并不容易实现。这种可访问性有限的主要原因是深度学习模型推理环境多种多样，模型的大小、架构、系统性能特征、硬件要求等各不相同。确定适用于给定模型的适当系统优化集并正确应用它们是很重要的。通常超出了大多数数据科学家的能力范围，导致低延迟和低成本推理几乎无法实现。\n",
        "\n",
        "DeepSpeed-MII 是 DeepSpeed 的一个新的开源 Python 库，旨在使强大模型的低延迟、低成本推理不仅可行，而且易于访问。\n",
        "\n",
        "- MII 提供对数千种广泛使用的深度学习模型的高度优化实现。\n",
        "- 与原始实施相比，MII 支持的模型可显着降低延迟和成本。例如，MII 将 Big-Science Bloom 176B 模型的延迟降低了 5.7 倍，同时将成本降低了 40 倍以上。同样，它将部署稳定扩散的延迟和成本降低了 1.9 倍。查看更多详细信息，[了解MII 的详尽延迟和成本分析](https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy#quantifying-latency-and-cost-reduction)。\n",
        "- 为了实现低延迟/成本推理，MII 利用 DeepSpeed-Inference 的一系列优化，例如transformer的深度融合、用于多 GPU 推理的自动张量切片、使用 ZeroQuant 进行动态量化以及其他几个（请参阅我们的博客文章[了解更多详细信息](https://www.deepspeed.ai/2022/10/10/mii.html)）。\n",
        "- 凭借最先进的性能，MII 只需几行代码即可通过 AML 在本地和 Azure 上低成本部署这些模型。\n",
        "\n",
        "\n",
        "![](https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/legacy/docs/images/mii-arch.png?raw=true)\n",
        "MII 架构，显示 MII 如何使用 DS-Inference 自动优化 OSS 模型，然后使用 GRPC 在本地部署，或使用 AML Inference 在 Microsoft Azure 上部署。\n",
        "\n",
        "MII 的底层由DeepSpeed-Inference提供支持。根据模型类型、模型大小、批量大小和可用硬件资源，MII 自动应用 DeepSpeed-Inference 中的一组适当的系统优化，以最小化延迟并最大化吞吐量。它通过使用许多预先指定的模型注入策略之一来实现这一点，该策略允许 MII 和 DeepSpeed-Inference 识别底层 PyTorch 模型架构并用优化的实现替换它（见图A）。在此过程中，MII 使其 DeepSpeed-Inference 中的广泛优化自动可用于其支持的数千种流行模型。\n",
        "\n",
        "\n",
        "MII 可以使用 DeepSpeed-Inference 的两种变体。\n",
        "- 第一个称为 ds-public，包含此处讨论的大部分 DeepSpeed-Inference 优化，也可以通过我们的开源 DeepSpeed 库获得。\n",
        "- 第二个称为 ds-azure，提供与 Azure 更紧密的集成，并可通过 MII 向所有 Microsoft Azure 客户提供。我们将运行两个 DeepSpeed-Inference 变体的 MII 分别称为 MII-Public 和 MII-Azure。\n",
        "\n",
        "虽然这两种变体都比开源 PyTorch 基线提供了显着的延迟和成本降低，但后者为基于生成的工作负载提供了额外的性能优势。此处提供了与 PyTorch 基线以及这两个版本之间的完整延迟和成本优势比较。\n",
        "\n",
        "云服务部署方式大同小异，aws部署方式差不多\n",
        "\n",
        "这里介绍第一种方式\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r4xJvVWnP_lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 部署"
      ],
      "metadata": {
        "id": "nk-SjUbJV2cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MII-Public 可以部署在本地或任何云产品上，只需几行代码。MII 创建一个轻量级 GRPC 服务器来支持这种形式的部署，并为查询提供 GRPC 推理端点。\n",
        "\n",
        "# 作为示例，这里是Hugging Face 的bigscience/bloom-560m模型的部署：\n",
        "import mii\n",
        "mii_configs = {\"tensor_parallel\": 1, \"dtype\": \"fp16\"}\n",
        "mii.deploy(task=\"text-generation\",\n",
        "           model=\"bigscience/bloom-560m\",\n",
        "           deployment_name=\"bloom560m_deployment\",\n",
        "           mii_config=mii_configs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SguRnOWkVlEG",
        "outputId": "37411537-640f-4052-b633-0645ce9c00b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:11:19,436] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:12:42,722] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:12:42,722] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:12:42,727] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:12:42,728] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:12:42,735] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:12:42,735] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:12:42,738] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp30gbq06t', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:42,738] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp30gbq06t', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:42,741] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:42,741] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'bloom560m_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJiaWdzY2llbmNlL2Jsb29tLTU2MG0iLCAidGFzayI6ICJ0ZXh0LWdlbmVyYXRpb24iLCAiZHR5cGUiOiAidG9yY2guZmxvYXQxNiIsICJtb2RlbF9wYXRoIjogIi90bXAvbWlpX21vZGVscyIsICJsb2FkX3dpdGhfc3lzX21lbSI6IGZhbHNlLCAibWV0YV90ZW5zb3IiOiBmYWxzZSwgImRlcGxveV9yYW5rIjogWzBdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJyZXBsaWNhX251bSI6IDEsICJyZXBsaWNhX2NvbmZpZ3MiOiBbeyJob3N0bmFtZSI6ICJsb2NhbGhvc3QiLCAidGVuc29yX3BhcmFsbGVsX3BvcnRzIjogWzUwMDUxXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiZ3B1X2luZGljZXMiOiBbMF19XSwgInByb2ZpbGVfbW9kZWxfdGltZSI6IGZhbHNlLCAic2tpcF9tb2RlbF9jaGVjayI6IHRydWUsICJoZl9hdXRoX3Rva2VuIjogbnVsbCwgInRydXN0X3JlbW90ZV9jb2RlIjogZmFsc2UsICJwaXBlbGluZV9rd2FyZ3MiOiB7fSwgImVuYWJsZV9kZWVwc3BlZWQiOiB0cnVlLCAiZW5hYmxlX3plcm8iOiBmYWxzZSwgImRzX2NvbmZpZyI6IHt9LCAidGVuc29yX3BhcmFsbGVsIjogMSwgImVuYWJsZV9jdWRhX2dyYXBoIjogZmFsc2UsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJjaGVja3BvaW50X2RpY3QiOiBudWxsLCAibWF4X3Rva2VucyI6IDEwMjR9']\n",
            "[2023-12-13 09:12:47,749] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:47,749] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:52,756] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:52,756] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:57,764] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:12:57,764] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:02,768] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:02,768] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:07,775] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:07,775] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:12,782] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:12,782] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:17,789] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:17,789] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:22,796] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:22,796] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:27,803] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:27,803] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:32,809] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:32,809] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:37,815] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:37,815] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:42,822] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:42,822] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:47,830] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:47,830] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:52,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:52,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:57,845] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:13:57,845] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:02,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:02,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:07,859] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:07,859] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:12,865] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:12,865] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:17,873] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:17,873] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:22,879] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:22,879] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:27,887] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:27,887] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:32,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:32,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:37,904] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:37,904] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:42,913] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:42,913] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:47,921] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:47,921] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:52,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:52,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:57,935] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:14:57,935] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:02,942] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:02,942] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:07,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:07,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:15:07,952] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n",
            "[2023-12-13 09:15:07,952] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeGFUem0dYtS",
        "outputId": "667ef9e5-58da-4431-fe54-a9b7ea60c080"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 1480 root   40u  IPv4  62907      0t0  TCP *:50051 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep 1480"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taw703dDddCF",
        "outputId": "a7cee095-dd7f-4482-9903-5d33f678edff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        1480    1423 45 09:12 ?        00:01:21 /usr/bin/python3 -m mii.legacy.launch.multi_gpu_\n",
            "root        2486     392  0 09:15 ?        00:00:00 /bin/bash -c ps -ef | grep 1480\n",
            "root        2488    2486  0 09:15 ?        00:00:00 grep 1480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 查询"
      ],
      "metadata": {
        "id": "K1qjbYn5V36j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 唯一需要的键是\"query\"，字典之外的所有其他项目都将作为generatekwargs 传递。对于 Hugging Face 提供的模型，您可以在其生成文档中找到所有可能的参数。\n",
        "#import nest_asyncio\n",
        "#nest_asyncio.apply()\n",
        "\n",
        "import mii\n",
        "generator = mii.mii_query_handle(\"bloom560m_deployment\")\n",
        "result = generator.query({\"query\": [\"DeepSpeed is\", \"Seattle is\"]}, do_sample=True, max_new_tokens=30)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPmaQQ5EV8iY",
        "outputId": "926459aa-a6d1-483b-f9d5-3d62d81e41ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:24:59,842] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:24:59,843] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "response: \"DeepSpeed is [ ] \"\n",
            "response: \"Seattle is The Crown:\\n The...\\n...\\n...\\n.........\\n...\\n...\\n...\\n...\\n...\\n...\\n......\\n...\\n......\\n...\\n......\\n...\\n...\\n...\\n...\\n......\\n\"\n",
            "time_taken: 0.654122353\n",
            "model_time_taken: -1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 负载均衡\n",
        "\n",
        "您可以启动负载平衡器和 MII 服务器的多个副本。当您指定 的值时replica_num，mii.deploy()启动负载均衡器服务器和replica_num副本数。请注意，每个副本均由部署在同一服务器上的tensor_parallel服务器进程组成。\n",
        "```\n",
        "mii_configs = {\n",
        "...\n",
        "    \"tensor_parallel\": tensor_parallel,\n",
        "    \"replica_num\": replica_num,\n",
        "    \"hostfile\": hostfile\n",
        "}\n",
        "mii.deploy(...\n",
        "           mii_config=mii_configs,\n",
        "           ...)\n",
        "```\n",
        "\n",
        "客户端将请求发送到负载均衡器，负载均衡器将请求转发到副本，而不是将请求发送到各个 MII 服务器。目前，负载均衡器实现了简单的循环算法。replica_num当设置为 1 时，负载均衡器充当简单代理。\n",
        "\n",
        "hostfile是 DeepSpeed 启动器使用的主机文件的路径。当未指定 hostfile 时，DeepSpeed-MII 使用/job/hostfile为 DeepSpeed 定义的默认路径。有关详细信息，请参阅DeepSpeed 的文档。\n",
        "\n",
        "\n",
        "https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node"
      ],
      "metadata": {
        "id": "uhNXb6HCdGhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 关闭服务"
      ],
      "metadata": {
        "id": "C-QBdUiJWo2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.terminate(\"bloom560m_deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftp-NJ4aWsHe",
        "outputId": "a3c8848e-4ddb-4769-e431-602908448ff6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:25:31,531] [INFO] [terminate.py:12:terminate] Terminating server for bloom560m_deployment\n",
            "[2023-12-13 09:25:31,531] [INFO] [terminate.py:12:terminate] Terminating server for bloom560m_deployment\n",
            "[2023-12-13 09:25:31,535] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:25:31,536] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:25:31,550] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:25:31,551] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## chat\n",
        "\n",
        "https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/legacy/examples/local/chat/README.md\n",
        "\n",
        "\n",
        "**训练笔记：https://github.com/weedge/doraemon-nb/blob/main/ds_examples_chatbot.ipynb**\n",
        "\n",
        "\n",
        "使用[DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/README.md)通过RLHF训练的模型，已经上传至huggingface:\n",
        "\n",
        "https://huggingface.co/AdamG012/chat-opt-1.3b-rlhf-actor-deepspeed"
      ],
      "metadata": {
        "id": "1YFj5Ck3Zr2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy start server\n",
        "!cd DeepSpeed-MII/mii/legacy/examples/local/chat && (nohup python chat-server-example.py &)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY6YhwXJZMgl",
        "outputId": "f866cb4f-7eda-42da-b327-f78d51f45018"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat DeepSpeed-MII/mii/legacy/examples/local/chat/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUTAidYMkMjl",
        "outputId": "1689415c-6b1d-4710-e052-b2a1a731fe0c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:45:03,835] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:45:05.726142: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:45:05.726202: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:45:05.726229: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:45:06.874407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Deploying AdamG012/chat-opt-1.3b-rlhf-actor-deepspeed...\n",
            "[2023-12-13 09:45:44,816] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:45:44,816] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:45:44,818] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:44,818] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:44,824] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:45:44,824] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:45:44,825] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp0staojpq', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:44,825] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp0staojpq', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:44,827] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:44,827] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'chat_example_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==']\n",
            "[2023-12-13 09:45:47,160] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:45:47,233] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:45:48,991] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=localhost --master_port=29500 --no_python --no_local_rank --enable_each_rank_log=None /usr/bin/python3 -m mii.legacy.launch.multi_gpu_server --deployment-name chat_example_deployment --load-balancer-port 50050 --restful-gateway-port 51080 --server-port 50051 --model-config eyJtb2RlbCI6ICJBZGFtRzAxMi9jaGF0LW9wdC0xLjNiLXJsaGYtYWN0b3ItZGVlcHNwZWVkIiwgInRhc2siOiAidGV4dC1nZW5lcmF0aW9uIiwgImR0eXBlIjogInRvcmNoLmZsb2F0MzIiLCAibW9kZWxfcGF0aCI6ICIvdG1wL21paV9tb2RlbHMiLCAibG9hZF93aXRoX3N5c19tZW0iOiBmYWxzZSwgIm1ldGFfdGVuc29yIjogZmFsc2UsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAicmVwbGljYV9udW0iOiAxLCAicmVwbGljYV9jb25maWdzIjogW3siaG9zdG5hbWUiOiAibG9jYWxob3N0IiwgInRlbnNvcl9wYXJhbGxlbF9wb3J0cyI6IFs1MDA1MV0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgImdwdV9pbmRpY2VzIjogWzBdfV0sICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiB0cnVlLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJ0cnVzdF9yZW1vdGVfY29kZSI6IGZhbHNlLCAicGlwZWxpbmVfa3dhcmdzIjoge30sICJlbmFibGVfZGVlcHNwZWVkIjogdHJ1ZSwgImVuYWJsZV96ZXJvIjogZmFsc2UsICJkc19jb25maWciOiB7fSwgInRlbnNvcl9wYXJhbGxlbCI6IDEsICJlbmFibGVfY3VkYV9ncmFwaCI6IGZhbHNlLCAicmVwbGFjZV93aXRoX2tlcm5lbF9pbmplY3QiOiB0cnVlLCAiY2hlY2twb2ludF9kaWN0IjogbnVsbCwgIm1heF90b2tlbnMiOiAxMDI0fQ==\n",
            "2023-12-13 09:45:49.299986: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:45:49.300052: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:45:49.300079: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2023-12-13 09:45:49,829] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:49,829] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "2023-12-13 09:45:50.496774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:45:51,242] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:45:51,666] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:51,666] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2023-12-13 09:45:52,923] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2023-12-13 09:45:54,833] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:54,833] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:55,196] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:45:57.061922: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:45:57.061972: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:45:57.062006: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:45:58.164593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:45:59,267] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:45:59,267] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "\rconfig.json:   0%|          | 0.00/800 [00:00<?, ?B/s]\rconfig.json: 100%|██████████| 800/800 [00:00<00:00, 4.31MB/s]\n",
            "[2023-12-13 09:45:59,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:45:59,838] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:   0%|          | 0.00/3.24G [00:00<?, ?B/s]\rpytorch_model.bin:   0%|          | 10.5M/3.24G [00:00<04:18, 12.5MB/s]\rpytorch_model.bin:   1%|          | 21.0M/3.24G [00:01<02:36, 20.5MB/s]\rpytorch_model.bin:   1%|          | 31.5M/3.24G [00:01<02:16, 23.5MB/s]\rpytorch_model.bin:   1%|▏         | 41.9M/3.24G [00:01<01:55, 27.8MB/s]\rpytorch_model.bin:   2%|▏         | 52.4M/3.24G [00:02<01:43, 30.7MB/s]\rpytorch_model.bin:   2%|▏         | 62.9M/3.24G [00:02<01:45, 30.2MB/s]\rpytorch_model.bin:   2%|▏         | 73.4M/3.24G [00:02<01:38, 32.2MB/s]\rpytorch_model.bin:   3%|▎         | 83.9M/3.24G [00:02<01:35, 33.0MB/s]\rpytorch_model.bin:   3%|▎         | 94.4M/3.24G [00:03<01:31, 34.4MB/s]\rpytorch_model.bin:   3%|▎         | 105M/3.24G [00:03<01:28, 35.4MB/s] \rpytorch_model.bin:   4%|▎         | 115M/3.24G [00:03<01:26, 36.1MB/s][2023-12-13 09:46:04,843] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:04,843] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:   4%|▍         | 126M/3.24G [00:04<01:25, 36.4MB/s]\rpytorch_model.bin:   4%|▍         | 136M/3.24G [00:04<01:24, 36.7MB/s]\rpytorch_model.bin:   5%|▍         | 147M/3.24G [00:04<01:23, 36.8MB/s]\rpytorch_model.bin:   5%|▍         | 157M/3.24G [00:04<01:28, 34.8MB/s]\rpytorch_model.bin:   5%|▌         | 168M/3.24G [00:05<01:26, 35.7MB/s]\rpytorch_model.bin:   6%|▌         | 178M/3.24G [00:05<01:22, 37.2MB/s]\rpytorch_model.bin:   6%|▌         | 189M/3.24G [00:05<01:29, 34.2MB/s]\rpytorch_model.bin:   6%|▌         | 199M/3.24G [00:06<01:26, 35.2MB/s]\rpytorch_model.bin:   6%|▋         | 210M/3.24G [00:06<01:20, 37.5MB/s]\rpytorch_model.bin:   7%|▋         | 220M/3.24G [00:06<01:17, 38.7MB/s]\rpytorch_model.bin:   7%|▋         | 231M/3.24G [00:06<01:17, 39.0MB/s]\rpytorch_model.bin:   7%|▋         | 241M/3.24G [00:07<01:23, 36.0MB/s]\rpytorch_model.bin:   8%|▊         | 252M/3.24G [00:07<01:21, 36.5MB/s]\rpytorch_model.bin:   8%|▊         | 262M/3.24G [00:07<01:15, 39.4MB/s]\rpytorch_model.bin:   8%|▊         | 273M/3.24G [00:08<01:29, 33.3MB/s]\rpytorch_model.bin:   9%|▊         | 283M/3.24G [00:08<01:26, 34.3MB/s]\rpytorch_model.bin:   9%|▉         | 294M/3.24G [00:08<01:29, 32.7MB/s][2023-12-13 09:46:09,847] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:09,847] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:   9%|▉         | 304M/3.24G [00:09<01:26, 34.0MB/s]\rpytorch_model.bin:  10%|▉         | 315M/3.24G [00:09<01:30, 32.5MB/s]\rpytorch_model.bin:  10%|█         | 325M/3.24G [00:09<01:26, 33.7MB/s]\rpytorch_model.bin:  10%|█         | 336M/3.24G [00:10<01:23, 34.8MB/s]\rpytorch_model.bin:  11%|█         | 346M/3.24G [00:10<01:26, 33.3MB/s]\rpytorch_model.bin:  11%|█         | 357M/3.24G [00:10<01:24, 33.9MB/s]\rpytorch_model.bin:  11%|█▏        | 367M/3.24G [00:11<01:45, 27.1MB/s]\rpytorch_model.bin:  12%|█▏        | 377M/3.24G [00:11<01:41, 28.3MB/s]\rpytorch_model.bin:  12%|█▏        | 388M/3.24G [00:11<01:33, 30.6MB/s]\rpytorch_model.bin:  12%|█▏        | 398M/3.24G [00:12<01:27, 32.4MB/s]\rpytorch_model.bin:  13%|█▎        | 409M/3.24G [00:12<01:26, 32.6MB/s]\rpytorch_model.bin:  13%|█▎        | 419M/3.24G [00:12<01:26, 32.6MB/s]\rpytorch_model.bin:  13%|█▎        | 430M/3.24G [00:13<01:24, 33.1MB/s]\rpytorch_model.bin:  14%|█▎        | 440M/3.24G [00:13<01:23, 33.3MB/s]\rpytorch_model.bin:  14%|█▍        | 451M/3.24G [00:13<01:20, 34.6MB/s]\rpytorch_model.bin:  14%|█▍        | 461M/3.24G [00:13<01:18, 35.5MB/s][2023-12-13 09:46:14,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:14,852] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  15%|█▍        | 472M/3.24G [00:14<01:17, 35.6MB/s]\rpytorch_model.bin:  15%|█▍        | 482M/3.24G [00:14<01:20, 34.0MB/s]\rpytorch_model.bin:  15%|█▌        | 493M/3.24G [00:14<01:18, 35.1MB/s]\rpytorch_model.bin:  16%|█▌        | 503M/3.24G [00:15<01:16, 35.8MB/s]\rpytorch_model.bin:  16%|█▌        | 514M/3.24G [00:15<01:14, 36.4MB/s]\rpytorch_model.bin:  16%|█▌        | 524M/3.24G [00:15<01:20, 33.8MB/s]\rpytorch_model.bin:  17%|█▋        | 535M/3.24G [00:16<01:17, 34.9MB/s]\rpytorch_model.bin:  17%|█▋        | 545M/3.24G [00:16<01:15, 35.7MB/s]\rpytorch_model.bin:  17%|█▋        | 556M/3.24G [00:16<01:13, 36.3MB/s]\rpytorch_model.bin:  17%|█▋        | 566M/3.24G [00:16<01:14, 35.6MB/s]\rpytorch_model.bin:  18%|█▊        | 577M/3.24G [00:17<01:19, 33.5MB/s]\rpytorch_model.bin:  18%|█▊        | 587M/3.24G [00:17<01:16, 34.7MB/s]\rpytorch_model.bin:  18%|█▊        | 598M/3.24G [00:17<01:14, 35.6MB/s]\rpytorch_model.bin:  19%|█▉        | 608M/3.24G [00:18<01:13, 35.8MB/s]\rpytorch_model.bin:  19%|█▉        | 619M/3.24G [00:18<01:18, 33.3MB/s]\rpytorch_model.bin:  19%|█▉        | 629M/3.24G [00:18<01:21, 31.8MB/s][2023-12-13 09:46:19,856] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:19,856] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  20%|█▉        | 640M/3.24G [00:19<01:24, 30.8MB/s]\rpytorch_model.bin:  20%|██        | 650M/3.24G [00:19<01:25, 30.1MB/s]\rpytorch_model.bin:  20%|██        | 661M/3.24G [00:19<01:20, 31.9MB/s]\rpytorch_model.bin:  21%|██        | 671M/3.24G [00:20<01:22, 31.0MB/s]\rpytorch_model.bin:  21%|██        | 682M/3.24G [00:20<01:23, 30.4MB/s]\rpytorch_model.bin:  21%|██▏       | 692M/3.24G [00:20<01:19, 32.1MB/s]\rpytorch_model.bin:  22%|██▏       | 703M/3.24G [00:21<01:20, 31.3MB/s]\rpytorch_model.bin:  22%|██▏       | 713M/3.24G [00:21<01:17, 32.7MB/s]\rpytorch_model.bin:  22%|██▏       | 724M/3.24G [00:21<01:18, 31.9MB/s]\rpytorch_model.bin:  23%|██▎       | 734M/3.24G [00:22<01:15, 33.0MB/s]\rpytorch_model.bin:  23%|██▎       | 744M/3.24G [00:22<01:12, 34.2MB/s]\rpytorch_model.bin:  23%|██▎       | 755M/3.24G [00:22<01:14, 33.2MB/s]\rpytorch_model.bin:  24%|██▎       | 765M/3.24G [00:23<01:13, 33.7MB/s]\rpytorch_model.bin:  24%|██▍       | 776M/3.24G [00:23<01:10, 34.8MB/s]\rpytorch_model.bin:  24%|██▍       | 786M/3.24G [00:23<01:14, 32.7MB/s]\rpytorch_model.bin:  25%|██▍       | 797M/3.24G [00:23<01:11, 34.1MB/s][2023-12-13 09:46:24,861] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:24,861] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  25%|██▍       | 807M/3.24G [00:24<01:09, 35.1MB/s]\rpytorch_model.bin:  25%|██▌       | 818M/3.24G [00:24<01:07, 35.8MB/s]\rpytorch_model.bin:  26%|██▌       | 828M/3.24G [00:24<01:06, 36.3MB/s]\rpytorch_model.bin:  26%|██▌       | 839M/3.24G [00:25<01:11, 33.6MB/s]\rpytorch_model.bin:  26%|██▌       | 849M/3.24G [00:25<01:13, 32.5MB/s]\rpytorch_model.bin:  27%|██▋       | 860M/3.24G [00:25<01:10, 33.9MB/s]\rpytorch_model.bin:  27%|██▋       | 870M/3.24G [00:26<01:17, 30.4MB/s]\rpytorch_model.bin:  27%|██▋       | 881M/3.24G [00:26<01:13, 31.9MB/s]\rpytorch_model.bin:  28%|██▊       | 891M/3.24G [00:26<01:15, 31.2MB/s]\rpytorch_model.bin:  28%|██▊       | 902M/3.24G [00:27<01:17, 30.1MB/s]\rpytorch_model.bin:  28%|██▊       | 912M/3.24G [00:27<01:12, 32.1MB/s]\rpytorch_model.bin:  29%|██▊       | 923M/3.24G [00:28<01:32, 25.0MB/s]\rpytorch_model.bin:  29%|██▉       | 933M/3.24G [00:28<01:22, 27.9MB/s]\rpytorch_model.bin:  29%|██▉       | 944M/3.24G [00:28<01:20, 28.6MB/s][2023-12-13 09:46:29,866] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:29,866] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  29%|██▉       | 954M/3.24G [00:29<01:14, 30.5MB/s]\rpytorch_model.bin:  30%|██▉       | 965M/3.24G [00:29<01:10, 32.3MB/s]\rpytorch_model.bin:  30%|███       | 975M/3.24G [00:29<01:10, 32.3MB/s]\rpytorch_model.bin:  30%|███       | 986M/3.24G [00:29<01:07, 33.6MB/s]\rpytorch_model.bin:  31%|███       | 996M/3.24G [00:30<01:14, 30.2MB/s]\rpytorch_model.bin:  31%|███       | 1.01G/3.24G [00:30<01:09, 32.1MB/s]\rpytorch_model.bin:  31%|███▏      | 1.02G/3.24G [00:30<01:06, 33.5MB/s]\rpytorch_model.bin:  32%|███▏      | 1.03G/3.24G [00:31<01:09, 31.8MB/s]\rpytorch_model.bin:  32%|███▏      | 1.04G/3.24G [00:31<01:11, 30.8MB/s]\rpytorch_model.bin:  32%|███▏      | 1.05G/3.24G [00:32<01:12, 30.1MB/s]\rpytorch_model.bin:  33%|███▎      | 1.06G/3.24G [00:32<01:13, 29.7MB/s]\rpytorch_model.bin:  33%|███▎      | 1.07G/3.24G [00:32<01:13, 29.5MB/s]\rpytorch_model.bin:  33%|███▎      | 1.08G/3.24G [00:33<01:13, 29.4MB/s]\rpytorch_model.bin:  34%|███▎      | 1.09G/3.24G [00:33<01:08, 31.3MB/s]\rpytorch_model.bin:  34%|███▍      | 1.10G/3.24G [00:33<01:10, 30.1MB/s][2023-12-13 09:46:34,870] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:34,870] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  34%|███▍      | 1.11G/3.24G [00:34<01:09, 30.5MB/s]\rpytorch_model.bin:  35%|███▍      | 1.12G/3.24G [00:34<01:07, 31.5MB/s]\rpytorch_model.bin:  35%|███▍      | 1.13G/3.24G [00:34<01:10, 29.9MB/s]\rpytorch_model.bin:  35%|███▌      | 1.14G/3.24G [00:35<01:05, 31.8MB/s]\rpytorch_model.bin:  36%|███▌      | 1.15G/3.24G [00:35<01:02, 33.3MB/s]\rpytorch_model.bin:  36%|███▌      | 1.16G/3.24G [00:35<01:04, 32.3MB/s]\rpytorch_model.bin:  36%|███▋      | 1.17G/3.24G [00:36<01:01, 33.3MB/s]\rpytorch_model.bin:  37%|███▋      | 1.18G/3.24G [00:36<01:04, 32.0MB/s]\rpytorch_model.bin:  37%|███▋      | 1.20G/3.24G [00:36<01:00, 33.5MB/s]\rpytorch_model.bin:  37%|███▋      | 1.21G/3.24G [00:37<01:11, 28.4MB/s]\rpytorch_model.bin:  38%|███▊      | 1.22G/3.24G [00:37<01:05, 30.7MB/s]\rpytorch_model.bin:  38%|███▊      | 1.23G/3.24G [00:37<01:01, 32.5MB/s]\rpytorch_model.bin:  38%|███▊      | 1.24G/3.24G [00:38<00:59, 33.8MB/s]\rpytorch_model.bin:  39%|███▊      | 1.25G/3.24G [00:38<01:00, 32.7MB/s]\rpytorch_model.bin:  39%|███▉      | 1.26G/3.24G [00:38<01:06, 29.7MB/s][2023-12-13 09:46:39,875] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:39,875] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  39%|███▉      | 1.27G/3.24G [00:39<01:02, 31.7MB/s]\rpytorch_model.bin:  40%|███▉      | 1.28G/3.24G [00:39<00:58, 33.3MB/s]\rpytorch_model.bin:  40%|███▉      | 1.29G/3.24G [00:39<00:59, 32.5MB/s]\rpytorch_model.bin:  40%|████      | 1.30G/3.24G [00:39<00:57, 33.9MB/s]\rpytorch_model.bin:  41%|████      | 1.31G/3.24G [00:40<01:24, 22.7MB/s]\rpytorch_model.bin:  41%|████      | 1.32G/3.24G [00:41<01:18, 24.5MB/s]\rpytorch_model.bin:  41%|████      | 1.33G/3.24G [00:41<01:09, 27.4MB/s]\rpytorch_model.bin:  41%|████▏     | 1.34G/3.24G [00:41<01:03, 29.9MB/s]\rpytorch_model.bin:  42%|████▏     | 1.35G/3.24G [00:41<01:01, 30.5MB/s]\rpytorch_model.bin:  42%|████▏     | 1.36G/3.24G [00:42<01:04, 29.2MB/s]\rpytorch_model.bin:  42%|████▏     | 1.37G/3.24G [00:42<01:01, 30.4MB/s]\rpytorch_model.bin:  43%|████▎     | 1.38G/3.24G [00:42<00:57, 32.2MB/s]\rpytorch_model.bin:  43%|████▎     | 1.39G/3.24G [00:43<00:59, 30.9MB/s]\rpytorch_model.bin:  43%|████▎     | 1.41G/3.24G [00:43<00:55, 32.7MB/s]\rpytorch_model.bin:  44%|████▎     | 1.42G/3.24G [00:43<00:53, 34.1MB/s][2023-12-13 09:46:44,877] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:44,877] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  44%|████▍     | 1.43G/3.24G [00:44<00:51, 35.0MB/s]\rpytorch_model.bin:  44%|████▍     | 1.44G/3.24G [00:44<00:50, 35.8MB/s]\rpytorch_model.bin:  45%|████▍     | 1.45G/3.24G [00:44<00:49, 35.9MB/s]\rpytorch_model.bin:  45%|████▌     | 1.46G/3.24G [00:45<00:48, 36.4MB/s]\rpytorch_model.bin:  45%|████▌     | 1.47G/3.24G [00:45<00:59, 29.9MB/s]\rpytorch_model.bin:  46%|████▌     | 1.48G/3.24G [00:46<01:04, 27.2MB/s]\rpytorch_model.bin:  46%|████▌     | 1.49G/3.24G [00:46<01:07, 25.9MB/s]\rpytorch_model.bin:  46%|████▋     | 1.50G/3.24G [00:46<01:13, 23.5MB/s]\rpytorch_model.bin:  47%|████▋     | 1.51G/3.24G [00:47<01:14, 23.3MB/s]\rpytorch_model.bin:  47%|████▋     | 1.52G/3.24G [00:47<01:18, 22.0MB/s]\rpytorch_model.bin:  47%|████▋     | 1.53G/3.24G [00:48<01:16, 22.2MB/s]\rpytorch_model.bin:  48%|████▊     | 1.54G/3.24G [00:48<01:15, 22.4MB/s][2023-12-13 09:46:49,882] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:49,882] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  48%|████▊     | 1.55G/3.24G [00:49<01:14, 22.6MB/s]\rpytorch_model.bin:  48%|████▊     | 1.56G/3.24G [00:49<01:13, 22.7MB/s]\rpytorch_model.bin:  49%|████▊     | 1.57G/3.24G [00:50<01:12, 22.8MB/s]\rpytorch_model.bin:  49%|████▉     | 1.58G/3.24G [00:50<01:12, 22.9MB/s]\rpytorch_model.bin:  49%|████▉     | 1.59G/3.24G [00:51<01:11, 22.9MB/s]\rpytorch_model.bin:  50%|████▉     | 1.60G/3.24G [00:51<01:11, 23.0MB/s]\rpytorch_model.bin:  50%|████▉     | 1.61G/3.24G [00:52<01:10, 23.0MB/s]\rpytorch_model.bin:  50%|█████     | 1.63G/3.24G [00:52<01:09, 23.0MB/s]\rpytorch_model.bin:  51%|█████     | 1.64G/3.24G [00:53<01:09, 23.0MB/s]\rpytorch_model.bin:  51%|█████     | 1.65G/3.24G [00:53<01:09, 23.0MB/s]\rpytorch_model.bin:  51%|█████     | 1.66G/3.24G [00:53<01:08, 23.0MB/s][2023-12-13 09:46:54,886] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:54,886] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  52%|█████▏    | 1.67G/3.24G [00:54<01:07, 23.1MB/s]\rpytorch_model.bin:  52%|█████▏    | 1.68G/3.24G [00:54<01:07, 23.1MB/s]\rpytorch_model.bin:  52%|█████▏    | 1.69G/3.24G [00:55<01:07, 23.1MB/s]\rpytorch_model.bin:  52%|█████▏    | 1.70G/3.24G [00:55<01:03, 24.4MB/s]\rpytorch_model.bin:  53%|█████▎    | 1.71G/3.24G [00:56<01:11, 21.3MB/s]\rpytorch_model.bin:  53%|█████▎    | 1.72G/3.24G [00:56<01:13, 20.6MB/s]\rpytorch_model.bin:  53%|█████▎    | 1.73G/3.24G [00:57<01:14, 20.2MB/s]\rpytorch_model.bin:  54%|█████▍    | 1.74G/3.24G [00:57<01:15, 19.9MB/s]\rpytorch_model.bin:  54%|█████▍    | 1.75G/3.24G [00:58<01:15, 19.7MB/s]\rpytorch_model.bin:  54%|█████▍    | 1.76G/3.24G [00:58<01:11, 20.5MB/s][2023-12-13 09:46:59,891] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:46:59,891] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  55%|█████▍    | 1.77G/3.24G [00:59<01:12, 20.2MB/s]\rpytorch_model.bin:  55%|█████▌    | 1.78G/3.24G [00:59<01:09, 20.9MB/s]\rpytorch_model.bin:  55%|█████▌    | 1.79G/3.24G [01:00<01:07, 21.5MB/s]\rpytorch_model.bin:  56%|█████▌    | 1.80G/3.24G [01:00<01:08, 20.9MB/s]\rpytorch_model.bin:  56%|█████▌    | 1.81G/3.24G [01:01<01:06, 21.4MB/s]\rpytorch_model.bin:  56%|█████▋    | 1.82G/3.24G [01:01<01:04, 21.9MB/s]\rpytorch_model.bin:  57%|█████▋    | 1.84G/3.24G [01:02<01:03, 22.2MB/s]\rpytorch_model.bin:  57%|█████▋    | 1.85G/3.24G [01:02<01:01, 22.5MB/s]\rpytorch_model.bin:  57%|█████▋    | 1.86G/3.24G [01:03<01:00, 22.6MB/s]\rpytorch_model.bin:  58%|█████▊    | 1.87G/3.24G [01:03<01:00, 22.8MB/s][2023-12-13 09:47:04,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:04,896] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  58%|█████▊    | 1.88G/3.24G [01:04<00:59, 22.9MB/s]\rpytorch_model.bin:  58%|█████▊    | 1.89G/3.24G [01:04<00:55, 24.1MB/s]\rpytorch_model.bin:  59%|█████▊    | 1.90G/3.24G [01:04<00:56, 23.8MB/s]\rpytorch_model.bin:  59%|█████▉    | 1.91G/3.24G [01:05<00:56, 23.6MB/s]\rpytorch_model.bin:  59%|█████▉    | 1.92G/3.24G [01:05<00:56, 23.4MB/s]\rpytorch_model.bin:  60%|█████▉    | 1.93G/3.24G [01:06<00:55, 23.4MB/s]\rpytorch_model.bin:  60%|█████▉    | 1.94G/3.24G [01:06<00:55, 23.3MB/s]\rpytorch_model.bin:  60%|██████    | 1.95G/3.24G [01:07<00:55, 23.2MB/s]\rpytorch_model.bin:  61%|██████    | 1.96G/3.24G [01:07<00:54, 23.2MB/s]\rpytorch_model.bin:  61%|██████    | 1.97G/3.24G [01:08<00:54, 23.2MB/s]\rpytorch_model.bin:  61%|██████    | 1.98G/3.24G [01:08<00:54, 23.1MB/s]\rpytorch_model.bin:  62%|██████▏   | 1.99G/3.24G [01:08<00:51, 24.3MB/s][2023-12-13 09:47:09,900] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:09,900] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  62%|██████▏   | 2.00G/3.24G [01:09<00:51, 24.0MB/s]\rpytorch_model.bin:  62%|██████▏   | 2.01G/3.24G [01:09<00:51, 23.7MB/s]\rpytorch_model.bin:  63%|██████▎   | 2.02G/3.24G [01:10<00:51, 23.5MB/s]\rpytorch_model.bin:  63%|██████▎   | 2.03G/3.24G [01:10<00:51, 23.5MB/s]\rpytorch_model.bin:  63%|██████▎   | 2.04G/3.24G [01:11<00:50, 23.4MB/s]\rpytorch_model.bin:  64%|██████▎   | 2.06G/3.24G [01:11<00:48, 24.5MB/s]\rpytorch_model.bin:  64%|██████▍   | 2.07G/3.24G [01:12<00:48, 24.2MB/s]\rpytorch_model.bin:  64%|██████▍   | 2.08G/3.24G [01:12<00:48, 24.0MB/s]\rpytorch_model.bin:  64%|██████▍   | 2.09G/3.24G [01:12<00:46, 25.0MB/s]\rpytorch_model.bin:  65%|██████▍   | 2.10G/3.24G [01:13<00:48, 23.6MB/s]\rpytorch_model.bin:  65%|██████▌   | 2.11G/3.24G [01:13<00:49, 23.0MB/s][2023-12-13 09:47:14,905] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:14,905] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  65%|██████▌   | 2.12G/3.24G [01:14<00:45, 24.4MB/s]\rpytorch_model.bin:  66%|██████▌   | 2.13G/3.24G [01:14<00:46, 24.1MB/s]\rpytorch_model.bin:  66%|██████▌   | 2.14G/3.24G [01:15<00:43, 25.3MB/s]\rpytorch_model.bin:  66%|██████▋   | 2.15G/3.24G [01:15<00:49, 21.9MB/s]\rpytorch_model.bin:  67%|██████▋   | 2.16G/3.24G [01:16<00:47, 22.7MB/s]\rpytorch_model.bin:  67%|██████▋   | 2.17G/3.24G [01:16<00:44, 24.2MB/s]\rpytorch_model.bin:  67%|██████▋   | 2.18G/3.24G [01:16<00:41, 25.4MB/s]\rpytorch_model.bin:  68%|██████▊   | 2.19G/3.24G [01:17<00:37, 27.5MB/s]\rpytorch_model.bin:  68%|██████▊   | 2.20G/3.24G [01:17<00:37, 27.9MB/s]\rpytorch_model.bin:  68%|██████▊   | 2.21G/3.24G [01:17<00:36, 28.3MB/s]\rpytorch_model.bin:  69%|██████▊   | 2.22G/3.24G [01:18<00:33, 30.5MB/s]\rpytorch_model.bin:  69%|██████▉   | 2.23G/3.24G [01:18<00:33, 30.0MB/s]\rpytorch_model.bin:  69%|██████▉   | 2.24G/3.24G [01:18<00:32, 30.1MB/s][2023-12-13 09:47:19,910] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:19,910] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  70%|██████▉   | 2.25G/3.24G [01:19<00:31, 31.6MB/s]\rpytorch_model.bin:  70%|██████▉   | 2.26G/3.24G [01:19<00:29, 33.2MB/s]\rpytorch_model.bin:  70%|███████   | 2.28G/3.24G [01:19<00:27, 34.4MB/s]\rpytorch_model.bin:  71%|███████   | 2.29G/3.24G [01:20<00:30, 31.1MB/s]\rpytorch_model.bin:  71%|███████   | 2.30G/3.24G [01:20<00:28, 32.8MB/s]\rpytorch_model.bin:  71%|███████▏  | 2.31G/3.24G [01:20<00:27, 34.1MB/s]\rpytorch_model.bin:  72%|███████▏  | 2.32G/3.24G [01:20<00:26, 35.2MB/s]\rpytorch_model.bin:  72%|███████▏  | 2.33G/3.24G [01:21<00:25, 36.0MB/s]\rpytorch_model.bin:  72%|███████▏  | 2.34G/3.24G [01:21<00:37, 23.9MB/s]\rpytorch_model.bin:  73%|███████▎  | 2.35G/3.24G [01:22<00:32, 27.0MB/s]\rpytorch_model.bin:  73%|███████▎  | 2.36G/3.24G [01:22<00:31, 27.5MB/s]\rpytorch_model.bin:  73%|███████▎  | 2.37G/3.24G [01:22<00:29, 29.3MB/s]\rpytorch_model.bin:  74%|███████▎  | 2.38G/3.24G [01:23<00:28, 29.8MB/s]\rpytorch_model.bin:  74%|███████▍  | 2.39G/3.24G [01:23<00:32, 26.4MB/s]\rpytorch_model.bin:  74%|███████▍  | 2.40G/3.24G [01:24<00:28, 28.8MB/s][2023-12-13 09:47:24,914] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:24,914] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  75%|███████▍  | 2.41G/3.24G [01:24<00:28, 29.1MB/s]\rpytorch_model.bin:  75%|███████▍  | 2.42G/3.24G [01:24<00:26, 31.0MB/s]\rpytorch_model.bin:  75%|███████▌  | 2.43G/3.24G [01:25<00:28, 27.8MB/s]\rpytorch_model.bin:  76%|███████▌  | 2.44G/3.24G [01:25<00:25, 31.5MB/s]\rpytorch_model.bin:  76%|███████▌  | 2.45G/3.24G [01:25<00:23, 33.2MB/s]\rpytorch_model.bin:  76%|███████▌  | 2.46G/3.24G [01:25<00:22, 33.8MB/s]\rpytorch_model.bin:  76%|███████▋  | 2.47G/3.24G [01:26<00:22, 34.6MB/s]\rpytorch_model.bin:  77%|███████▋  | 2.49G/3.24G [01:26<00:22, 33.9MB/s]\rpytorch_model.bin:  77%|███████▋  | 2.50G/3.24G [01:27<00:37, 19.9MB/s]\rpytorch_model.bin:  77%|███████▋  | 2.51G/3.24G [01:27<00:31, 23.2MB/s]\rpytorch_model.bin:  78%|███████▊  | 2.52G/3.24G [01:28<00:27, 26.1MB/s]\rpytorch_model.bin:  78%|███████▊  | 2.53G/3.24G [01:28<00:24, 28.8MB/s]\rpytorch_model.bin:  78%|███████▊  | 2.54G/3.24G [01:28<00:22, 31.0MB/s]\rpytorch_model.bin:  79%|███████▊  | 2.55G/3.24G [01:29<00:21, 32.7MB/s][2023-12-13 09:47:29,919] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:29,919] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  79%|███████▉  | 2.56G/3.24G [01:29<00:19, 33.9MB/s]\rpytorch_model.bin:  79%|███████▉  | 2.57G/3.24G [01:29<00:19, 34.8MB/s]\rpytorch_model.bin:  80%|███████▉  | 2.58G/3.24G [01:30<00:24, 27.0MB/s]\rpytorch_model.bin:  80%|████████  | 2.59G/3.24G [01:30<00:22, 29.0MB/s]\rpytorch_model.bin:  80%|████████  | 2.60G/3.24G [01:30<00:23, 26.9MB/s]\rpytorch_model.bin:  81%|████████  | 2.61G/3.24G [01:31<00:19, 31.5MB/s]\rpytorch_model.bin:  81%|████████  | 2.62G/3.24G [01:31<00:20, 29.4MB/s]\rpytorch_model.bin:  81%|████████▏ | 2.63G/3.24G [01:31<00:20, 29.2MB/s]\rpytorch_model.bin:  82%|████████▏ | 2.64G/3.24G [01:32<00:20, 29.1MB/s]\rpytorch_model.bin:  82%|████████▏ | 2.65G/3.24G [01:32<00:19, 29.2MB/s]\rpytorch_model.bin:  82%|████████▏ | 2.66G/3.24G [01:32<00:18, 31.0MB/s]\rpytorch_model.bin:  83%|████████▎ | 2.67G/3.24G [01:33<00:18, 30.4MB/s]\rpytorch_model.bin:  83%|████████▎ | 2.68G/3.24G [01:33<00:18, 30.0MB/s]\rpytorch_model.bin:  83%|████████▎ | 2.69G/3.24G [01:33<00:16, 31.9MB/s][2023-12-13 09:47:34,923] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:34,923] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  84%|████████▎ | 2.71G/3.24G [01:34<00:17, 31.0MB/s]\rpytorch_model.bin:  84%|████████▍ | 2.72G/3.24G [01:34<00:16, 30.6MB/s]\rpytorch_model.bin:  84%|████████▍ | 2.73G/3.24G [01:34<00:15, 32.1MB/s]\rpytorch_model.bin:  85%|████████▍ | 2.74G/3.24G [01:35<00:15, 31.4MB/s]\rpytorch_model.bin:  85%|████████▍ | 2.75G/3.24G [01:35<00:14, 32.8MB/s]\rpytorch_model.bin:  85%|████████▌ | 2.76G/3.24G [01:35<00:15, 31.9MB/s]\rpytorch_model.bin:  86%|████████▌ | 2.77G/3.24G [01:36<00:14, 33.1MB/s]\rpytorch_model.bin:  86%|████████▌ | 2.78G/3.24G [01:36<00:15, 29.6MB/s]\rpytorch_model.bin:  86%|████████▌ | 2.79G/3.24G [01:36<00:15, 29.5MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.80G/3.24G [01:37<00:14, 30.8MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.81G/3.24G [01:37<00:13, 31.1MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.82G/3.24G [01:37<00:13, 31.0MB/s]\rpytorch_model.bin:  87%|████████▋ | 2.83G/3.24G [01:38<00:14, 28.8MB/s]\rpytorch_model.bin:  88%|████████▊ | 2.84G/3.24G [01:38<00:13, 29.0MB/s][2023-12-13 09:47:39,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:39,928] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  88%|████████▊ | 2.85G/3.24G [01:39<00:14, 26.6MB/s]\rpytorch_model.bin:  88%|████████▊ | 2.86G/3.24G [01:39<00:15, 24.0MB/s]\rpytorch_model.bin:  89%|████████▉ | 2.87G/3.24G [01:40<00:17, 21.1MB/s]\rpytorch_model.bin:  89%|████████▉ | 2.88G/3.24G [01:40<00:17, 20.5MB/s]\rpytorch_model.bin:  89%|████████▉ | 2.89G/3.24G [01:41<00:17, 20.0MB/s]\rpytorch_model.bin:  90%|████████▉ | 2.90G/3.24G [01:42<00:18, 17.9MB/s]\rpytorch_model.bin:  90%|█████████ | 2.92G/3.24G [01:42<00:19, 16.7MB/s]\rpytorch_model.bin:  90%|█████████ | 2.93G/3.24G [01:43<00:19, 16.0MB/s][2023-12-13 09:47:44,932] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:44,932] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  91%|█████████ | 2.94G/3.24G [01:44<00:18, 16.1MB/s]\rpytorch_model.bin:  91%|█████████ | 2.95G/3.24G [01:45<00:18, 15.6MB/s]\rpytorch_model.bin:  91%|█████████▏| 2.96G/3.24G [01:45<00:17, 15.8MB/s]\rpytorch_model.bin:  92%|█████████▏| 2.97G/3.24G [01:46<00:16, 16.0MB/s]\rpytorch_model.bin:  92%|█████████▏| 2.98G/3.24G [01:47<00:17, 14.9MB/s]\rpytorch_model.bin:  92%|█████████▏| 2.99G/3.24G [01:47<00:17, 14.2MB/s]\rpytorch_model.bin:  93%|█████████▎| 3.00G/3.24G [01:48<00:17, 13.7MB/s][2023-12-13 09:47:49,937] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:49,937] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  93%|█████████▎| 3.01G/3.24G [01:49<00:16, 13.5MB/s]\rpytorch_model.bin:  93%|█████████▎| 3.02G/3.24G [01:50<00:16, 13.3MB/s]\rpytorch_model.bin:  94%|█████████▎| 3.03G/3.24G [01:51<00:15, 13.6MB/s]\rpytorch_model.bin:  94%|█████████▍| 3.04G/3.24G [01:51<00:14, 13.4MB/s]\rpytorch_model.bin:  94%|█████████▍| 3.05G/3.24G [01:52<00:13, 13.6MB/s]\rpytorch_model.bin:  95%|█████████▍| 3.06G/3.24G [01:53<00:12, 13.7MB/s][2023-12-13 09:47:54,941] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:54,941] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  95%|█████████▍| 3.07G/3.24G [01:54<00:12, 13.6MB/s]\rpytorch_model.bin:  95%|█████████▌| 3.08G/3.24G [01:54<00:11, 13.8MB/s]\rpytorch_model.bin:  96%|█████████▌| 3.09G/3.24G [01:55<00:10, 13.7MB/s]\rpytorch_model.bin:  96%|█████████▌| 3.10G/3.24G [01:56<00:09, 13.7MB/s]\rpytorch_model.bin:  96%|█████████▌| 3.11G/3.24G [01:57<00:08, 13.9MB/s]\rpytorch_model.bin:  97%|█████████▋| 3.12G/3.24G [01:57<00:08, 13.8MB/s]\rpytorch_model.bin:  97%|█████████▋| 3.14G/3.24G [01:58<00:07, 14.0MB/s][2023-12-13 09:47:59,945] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:47:59,945] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin:  97%|█████████▋| 3.15G/3.24G [01:59<00:06, 14.1MB/s]\rpytorch_model.bin:  98%|█████████▊| 3.16G/3.24G [02:00<00:05, 14.5MB/s]\rpytorch_model.bin:  98%|█████████▊| 3.17G/3.24G [02:00<00:04, 14.5MB/s]\rpytorch_model.bin:  98%|█████████▊| 3.18G/3.24G [02:01<00:03, 15.0MB/s]\rpytorch_model.bin:  99%|█████████▊| 3.19G/3.24G [02:02<00:03, 15.4MB/s]\rpytorch_model.bin:  99%|█████████▉| 3.20G/3.24G [02:02<00:02, 16.3MB/s]\rpytorch_model.bin:  99%|█████████▉| 3.21G/3.24G [02:03<00:01, 16.5MB/s]\rpytorch_model.bin:  99%|█████████▉| 3.22G/3.24G [02:03<00:00, 17.2MB/s][2023-12-13 09:48:04,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:04,950] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rpytorch_model.bin: 100%|█████████▉| 3.23G/3.24G [02:04<00:00, 18.5MB/s]\rpytorch_model.bin: 100%|██████████| 3.24G/3.24G [02:04<00:00, 18.7MB/s]\rpytorch_model.bin: 100%|██████████| 3.24G/3.24G [02:04<00:00, 26.0MB/s]\n",
            "[2023-12-13 09:48:09,954] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:09,954] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:14,959] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:14,959] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "\rvocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]\rvocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.43MB/s]\rvocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.42MB/s]\n",
            "\rmerges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\rmerges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.89MB/s]\rmerges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.88MB/s]\n",
            "[2023-12-13 09:48:19,964] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:19,964] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "> --------- MII Settings: ds_optimize=True, replace_with_kernel_inject=True, enable_cuda_graph=False \n",
            "[2023-12-13 09:48:23,355] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown\n",
            "[2023-12-13 09:48:23,356] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2023-12-13 09:48:23,356] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...\n",
            "Building extension module transformer_inference...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module transformer_inference...\n",
            "Time to load transformer_inference op: 0.09351205825805664 seconds\n",
            "[2023-12-13 09:48:23,831] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 2048, 'intermediate_size': 8192, 'heads': 32, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.ReLU: 2>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000}\n",
            "[2023-12-13 09:48:24,965] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:24,965] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n",
            "[2023-12-13 09:48:29,970] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w1nkfndkjKw",
        "outputId": "e4459a1f-70ef-4adc-f8c8-b8d4edd601ae"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 10919 root   40u  IPv4 262091      0t0  TCP *:50051 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cliet chat\n",
        "!cd DeepSpeed-MII/mii/legacy/examples/local/chat && python chat-client-example.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8snvrDj9are-",
        "outputId": "52efcf48-e2d0-4c08-9df0-8e6318da5c03"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:59:01,271] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:59:03.188707: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:59:03.188760: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:59:03.188785: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:59:04.323178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:59:05,180] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:59:05,180] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "# Start a conversation session. Type 'q' to exit.\n",
            "You: hello , are u a bot?\n",
            "Bot: Yes, I am a bot.  Yes, I am programmed to provide information.\n",
            "You: Can you tell me about deep learning?\n",
            "Bot: Yes, it is a form of deep learning.  It is a technique for developing artificial intelligence that learns from data.  It allows computers to develop their own intelligence.\n",
            "You: I want to try it.\n",
            "Bot: Yes, it is a good idea to try deep learning.  It can help you develop your own intelligence.\n",
            "You: Is it hard to learn?\n",
            "Bot: Yes, it is hard to learn.  It requires a lot of practice and practice is necessary to develop an effective deep learning system.\n",
            "You: Where can I start?\n",
            "Bot: Yes, it is a good idea to start with deep learning.  It can help you develop your own intelligence.  You should start with simple tasks and gradually increase the complexity of the tasks as you become more proficient with the system.\n",
            "You: thank you\n",
            "Bot: You are welcome\n",
            "You: bye\n",
            "Bot: \n",
            "You: 88\n",
            "Bot: \n",
            "You: 拜\n",
            "Bot: \n",
            "You: q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.terminate(\"chat_example_deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbNiSkexkJcR",
        "outputId": "4a01a0fb-51f2-435d-ac02-feb76ce535d1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 10:01:33,497] [INFO] [terminate.py:12:terminate] Terminating server for chat_example_deployment\n",
            "[2023-12-13 10:01:33,497] [INFO] [terminate.py:12:terminate] Terminating server for chat_example_deployment\n",
            "[2023-12-13 10:01:33,501] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 10:01:33,502] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 10:01:33,540] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 10:01:33,541] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "id": "4XkEGm4ooAOR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## more\n",
        "更多示例：\n",
        "https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy/examples/local"
      ],
      "metadata": {
        "id": "0-zJ_kwkYPp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh DeepSpeed-MII/mii/legacy/examples/local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T40MpO32gcKw",
        "outputId": "012bc6f4-0702-4c7a-fcce-88ff3bc8e4ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 64K\n",
            "drwxr-xr-x 2 root root 4.0K Dec 13 09:11 chat\n",
            "-rw-r--r-- 1 root root  302 Dec 13 09:11 conversational-example.py\n",
            "-rw-r--r-- 1 root root  757 Dec 13 09:11 conversational-query-example.py\n",
            "-rw-r--r-- 1 root root  677 Dec 13 09:11 fill-mask-example.py\n",
            "-rw-r--r-- 1 root root  351 Dec 13 09:11 question-answering-example.py\n",
            "-rw-r--r-- 1 root root  386 Dec 13 09:11 question-answering-query-example.py\n",
            "-rw-r--r-- 1 root root  317 Dec 13 09:11 text-classification-example.py\n",
            "-rw-r--r-- 1 root root  410 Dec 13 09:11 text-classification-query-example.py\n",
            "-rw-r--r-- 1 root root  324 Dec 13 09:11 text-generation-bloom560m-example.py\n",
            "-rw-r--r-- 1 root root  456 Dec 13 09:11 text-generation-bloom-example.py\n",
            "-rw-r--r-- 1 root root  721 Dec 13 09:11 text-generation-fbopt-example.py\n",
            "-rw-r--r-- 1 root root  568 Dec 13 09:11 text-generation-query-example.py\n",
            "-rw-r--r-- 1 root root 1.4K Dec 13 09:11 text-generation-zero-example.py\n",
            "-rw-r--r-- 1 root root  288 Dec 13 09:11 token-classification-example.py\n",
            "-rw-r--r-- 1 root root  403 Dec 13 09:11 token-classification-query-example.py\n",
            "-rw-r--r-- 1 root root 1.3K Dec 13 09:11 txt2img-example.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd DeepSpeed-MII/mii/legacy/examples/local && (nohup python conversational-example.py &)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd5oPp6ChEAk",
        "outputId": "11270784-6871-4083-d676-315572f46163"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat DeepSpeed-MII/mii/legacy/examples/local/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3YOtxtqYQob",
        "outputId": "0f4597e6-c3c1-4c54-8105-b5647d3d079f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:34:21,814] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:34:23.741561: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:34:23.741615: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:34:23.741646: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:34:24.887416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Deploying microsoft/DialoGPT-large...\n",
            "[2023-12-13 09:35:02,358] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:35:02,358] [INFO] [deployment.py:75:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
            "[2023-12-13 09:35:02,360] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:02,360] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:02,365] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:35:02,365] [INFO] [server.py:38:__init__] Hostfile /job/hostfile not found, creating hostfile.\n",
            "[2023-12-13 09:35:02,367] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp_tzqzbzl', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:02,367] [INFO] [server.py:103:_launch_server_process] MII server server launch: ['deepspeed', '-H', '/tmp/tmp_tzqzbzl', '-i', 'localhost:0', '--master_port', '29500', '--master_addr', 'localhost', '--no_ssh_check', '--no_local_rank', '--no_python', '/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--server-port', '50051', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:02,368] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:02,368] [INFO] [server.py:103:_launch_server_process] load balancer server launch: ['/usr/bin/python3', '-m', 'mii.legacy.launch.multi_gpu_server', '--deployment-name', 'microsoft/DialoGPT-large_deployment', '--load-balancer-port', '50050', '--restful-gateway-port', '51080', '--load-balancer', '--model-config', 'eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=']\n",
            "[2023-12-13 09:35:04,701] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:35:04,792] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:35:06,441] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=localhost --master_port=29500 --no_python --no_local_rank --enable_each_rank_log=None /usr/bin/python3 -m mii.legacy.launch.multi_gpu_server --deployment-name microsoft/DialoGPT-large_deployment --load-balancer-port 50050 --restful-gateway-port 51080 --server-port 50051 --model-config eyJtb2RlbCI6ICJtaWNyb3NvZnQvRGlhbG9HUFQtbGFyZ2UiLCAidGFzayI6ICJjb252ZXJzYXRpb25hbCIsICJkdHlwZSI6ICJ0b3JjaC5mbG9hdDMyIiwgIm1vZGVsX3BhdGgiOiAiL3RtcC9taWlfbW9kZWxzIiwgImxvYWRfd2l0aF9zeXNfbWVtIjogZmFsc2UsICJtZXRhX3RlbnNvciI6IGZhbHNlLCAiZGVwbG95X3JhbmsiOiBbMF0sICJ0b3JjaF9kaXN0X3BvcnQiOiAyOTUwMCwgInJlcGxpY2FfbnVtIjogMSwgInJlcGxpY2FfY29uZmlncyI6IFt7Imhvc3RuYW1lIjogImxvY2FsaG9zdCIsICJ0ZW5zb3JfcGFyYWxsZWxfcG9ydHMiOiBbNTAwNTFdLCAidG9yY2hfZGlzdF9wb3J0IjogMjk1MDAsICJncHVfaW5kaWNlcyI6IFswXX1dLCAicHJvZmlsZV9tb2RlbF90aW1lIjogZmFsc2UsICJza2lwX21vZGVsX2NoZWNrIjogdHJ1ZSwgImhmX2F1dGhfdG9rZW4iOiBudWxsLCAidHJ1c3RfcmVtb3RlX2NvZGUiOiBmYWxzZSwgInBpcGVsaW5lX2t3YXJncyI6IHt9LCAiZW5hYmxlX2RlZXBzcGVlZCI6IHRydWUsICJlbmFibGVfemVybyI6IGZhbHNlLCAiZHNfY29uZmlnIjoge30sICJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgInJlcGxhY2Vfd2l0aF9rZXJuZWxfaW5qZWN0IjogdHJ1ZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJtYXhfdG9rZW5zIjogMTAyNH0=\n",
            "2023-12-13 09:35:06.733281: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:35:06.733335: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:35:06.733361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2023-12-13 09:35:07,373] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:07,373] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "2023-12-13 09:35:07.945468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:35:08,670] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-12-13 09:35:09,107] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:09,107] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2023-12-13 09:35:10,326] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2023-12-13 09:35:12,378] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:12,378] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:12,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:35:14.450109: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:35:14.450168: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:35:14.450206: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:35:15.559751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-12-13 09:35:16,710] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:16,710] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:35:17,382] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:17,382] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:22,387] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:22,387] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:27,392] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:27,392] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "> --------- MII Settings: ds_optimize=True, replace_with_kernel_inject=True, enable_cuda_graph=False \n",
            "[2023-12-13 09:35:28,542] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown\n",
            "[2023-12-13 09:35:28,542] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2023-12-13 09:35:28,543] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/transformer_inference/build.ninja...\n",
            "Building extension module transformer_inference...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module transformer_inference...\n",
            "Time to load transformer_inference op: 0.09584593772888184 seconds\n",
            "[2023-12-13 09:35:29,007] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1280, 'intermediate_size': 5120, 'heads': 20, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000}\n",
            "[2023-12-13 09:35:32,397] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:32,397] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:63:_wait_until_server_is_live] waiting for server to start...\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n",
            "[2023-12-13 09:35:37,402] [INFO] [server.py:64:_wait_until_server_is_live] server has started on ports [50051]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd DeepSpeed-MII/mii/legacy/examples/local && python conversational-query-example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkEnx0FLgyrr",
        "outputId": "796ea767-bf9c-4e16-abd6-bd3b641ae302"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:41:57,043] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-12-13 09:41:58.932739: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 09:41:58.932786: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 09:41:58.932813: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 09:42:00.069422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Querying microsoft/DialoGPT-large...\n",
            "[2023-12-13 09:42:00,919] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:42:00,919] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "conversation_id: \"6af613b6-569c-5c22-9c37-2ed93f31d3af\"\n",
            "past_user_inputs: \"DeepSpeed is the greatest\"\n",
            "generated_responses: \"I love it. It\\'s so much fun.\"\n",
            "time_taken: 0.218658924\n",
            "model_time_taken: -1\n",
            "\n",
            "time_taken: 0.2186589241027832\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DeepSpeed-MII/mii/legacy/examples/local/conversational-query-example.py\", line 30, in <module>\n",
            "    result = generator.query({\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mii/legacy/client.py\", line 77, in query\n",
            "    return self.asyncio_loop.run_until_complete(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 99, in run_until_complete\n",
            "    return f.result()\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mii/legacy/client.py\", line 72, in _request_async_response\n",
            "    proto_request = task_methods.pack_request_to_proto(request_dict, **query_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/mii/legacy/method_table.py\", line 227, in pack_request_to_proto\n",
            "    return modelresponse_pb2.ConversationRequest(\n",
            "TypeError: bad argument type for built-in operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GnJmsWViIPY",
        "outputId": "01e749fc-ad0c-4e93-82c6-36ec6a0babed"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 7710 root   23u  IPv4 190929      0t0  TCP localhost:45268->localhost:50051 (ESTABLISHED)\n",
            "python3 7844 root   39u  IPv4 185094      0t0  TCP *:50051 (LISTEN)\n",
            "python3 7844 root   40u  IPv4 189060      0t0  TCP localhost:50051->localhost:45268 (ESTABLISHED)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.terminate(\"microsoft/DialoGPT-large_deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob07Ri3Cj37Z",
        "outputId": "ce45648d-049f-417c-bfdf-71892d92d8f6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-13 09:43:59,850] [INFO] [terminate.py:12:terminate] Terminating server for microsoft/DialoGPT-large_deployment\n",
            "[2023-12-13 09:43:59,850] [INFO] [terminate.py:12:terminate] Terminating server for microsoft/DialoGPT-large_deployment\n",
            "[2023-12-13 09:43:59,854] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:43:59,855] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:43:59,861] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter hf_auth_token is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n",
            "[2023-12-13 09:43:59,862] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter trust_remote_code is deprecated. Parameter will be removed. Please use the `pipeline_kwargs` field to pass kwargs to the HuggingFace pipeline creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i:50051"
      ],
      "metadata": {
        "id": "9Hc1PHeij-p9"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSpeed-FastGen\n",
        "\n",
        "https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/chinese/README.md\n",
        "\n",
        "ds-mii-fastgen 是为了解决LLMs推理加速，提高推理吞吐，以及长prompt问题(llm已经支持长token,树万token)。\n",
        "\n",
        "GPT-4 和 LLaMA 这样的大型语言模型（LLMs）已在各个层次上成为了集成 AI 的主流服务应用。从常规聊天模型到文档摘要，从自动驾驶到各个软件中的Copilot功能，这些模型的部署和服务需求正在迅速增加。像 DeepSpeed、PyTorch 和其他几个框架可以在 LLM 训练期间实现良好的硬件利用率。但它们在与用户互动及处理开放式文本生成等任务时，受限于这些操作的计算密集度相对较低，现有系统往往在推理吞吐量上遇到瓶颈。\n",
        "\n",
        "为了解决这一问题， vLLM 这样由 PagedAttention 驱动的框架和 [Orca](https://www.usenix.org/system/files/osdi22-yu.pdf) 这样的系统显著提高了 LLM 推理的性能。然而，这些系统在面对长提示的工作负载时，依旧难以提供良好的服务质量。随着越来越多的模型（例如 MPT-StoryWriter）和系统（例如DeepSpeed Ulysses）支持延伸到数万个令牌的上下文窗口，这些长提示工作负载变得越来越重要。为了更好地理解问题，我们在下文中提供了详细的示例来说明 LLM 的文本生成是如何在“提示处理”和“生成”的这两个阶段中工作的。当系统将它们视为不同的阶段时，生成阶段将被提示处理所抢占，这可能会破坏服务级别协议（SLAs）。\n",
        "\n",
        "tips:\n",
        "- vllm 跟进 https://github.com/vllm-project/vllm/issues/1562\n",
        "\n",
        "DeepSpeed-FastGen 框架，它通过采用动态 SplitFuse 技术，能够提供比vLLM 等先进系统高出多达 2.3 倍的有效吞吐量。DeepSpeed-FastGen 是 DeepSpeed-MII 和 DeepSpeed-Inference 的结合，提供了一个易于使用的服务系统\n",
        "\n",
        "\n",
        "\n",
        "DeepSpeed-FastGen 是 [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) 和 [DeepSpeed-Inference](https://github.com/microsoft/DeepSpeed) 的协同组合，如下图所示。这两个软件包共同提供了系统的各个组成部分，包括前端 API、用于使用动态 SplitFuse 调度批次的主机和设备基础设施、优化的内核实现，以及构建新模型实现的工具。\n",
        "![](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-fastgen/assets/images/fastgen-arch-light.png?raw=true)"
      ],
      "metadata": {
        "id": "OqJfbV1oPadS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 部署选项\n",
        "示例均可在 DeepSpeedExamples 中运行。安装后，有两种部署方式：交互式非持久管道或持久化服务部署："
      ],
      "metadata": {
        "id": "EqMkpc3OORnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 非持久管道\n",
        "非持久管道部署是快速入门的好方法，只需几行代码即可完成。非持久模型只在您运行的 python 脚本期间存在，适用于临时交互式会话。\n",
        "\n"
      ],
      "metadata": {
        "id": "joIQE6ufOaR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/microsoft/DeepSpeed-MII/issues/273 need SM>=8.0 (Ampere+) A100\n",
        "# if use T4 GPU , need use https://github.com/microsoft/DeepSpeed-MII/tree/main/mii/legacy\n",
        "from mii import pipeline\n",
        "pipe = pipeline(\"mistralai/Mistral-7B-v0.1\")\n",
        "output = pipe([\"Hello, my name is\", \"DeepSpeed is\"], max_new_tokens=128)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "C-Gkv5ZAHgBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 持久部署\n",
        "持久部署非常适合用于长时间运行和生产的应用。持久部署使用了轻量级的 GRPC 服务器，可以使用以下两行代码创建：\n",
        "\n"
      ],
      "metadata": {
        "id": "VmbRsaW7OdGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mii\n",
        "mii.serve(\"mistralai/Mistral-7B-v0.1\")\n"
      ],
      "metadata": {
        "id": "UdH3NCrBOfjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上述服务器可以同时被多个客户端查询，这要归功于 DeepSpeed-MII 内置的负载平衡器。创建客户端也只需要两行代码：\n",
        "\n",
        "client = mii.client(\"mistralai/Mistral-7B-v0.1\")\n",
        "output = client.generate(\"Deepspeed is\", max_new_tokens=128)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "KruNB7amO1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#持久部署可以在不再需要时终止：\n",
        "\n",
        "client.terminate_server()\n"
      ],
      "metadata": {
        "id": "1DzeTAl4O9Vg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}