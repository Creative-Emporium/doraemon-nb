{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrDOklcvvkFKlPGoFraBNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/corenet_train_a_pic_classification_model_on_a_new_dataset_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vtX_nCcR9Ta",
        "outputId": "ddc7e242-d2a0-4fd6-aa7e-e89916dcb987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'corenet'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (837/837), done.\u001b[K\n",
            "remote: Compressing objects: 100% (640/640), done.\u001b[K\n",
            "remote: Total 837 (delta 188), reused 834 (delta 185), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (837/837), 795.35 KiB | 6.17 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/apple/corenet.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/corenet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-lgvjLJmOS4",
        "outputId": "e20e7308-a01c-42f2-fe1e-01e044ee6a9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/corenet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install && git lfs pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98U8O6IWpDOg",
        "outputId": "d726e4d7-ba91-479e-ecaf-19ac8d5b2088"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --editable ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD3BRa14pHII",
        "outputId": "c0f7d6e0-f013-4025-9b4e-1df00021e756"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/corenet\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting psutil==5.9.8 (from corenet==0.1.0)\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ujson==5.9.0 (from corenet==0.1.0)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.4.1.post1 (from corenet==0.1.0)\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-image==0.22.0 (from corenet==0.1.0)\n",
            "  Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml==6.0.1 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (6.0.1)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision==0.17.1 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: torchtext==0.17.1 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (0.17.1)\n",
            "Requirement already satisfied: torchaudio==2.2.1 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (0.7.1)\n",
            "Collecting coremltools==7.1 (from corenet==0.1.0)\n",
            "  Downloading coremltools-7.1-cp310-none-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools==2.0.7 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (2.0.7)\n",
            "Collecting cityscapesscripts==2.2.2 (from corenet==0.1.0)\n",
            "  Downloading cityscapesScripts-2.2.2-py3-none-any.whl (473 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.3/473.3 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorchvideo==0.1.5 (from corenet==0.1.0)\n",
            "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av==12.0.0 (from corenet==0.1.0)\n",
            "  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fvcore==0.1.5.post20221221 (from corenet==0.1.0)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy==6.2.0 (from corenet==0.1.0)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py==3.10.0 (from corenet==0.1.0)\n",
            "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybase64==1.3.2 (from corenet==0.1.0)\n",
            "  Downloading pybase64-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex==2023.12.25 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (2023.12.25)\n",
            "Collecting pyarrow==15.0.2 (from corenet==0.1.0)\n",
            "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.26.4 (from corenet==0.1.0)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.13.0 (from corenet==0.1.0)\n",
            "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.2.1 (from corenet==0.1.0)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.10/dist-packages (from corenet==0.1.0) (4.66.2)\n",
            "Collecting setuptools==69.2.0 (from corenet==0.1.0)\n",
            "  Downloading setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.5/821.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3==1.28.30 (from corenet==0.1.0)\n",
            "  Downloading boto3-1.28.30-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.32.0,>=1.31.30 (from boto3==1.28.30->corenet==0.1.0)\n",
            "  Downloading botocore-1.31.85-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3==1.28.30->corenet==0.1.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3==1.28.30->corenet==0.1.0)\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts==2.2.2->corenet==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts==2.2.2->corenet==0.1.0) (9.4.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from cityscapesscripts==2.2.2->corenet==0.1.0) (1.4.4)\n",
            "Collecting pyquaternion (from cityscapesscripts==2.2.2->corenet==0.1.0)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Collecting coloredlogs (from cityscapesscripts==2.2.2->corenet==0.1.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing (from cityscapesscripts==2.2.2->corenet==0.1.0)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf<=4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from coremltools==7.1->corenet==0.1.0) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from coremltools==7.1->corenet==0.1.0) (1.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from coremltools==7.1->corenet==0.1.0) (24.0)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from coremltools==7.1->corenet==0.1.0) (23.2.0)\n",
            "Collecting cattrs (from coremltools==7.1->corenet==0.1.0)\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyaml (from coremltools==7.1->corenet==0.1.0)\n",
            "  Downloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.2.0->corenet==0.1.0) (0.2.13)\n",
            "Collecting yacs>=0.1.6 (from fvcore==0.1.5.post20221221->corenet==0.1.0)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20221221->corenet==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20221221->corenet==0.1.0) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore==0.1.5.post20221221->corenet==0.1.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.1->corenet==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.1->corenet==0.1.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.1->corenet==0.1.0) (2024.1)\n",
            "Collecting parameterized (from pytorchvideo==0.1.5->corenet==0.1.0)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo==0.1.5->corenet==0.1.0) (3.3)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.22.0->corenet==0.1.0) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.22.0->corenet==0.1.0) (2024.4.18)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.22.0->corenet==0.1.0) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.1.post1->corenet==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.1.post1->corenet==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->corenet==0.1.0) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->corenet==0.1.0) (4.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->corenet==0.1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->corenet==0.1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->corenet==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->corenet==0.1.0) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->corenet==0.1.0) (2.31.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->corenet==0.1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore==0.1.5.post20221221->corenet==0.1.0)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts==2.2.2->corenet==0.1.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts==2.2.2->corenet==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts==2.2.2->corenet==0.1.0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts==2.2.2->corenet==0.1.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cityscapesscripts==2.2.2->corenet==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.1->corenet==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs->coremltools==7.1->corenet==0.1.0) (1.2.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->cityscapesscripts==2.2.2->corenet==0.1.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->corenet==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.1->corenet==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.1->corenet==0.1.0) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.1->corenet==0.1.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->coremltools==7.1->corenet==0.1.0) (1.3.0)\n",
            "Building wheels for collected packages: corenet, fvcore, pytorchvideo, iopath, typing\n",
            "  Building editable for corenet (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for corenet: filename=corenet-0.1.0-0.editable-py3-none-any.whl size=4844 sha256=6f44d7eeeb3897f56a60e1f91f1facf7751fcc4852b03a6ab9f954cad2db2415\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-klqlgcb4/wheels/76/51/96/c9bd9458d66a6f6f772660e9d76455ad93dce6d0bd3cc98be6\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=59879f90e38fd93a11ba8951567742b012eccab71ee589430e25d9c1eb02602e\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=257319b8abd3c0d60d655640bc113d5d8ab68729d768652ccf26b78a2f5c43da\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=6d0f237df83c5782431a0ce6b0eb930b00168a5a5e4382ca40269b06349623ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26306 sha256=199a7b72b241a97802ed52989a0f3db514c7bd192e3170859e003f9af5cfcdf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "Successfully built corenet fvcore pytorchvideo iopath typing\n",
            "Installing collected packages: yacs, ujson, typing, setuptools, pybase64, pyaml, psutil, portalocker, parameterized, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, jmespath, humanfriendly, ftfy, cattrs, av, scipy, pyquaternion, pyarrow, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, h5py, coremltools, coloredlogs, botocore, scikit-learn, scikit-image, s3transfer, nvidia-cusolver-cu12, fvcore, pytorchvideo, cityscapesscripts, boto3, corenet\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.19.3\n",
            "    Uninstalling scikit-image-0.19.3:\n",
            "      Successfully uninstalled scikit-image-0.19.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed av-12.0.0 boto3-1.28.30 botocore-1.31.85 cattrs-23.2.3 cityscapesscripts-2.2.2 coloredlogs-15.0.1 coremltools-7.1 corenet-0.1.0 ftfy-6.2.0 fvcore-0.1.5.post20221221 h5py-3.10.0 humanfriendly-10.0 iopath-0.1.10 jmespath-1.0.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pandas-2.2.1 parameterized-0.9.0 portalocker-2.8.2 psutil-5.9.8 pyaml-24.4.0 pyarrow-15.0.2 pybase64-1.3.2 pyquaternion-0.9.9 pytorchvideo-0.1.5 s3transfer-0.6.2 scikit-image-0.22.0 scikit-learn-1.4.1.post1 scipy-1.13.0 setuptools-69.2.0 typing-3.7.4.3 ujson-5.9.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if os.getcwd().endswith(\"tutorials\"):\n",
        "    os.chdir(\"..\")\n",
        "\n",
        "assert os.path.exists(\n",
        "    \"corenet\"\n",
        "), f\"We should be in the root repository folder, but we are in {os.getcwd()}\"\n",
        "\n",
        "! mkdir -p projects/playground_cifar10/classification\n"
      ],
      "metadata": {
        "id": "-Xs1UF6QlxoA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file projects/playground_cifar10/classification/cifar10.yaml\n",
        "common:\n",
        "    log_freq: 2000                 # Log the training metrics every 2000 iterations.\n",
        "\n",
        "dataset:\n",
        "    category: classification\n",
        "    name: \"cifar10\"                # We'll register the \"cifar10\" name at DATASET_REGISTRY later in this tutorial.\n",
        "\n",
        "    # The `corenet-train` entrypoint uses train_batch_size0 and val_batch_size0 values to construct\n",
        "    # training/validation batches during training. The `corenet-eval` entrypoint uses eval_batch_size0 to\n",
        "    # construct batches during evaluation (ie test).\n",
        "    #\n",
        "    # The effective batch size is: num_nodes x num_gpus x train_batch_size0\n",
        "    train_batch_size0: 4\n",
        "    val_batch_size0: 4\n",
        "    eval_batch_size0: 1\n",
        "\n",
        "    workers: 2\n",
        "    persistent_workers: true\n",
        "    pin_memory: true\n",
        "\n",
        "model:\n",
        "    classification:\n",
        "        name: \"two_layer\"          # We'll register the \"two_layer\" name at MODEL_REGISTRY later in this tutorial.\n",
        "        n_classes: 10\n",
        "\n",
        "    layer:\n",
        "        # Weight initialization parameters:\n",
        "        conv_init: \"kaiming_normal\"\n",
        "        linear_init: \"trunc_normal\"\n",
        "        linear_init_std_dev: 0.02\n",
        "\n",
        "\n",
        "sampler:\n",
        "    name: batch_sampler\n",
        "\n",
        "    # The following dimensions will be passed to the dataset.__get__ method, and the dataset produces samples\n",
        "    # cropped and resized to the requested dimensions.\n",
        "    bs:\n",
        "        crop_size_width: 32\n",
        "        crop_size_height: 32\n",
        "\n",
        "loss:\n",
        "    category: classification\n",
        "    classification:\n",
        "        name: cross_entropy       # The implemention is available in \"corenet/loss_fn/\" folder.\n",
        "\n",
        "optim:\n",
        "    name: sgd\n",
        "    sgd:\n",
        "        momentum: 0.9\n",
        "\n",
        "scheduler:\n",
        "    name: fixed                    # The implementation is available in \"corenet/optims/scheduler/\" folder.\n",
        "    max_epochs: 2\n",
        "    fixed:\n",
        "        lr: 0.001                  # Fixed Learning Rate\n",
        "\n",
        "stats:\n",
        "  val: [\"loss\", \"top1\"]            # Metrics to log\n",
        "  train: [\"loss\", \"top1\"]\n",
        "  checkpoint_metric: top1          # Assigns a checkpoint to results/checkpoint_best.pt\n",
        "  checkpoint_metric_max: true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cae2K9Oql-0R",
        "outputId": "ecaedf49-ecb4-4952-a5ac-e6c7cc316835"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing projects/playground_cifar10/classification/cifar10.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file corenet/data/datasets/classification/playground_dataset.py\n",
        "\n",
        "from argparse import Namespace\n",
        "from typing import Any, Dict, Tuple\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from corenet.data.datasets import DATASET_REGISTRY\n",
        "from corenet.data.datasets.dataset_base import BaseDataset\n",
        "\n",
        "\n",
        "@DATASET_REGISTRY.register(name=\"cifar10\", type=\"classification\")\n",
        "class Cifar10(BaseDataset):\n",
        "    CLASS_NAMES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    def __init__(self, opts: Namespace, **kwargs) -> None:\n",
        "        super().__init__(opts, **kwargs)\n",
        "        self._torchvision_dataset = torchvision.datasets.CIFAR10(\n",
        "            \"/tmp/cifar10_cache\",\n",
        "            train=self.is_training,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._torchvision_dataset)\n",
        "\n",
        "    def __getitem__(self, sample_size_and_index: Tuple[int]) -> Dict[str, Any]:\n",
        "        # In CoreNet, not only does the sampler determine the index of the samples, but\n",
        "        # also the sampler determines the crop size dynamically for each batch. This\n",
        "        # allows samplers to train multi-scale models more efficiently.\n",
        "        # See: corenet/data/sampler/variable_batch_sampler.py\n",
        "        (crop_size_h, crop_size_w, sample_index) = sample_size_and_index\n",
        "\n",
        "        img, target = self._torchvision_dataset[sample_index]\n",
        "\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                transforms.Resize(size=(crop_size_h, crop_size_w)),\n",
        "            ]\n",
        "        )\n",
        "        img = transform(img)\n",
        "        return {\n",
        "            \"samples\": img,\n",
        "            \"targets\": target,\n",
        "        }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O52LqGhjmfFZ",
        "outputId": "cee0b1fd-a8e1-4dce-f6fd-76cd25cb8dc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing corenet/data/datasets/classification/playground_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file corenet/modeling/models/classification/playground_model.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from corenet.modeling.models import MODEL_REGISTRY\n",
        "from corenet.modeling.models.base_model import BaseAnyNNModel\n",
        "\n",
        "\n",
        "@MODEL_REGISTRY.register(\"two_layer\", type=\"classification\")\n",
        "class Net(BaseAnyNNModel):\n",
        "    \"\"\"A simple 2-layer CNN, inspired by https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\"\"\"\n",
        "\n",
        "    def __init__(self, opts: argparse.Namespace) -> None:\n",
        "        super().__init__(opts)\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.reset_parameters(opts)  # Initialize the weights\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2zS5wbfndT4",
        "outputId": "0f4da5b8-354c-4cbb-c1d8-48d1e4ca06d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing corenet/modeling/models/classification/playground_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!corenet-train --common.config-file projects/playground_cifar10/classification/cifar10.yaml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcFCQ9X6ooYZ",
        "outputId": "20f70491-58e0-4bf5-f353-d0377395b7fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-27 03:45:34 - \u001b[93m\u001b[1mDEBUG   \u001b[0m - Cannot load internal arguments, skipping.\n",
            "2024-04-27 03:45:34 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Random seeds are set to 0\n",
            "2024-04-27 03:45:34 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Using PyTorch version 2.2.1+cu121\n",
            "2024-04-27 03:45:34 - \u001b[33m\u001b[1mWARNING\u001b[0m - No GPUs available. Using CPU\n",
            "2024-04-27 03:45:34 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Setting --ddp.world-size the same as the number of available gpus.\n",
            "2024-04-27 03:45:34 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Directory created at: results/run_1\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/cifar10_cache/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:01<00:00, 105036546.72it/s]\n",
            "Extracting /tmp/cifar10_cache/cifar-10-python.tar.gz to /tmp/cifar10_cache\n",
            "2024-04-27 03:45:40 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Training dataset details are given below\n",
            "Cifar10(\n",
            "\troot= \n",
            "\tis_training=True \n",
            "\tnum_samples=50000\n",
            ")\n",
            "Files already downloaded and verified\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Validation dataset details are given below\n",
            "Cifar10(\n",
            "\troot= \n",
            "\tis_training=False \n",
            "\tnum_samples=10000\n",
            ")\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Training sampler details: BatchSampler(\n",
            "\t num_repeat=1\n",
            "\t trunc_rep_aug=False\n",
            "\tbase_im_size=(h=32, w=32)\n",
            "\tbase_batch_size=4\n",
            ")\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Validation sampler details: BatchSampler(\n",
            "\t num_repeat=1\n",
            "\t trunc_rep_aug=False\n",
            "\tbase_im_size=(h=32, w=32)\n",
            "\tbase_batch_size=4\n",
            ")\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Number of data workers: 2\n",
            "2024-04-27 03:45:41 - \u001b[32m\u001b[1mINFO   \u001b[0m - Trainable parameters: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - \u001b[36mModel\u001b[0m\n",
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "\u001b[31m=================================================================\u001b[0m\n",
            "                                Net Summary\n",
            "\u001b[31m=================================================================\u001b[0m\n",
            "Total parameters     =    0.062 M\n",
            "Total trainable parameters =    0.062 M\n",
            "\n",
            "2024-04-27 03:45:41 - \u001b[33m\u001b[1mWARNING\u001b[0m - Profiling not available, dummy_input_and_label not implemented for this model.\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - \u001b[36mLoss function\u001b[0m\n",
            "CrossEntropy(\n",
            "\t ignore_idx=-1\n",
            "\t class_weighting=False\n",
            "\t label_smoothing=0.0\n",
            ")\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - \u001b[36mOptimizer\u001b[0m\n",
            "SGDOptimizer (\n",
            "\t dampening: [0]\n",
            "\t differentiable: [False]\n",
            "\t foreach: [None]\n",
            "\t lr: [0.1]\n",
            "\t maximize: [False]\n",
            "\t momentum: [0.9]\n",
            "\t nesterov: [False]\n",
            "\t weight_decay: [4e-05]\n",
            ")\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Max. epochs for training: 2\n",
            "2024-04-27 03:45:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - \u001b[36mLearning rate scheduler\u001b[0m\n",
            "FixedLRScheduler(\n",
            "\tlr=0.001\n",
            " )\n",
            "2024-04-27 03:45:41 - \u001b[32m\u001b[1mINFO   \u001b[0m - Configuration file is stored here: \u001b[36mresults/run_1/config.yaml\u001b[0m\n",
            "\u001b[31m===========================================================================\u001b[0m\n",
            "2024-04-27 03:45:43 - \u001b[32m\u001b[1mINFO   \u001b[0m - Training epoch 0\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "2024-04-27 03:45:43 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [       1/10000000], loss: 2.2316, top1: 0.0, LR: [0.001], Avg. batch load time: 0.204, Elapsed time:  0.42\n",
            "2024-04-27 03:46:00 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    2001/10000000], loss: 2.2085, top1: 15.9545, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 17.16\n",
            "2024-04-27 03:46:18 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    4001/10000000], loss: 2.0467, top1: 23.0005, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 34.95\n",
            "2024-04-27 03:46:35 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    6001/10000000], loss: 1.9302, top1: 27.7662, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 52.36\n",
            "2024-04-27 03:46:52 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    8001/10000000], loss: 1.8426, top1: 31.3086, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 69.02\n",
            "2024-04-27 03:47:08 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [   10001/10000000], loss: 1.7835, top1: 33.7816, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 85.55\n",
            "2024-04-27 03:47:27 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [   12001/10000000], loss: 1.7358, top1: 35.6929, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 104.54\n",
            "2024-04-27 03:47:31 - \u001b[34m\u001b[1mLOGS   \u001b[0m - *** Training summary for epoch 0\n",
            "\t loss=1.7252 || top1=36.176\n",
            "2024-04-27 03:47:34 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [       4/   10000], loss: 0.6952, top1: 100.0, LR: [0.001], Avg. batch load time: 0.000, Elapsed time:  0.08\n",
            "2024-04-27 03:47:44 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    8004/   10000], loss: 1.4251, top1: 49.1004, LR: [0.001], Avg. batch load time: 0.000, Elapsed time: 10.90\n",
            "2024-04-27 03:47:47 - \u001b[34m\u001b[1mLOGS   \u001b[0m - *** Validation summary for epoch 0\n",
            "\t loss=1.433 || top1=48.47\n",
            "2024-04-27 03:47:47 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Best checkpoint with score 48.47 saved at results/run_1/checkpoint_best.pt\n",
            "2024-04-27 03:47:47 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Last training checkpoint is saved at: results/run_1/training_checkpoint_last.pt\n",
            "2024-04-27 03:47:47 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Last checkpoint's model state is saved at: results/run_1/checkpoint_last.pt\n",
            "\u001b[31m===========================================================================\u001b[0m\n",
            "2024-04-27 03:47:49 - \u001b[32m\u001b[1mINFO   \u001b[0m - Training epoch 1\n",
            "2024-04-27 03:47:50 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [   12501/10000000], loss: 1.3636, top1: 50.0, LR: [0.001], Avg. batch load time: 0.133, Elapsed time:  0.15\n",
            "2024-04-27 03:48:07 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [   14501/10000000], loss: 1.4402, top1: 47.8761, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 17.29\n",
            "2024-04-27 03:48:23 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [   16501/10000000], loss: 1.4155, top1: 49.2002, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 33.84\n",
            "2024-04-27 03:48:41 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [   18501/10000000], loss: 1.4018, top1: 49.8042, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 51.22\n",
            "2024-04-27 03:48:59 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [   20501/10000000], loss: 1.3916, top1: 50.2343, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 69.37\n",
            "2024-04-27 03:49:15 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [   22501/10000000], loss: 1.378, top1: 50.5974, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 85.88\n",
            "2024-04-27 03:49:32 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [   24501/10000000], loss: 1.3638, top1: 51.0624, LR: [0.001], Avg. batch load time: 0.003, Elapsed time: 102.53\n",
            "2024-04-27 03:49:38 - \u001b[34m\u001b[1mLOGS   \u001b[0m - *** Training summary for epoch 1\n",
            "\t loss=1.3633 || top1=51.062\n",
            "2024-04-27 03:49:40 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [       4/   10000], loss: 1.0539, top1: 50.0, LR: [0.001], Avg. batch load time: 0.000, Elapsed time:  0.02\n",
            "2024-04-27 03:49:51 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   1 [    8004/   10000], loss: 1.2947, top1: 53.6607, LR: [0.001], Avg. batch load time: 0.000, Elapsed time: 10.64\n",
            "2024-04-27 03:49:53 - \u001b[34m\u001b[1mLOGS   \u001b[0m - *** Validation summary for epoch 1\n",
            "\t loss=1.2976 || top1=53.53\n",
            "2024-04-27 03:49:54 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Best checkpoint with score 53.53 saved at results/run_1/checkpoint_best.pt\n",
            "2024-04-27 03:49:54 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Last training checkpoint is saved at: results/run_1/training_checkpoint_last.pt\n",
            "2024-04-27 03:49:54 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Last checkpoint's model state is saved at: results/run_1/checkpoint_last.pt\n",
            "2024-04-27 03:49:54 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Training took 00:04:12.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh results/run_1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FDJj3vGtlyT",
        "outputId": "fedd51a0-0d67-400a-dafb-e88ae01a3afe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.5M\n",
            "-rw-r--r-- 1 root root 246K Apr 27 03:49 checkpoint_best.pt\n",
            "-rw-r--r-- 1 root root 246K Apr 27 03:49 checkpoint_last.pt\n",
            "-rw-r--r-- 1 root root 246K Apr 27 03:47 checkpoint_score_48.4700.pt\n",
            "-rw-r--r-- 1 root root 246K Apr 27 03:49 checkpoint_score_53.5300.pt\n",
            "-rw-r--r-- 1 root root 1.9K Apr 27 03:45 config.yaml\n",
            "-rw-r--r-- 1 root root 492K Apr 27 03:49 training_checkpoint_last.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!corenet-eval \\\n",
        "    --common.config-file projects/playground_cifar10/classification/cifar10.yaml \\\n",
        "    --model.classification.pretrained results/run_1/checkpoint_best.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6E65JHDGEhS",
        "outputId": "74e19837-a1e6-42b9-dc40-aeaae6180cf6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-27 05:47:58 - \u001b[93m\u001b[1mDEBUG   \u001b[0m - Cannot load internal arguments, skipping.\n",
            "2024-04-27 05:47:59 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Random seeds are set to 0\n",
            "2024-04-27 05:47:59 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Using PyTorch version 2.2.1+cu121\n",
            "2024-04-27 05:47:59 - \u001b[33m\u001b[1mWARNING\u001b[0m - No GPUs available. Using CPU\n",
            "2024-04-27 05:47:59 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Setting --ddp.world-size the same as the number of available gpus.\n",
            "2024-04-27 05:47:59 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Directory exists at: results/run_1\n",
            "Files already downloaded and verified\n",
            "2024-04-27 05:48:00 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Evaluation dataset details: \n",
            "Cifar10(\n",
            "\troot= \n",
            "\tis_training=False \n",
            "\tnum_samples=10000\n",
            ")\n",
            "2024-04-27 05:48:00 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Evaluation sampler details: BatchSampler(\n",
            "\t num_repeat=1\n",
            "\t trunc_rep_aug=False\n",
            "\tbase_im_size=(h=32, w=32)\n",
            "\tbase_batch_size=1\n",
            ")\n",
            "2024-04-27 05:48:00 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Pretrained weights are loaded from results/run_1/checkpoint_best.pt\n",
            "2024-04-27 05:48:00 - \u001b[32m\u001b[1mINFO   \u001b[0m - Trainable parameters: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
            "2024-04-27 05:48:00 - \u001b[34m\u001b[1mLOGS   \u001b[0m - \u001b[36mModel\u001b[0m\n",
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "\u001b[31m=================================================================\u001b[0m\n",
            "                                Net Summary\n",
            "\u001b[31m=================================================================\u001b[0m\n",
            "Total parameters     =    0.062 M\n",
            "Total trainable parameters =    0.062 M\n",
            "\n",
            "2024-04-27 05:48:00 - \u001b[33m\u001b[1mWARNING\u001b[0m - Profiling not available, dummy_input_and_label not implemented for this model.\n",
            "2024-04-27 05:48:00 - \u001b[34m\u001b[1mLOGS   \u001b[0m - \u001b[36mLoss function\u001b[0m\n",
            "CrossEntropy(\n",
            "\t ignore_idx=-1\n",
            "\t class_weighting=False\n",
            "\t label_smoothing=0.0\n",
            ")\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "2024-04-27 05:48:00 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [       1/   10000], loss: 0.872, top1: 100.0, LR: 0.000000, Avg. batch load time: 0.000, Elapsed time:  0.08\n",
            "2024-04-27 05:48:08 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    2001/   10000], loss: 1.2733, top1: 54.023, LR: 0.000000, Avg. batch load time: 0.000, Elapsed time:  8.23\n",
            "2024-04-27 05:48:16 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    4001/   10000], loss: 1.2924, top1: 53.3117, LR: 0.000000, Avg. batch load time: 0.000, Elapsed time: 15.88\n",
            "2024-04-27 05:48:24 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    6001/   10000], loss: 1.2911, top1: 53.941, LR: 0.000000, Avg. batch load time: 0.000, Elapsed time: 23.79\n",
            "2024-04-27 05:48:32 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Epoch:   0 [    8001/   10000], loss: 1.295, top1: 53.6558, LR: 0.000000, Avg. batch load time: 0.000, Elapsed time: 31.75\n",
            "2024-04-27 05:48:39 - \u001b[34m\u001b[1mLOGS   \u001b[0m - *** Evaluation summary for epoch 0\n",
            "\t loss=1.2976 || top1=53.53\n",
            "2024-04-27 05:48:39 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Evaluation took 38.941898345947266 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -hl results/run_1/checkpoint_best.pt projects/playground_cifar10/classification/cifar10.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH8-U4ccHCSJ",
        "outputId": "9e8da881-2257-445e-a77c-3d92e8326d0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1.9K Apr 27 03:29 projects/playground_cifar10/classification/cifar10.yaml\n",
            "-rw-r--r-- 1 root root 246K Apr 27 03:49 results/run_1/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from corenet.options.opts import get_training_arguments\n",
        "from corenet.modeling import get_model\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision.transforms import Compose, Resize, PILToTensor, CenterCrop\n",
        "from torchvision.transforms import ToPILImage\n",
        "from corenet.data.datasets.classification.playground_dataset import Cifar10\n",
        "\n",
        "config_file = \"projects/playground_cifar10/classification/cifar10.yaml\"\n",
        "pretrained_weights = \"results/run_1/checkpoint_best.pt\"\n",
        "\n",
        "opts = get_training_arguments(\n",
        "    args=[\n",
        "        \"--common.config-file\",\n",
        "        config_file,\n",
        "        \"--model.classification.pretrained\",\n",
        "        pretrained_weights,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load the model\n",
        "model = get_model(opts)\n",
        "model.eval()\n",
        "\n",
        "for image_path in [\"assets/cat.jpeg\", \"assets/dog.jpeg\"]:\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_transforms = Compose([CenterCrop(600), Resize(size=(32, 32)), PILToTensor()])\n",
        "\n",
        "    # Transform the image, normalize between 0 and 1\n",
        "    input_tensor = img_transforms(image)\n",
        "\n",
        "    # Show the transformed image\n",
        "    ToPILImage()(input_tensor).show()\n",
        "\n",
        "    input_tensor = input_tensor.to(torch.float).div(255.0)\n",
        "\n",
        "    # add dummy batch dimension\n",
        "    input_tensor = input_tensor[None, ...]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[0]\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        predictions = sorted(zip(probs.tolist(), Cifar10.CLASS_NAMES), reverse=True)\n",
        "        print(\n",
        "            \"Top 3 Predictions:\",\n",
        "            [f\"{cls}: {prob:.1%}\" for prob, cls in predictions[:3]],\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhqTg8ABGV3e",
        "outputId": "46f0260f-604c-4dcb-8d87-b594b4321be9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-27 09:35:47 - \u001b[93m\u001b[1mDEBUG   \u001b[0m - Cannot load internal arguments, skipping.\n",
            "2024-04-27 09:35:47 - \u001b[34m\u001b[1mLOGS   \u001b[0m - Pretrained weights are loaded from results/run_1/checkpoint_best.pt\n",
            "2024-04-27 09:35:47 - \u001b[32m\u001b[1mINFO   \u001b[0m - Trainable parameters: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
            "Top 3 Predictions: ['bird: 40.6%', 'dog: 16.4%', 'cat: 15.3%']\n",
            "Top 3 Predictions: ['bird: 25.3%', 'ship: 25.1%', 'cat: 24.6%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里只是使用corenet 来训练模型的例子，训练仅仅是少量训练轮数，实现一个图片分类模型的训练。"
      ],
      "metadata": {
        "id": "cD7cvI0G7Jmh"
      }
    }
  ]
}