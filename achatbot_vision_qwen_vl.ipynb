{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/achatbot_vision_qwen_vl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CjzTHSHQwRK"
      },
      "source": [
        "# transformers qwen2-vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6p2eLzV1O0o"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi && lscpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5hIFzPcCeu4"
      },
      "outputs": [],
      "source": [
        "!pip show torch torchvision torchaudio transformers autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2xPMMcm-313"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U autoawq torch torchvision torchaudio git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCSBF0uY_uAZ"
      },
      "outputs": [],
      "source": [
        "!pip show torch torchvision torchaudio transformers autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kstQzvzkr5Jb"
      },
      "outputs": [],
      "source": [
        "!pip install -q qwen-vl-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ydqSihTr38d"
      },
      "outputs": [],
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from transformers import TextIteratorStreamer\n",
        "import torch\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from time import perf_counter\n",
        "\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-Int8\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-Int4\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-AWQ\"\n",
        "\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-Int8\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-Int4\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-AWQ\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "\n",
        "# default: Load the model on the available device(s)\n",
        "#model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "#    model_ckpt_name, torch_dtype=torch.float16, device_map=\"cuda\"\n",
        "#)\n",
        "\n",
        "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
        "# see detail: https://huggingface.co/docs/transformers/perf_infer_gpu_one\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "     model_ckpt_name,\n",
        "     torch_dtype=\"auto\",\n",
        "     #attn_implementation=\"sdpa\", #sdpa or flash_attention_2, no eager\n",
        "     #attn_implementation=\"flash_attention_2\", #sdpa or flash_attention_2, no eager\n",
        "     device_map=\"cuda\",\n",
        ")\n",
        "\n",
        "# default processer\n",
        "#processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
        "# You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
        "min_pixels = 256*28*28\n",
        "max_pixels = 1280*28*28\n",
        "processor = AutoProcessor.from_pretrained(model_ckpt_name, min_pixels=min_pixels, max_pixels=max_pixels)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"描述下图片内容\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "times=[]\n",
        "start_time = perf_counter()\n",
        "# Preparation for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "inputs = inputs.to(model.device)\n",
        "\n",
        "\n",
        "# Inference: Generation of the output with TextIteratorStreamer\n",
        "streamer = TextIteratorStreamer(processor, skip_prompt=True, skip_special_tokens=True)\n",
        "# Use Thread to run generation in background\n",
        "# Otherwise, the process is blocked until generation is complete\n",
        "# and no streaming effect can be observed.\n",
        "from threading import Thread\n",
        "generation_kwargs = dict(**inputs, streamer=streamer, max_new_tokens=128)\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "generated_text = \"\"\n",
        "for new_text in streamer:\n",
        "    #print(new_text)\n",
        "    times.append(perf_counter() - start_time)\n",
        "    generated_text += new_text\n",
        "    start_time = perf_counter()\n",
        "print(generated_text)\n",
        "\n",
        "print(\"times\",times)\n",
        "print(\"total cost:\",sum(times))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1V1ORJVRTWK"
      },
      "source": [
        "# achatbot vision lm qwen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyZ_1-dhRR-_"
      },
      "outputs": [],
      "source": [
        "!cd /content && rm -rf achatbot && git clone --recursive https://github.com/ai-bot-pro/achatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFvgRFMqRjlY",
        "outputId": "a994da5d-4d39-4332-ff51-607ab90df35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/achatbot\n"
          ]
        }
      ],
      "source": [
        "%cd /content/achatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEpbiR_SRnwl"
      },
      "outputs": [],
      "source": [
        "!bash scripts/pypi_achatbot.sh dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV7eALxLR1kT"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"dist/achatbot-0.0.7.2-py3-none-any.whl[tensorrt,daily_room_audio_stream,sense_voice_asr,deepgram_asr_processor,llm_transformers_manual_vision,einops,tts_edge,openai,queue]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKdU3WPptz7n"
      },
      "outputs": [],
      "source": [
        "# funasr > 1.1.6\n",
        "!pip show funasr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2U1dZCWDEHc"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download FunAudioLLM/SenseVoiceSmall  --local-dir ./models/FunAudioLLM/SenseVoiceSmall --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSneSkUoWF8Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download Qwen/Qwen2-VL-7B-Instruct --local-dir ./models/Qwen/Qwen2-VL-7B-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzNZFynaWVMy"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download Qwen/Qwen2-VL-2B-Instruct --local-dir ./models/Qwen/Qwen2-VL-2B-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lK8Af_Vs_Vq"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download unsloth/Llama-3.2-11B-Vision-Instruct --local-dir ./models/unsloth/Llama-3.2-11B-Vision-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CI3WeGKBaHKU"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download allenai/Molmo-7B-D-0924 --local-dir ./models/allenai/Molmo-7B-D-0924 --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT3Fd_HZTEku"
      },
      "outputs": [],
      "source": [
        "# cpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVP11g3FzCiF"
      },
      "outputs": [],
      "source": [
        "# gpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" LLM_DEVICE=cuda \\\n",
        "    VIDEO_FILE=\"/content/capture3_2024-09-15_20-10-27.mp4\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf4D2_cv7XK_"
      },
      "outputs": [],
      "source": [
        "# gpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" LLM_DEVICE=cuda LLM_CHAT_HISTORY_SIZE=0 \\\n",
        "    VIDEO_FILE=\"/content/capture3_2024-09-15_20-10-27.mp4\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98VEXQ8Adoqd"
      },
      "outputs": [],
      "source": [
        "#gpu\n",
        "!LLM_TAG=llm_transformers_manual_vision_molmo LLM_DEVICE=cuda \\\n",
        "    LLM_MODEL_NAME_OR_PATH=allenai/Molmo-7B-D-0924 \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_llama.TestTransformersVLlama.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u1-4DPXudpa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69936be-53b6-4ba1-8c2d-219cab93007f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-28 05:37:32.222413: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-28 05:37:32.240622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-28 05:37:32.261791: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-28 05:37:32.268334: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-28 05:37:32.283521: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-28 05:37:33.602229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-09-28 05:37:34,636 - numexpr.utils - INFO - /usr/local/lib/python3.10/dist-packages/numexpr/utils.py:161 - _init_num_threads - NumExpr defaulting to 12 threads.\n",
            "2024-09-28 05:37:34,862 - chat-bot - INFO - /content/achatbot/src/common/factory.py:68 - get_engine_by_tag - use llm_transformers_manual_vision_molmo engine\n",
            "2024-09-28 05:37:34,862 - chat-bot - INFO - /content/achatbot/src/common/factory.py:33 - get_instance - class: <class 'src.core.llm.transformers.manual_vision_molmo.TransformersManualVisionMolmoLLM'> args: {'lm_model_name_or_path': 'allenai/Molmo-7B-D-0924', 'lm_device_map': None, 'lm_device': 'cpu', 'lm_torch_dtype': 'auto', 'lm_attn_impl': None, 'user_role': 'user', 'warnup_prompt': \"Repeat the word 'weedge niu bi'.\", 'warnup_steps': 2, 'init_chat_role': 'system', 'init_chat_prompt': '', 'lm_gen_max_new_tokens': 1024, 'lm_gen_min_new_tokens': 0, 'lm_gen_do_sample': False, 'lm_gen_temperature': 0.0, 'lm_gen_top_k': 20, 'lm_gen_top_p': 0.8, 'lm_gen_repetition_penalty': 1.0, 'chat_history_size': 0, 'lm_stream': True, 'model_type': 'chat_completion'}\n",
            "Loading checkpoint shards: 100% 7/7 [00:01<00:00,  4.36it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "2024-09-28 05:37:39,350 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:124 - _warmup - Warming up TransformersManualVisionMolmoLLM\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "2024-09-28 05:38:32,364 - chat-bot - INFO - /content/achatbot/src/core/llm/__init__.py:46 - initLLMEngine - initLLMEngine: llm_transformers_manual_vision_molmo, TAG:llm_transformers_manual_vision_molmo | TransformersManualVisionMolmoLLM\n",
            "2024-09-28 05:38:32,365 - asyncio - WARNING - /usr/lib/python3.10/asyncio/base_events.py:1904 - _run_once - Executing <Task pending name='Task-1' coro=<IsolatedAsyncioTestCase._asyncioLoopRunner() running at /usr/lib/python3.10/unittest/async_case.py:95> wait_for=<Future pending cb=[Task.task_wakeup()] created at /usr/lib/python3.10/asyncio/base_events.py:429> created at /usr/lib/python3.10/unittest/async_case.py:117> took 62.885 seconds\n",
            "2024-09-28 05:38:32,369 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - PipelineSource#0 ---> FrameLogger#0 Frame: StartFrame#0\n",
            "2024-09-28 05:38:32,369 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: StartFrame#0\n",
            "2024-09-28 05:38:32,369 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - PipelineSource#0 ---> FrameLogger#0 Frame: VisionImageRawFrame#0(text: 请描述下图片, size: (570, 380), format: JPEG, bytes_len:649800, mode:RGB\n",
            "2024-09-28 05:38:32,369 - chat-bot - INFO - /content/achatbot/src/processors/vision/vision_processor.py:45 - run_vision - Analyzing image: VisionImageRawFrame#0(text: 请描述下图片, size: (570, 380), format: JPEG, bytes_len:649800, mode:RGB\n",
            "image_inputs--> [<PIL.Image.Image image mode=RGB size=560x392 at 0x7FF0B0555C00>]\n",
            "2024-09-28 05:39:04,984 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#0(text:  The image captures a surreal and humorous scene on a New York City street.)\n",
            "2024-09-28 05:39:15,665 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#1(text:  A man is sitting on the back of a yellow SUV,)\n",
            "2024-09-28 05:39:24,242 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#2(text:  which is parked in the middle of the road.)\n",
            "2024-09-28 05:39:37,600 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#3(text:  He's using an ironing board and iron to press a blue shirt,)\n",
            "2024-09-28 05:39:49,194 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#4(text:  seemingly oblivious to the fact that he's in the middle of traffic.)\n",
            "2024-09-28 05:39:59,005 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#5(text: \n",
            "\n",
            "The SUV has a silver bumper and black wheels.)\n",
            "2024-09-28 05:40:13,060 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#6(text:  The man is dressed in a long-sleeved yellow shirt with black writing,)\n",
            "2024-09-28 05:40:19,466 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#7(text:  and he's wearing brown shoes.)\n",
            "2024-09-28 05:40:23,780 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#8(text:  He's leaning forward,)\n",
            "2024-09-28 05:40:31,770 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#9(text:  focused on ironing the shirt on the board.)\n",
            "2024-09-28 05:40:36,132 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#10(text: \n",
            "\n",
            "In the background,)\n",
            "2024-09-28 05:40:43,525 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#11(text:  there's a yellow taxi passing by,)\n",
            "2024-09-28 05:40:54,221 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#12(text:  and a tall gray building with pink banners hanging from it.)\n",
            "2024-09-28 05:41:08,268 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#13(text:  The street is paved with black asphalt and has a white line running down the middle.)\n",
            "2024-09-28 05:41:26,872 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#14(text: \n",
            "\n",
            "This unusual situation creates a striking contrast between the everyday act of ironing and the busy urban environment.)\n",
            "2024-09-28 05:41:43,740 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#15(text:  It's a perfect example of the unexpected and quirky moments that can be found in city life,)\n",
            "2024-09-28 05:41:52,450 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#16(text:  blending the mundane with the extraordinary in a single,)\n",
            "2024-09-28 05:41:55,199 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#17(text:  captivating image.)\n",
            "2024-09-28 05:41:55,199 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - PipelineSource#0 ---> FrameLogger#0 Frame: StopTaskFrame#0\n",
            "2024-09-28 05:41:55,199 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: StopTaskFrame#0\n",
            "2024-09-28 05:41:55,200 - asyncio - WARNING - /usr/lib/python3.10/asyncio/base_events.py:1904 - _run_once - Executing <Task pending name='Task-3' coro=<PipelineTask._process_down_queue() running at /usr/local/lib/python3.10/dist-packages/apipeline/pipeline/task.py:140> wait_for=<Task pending name='Task-2' coro=<PipelineTask._process_up_queue() running at /usr/local/lib/python3.10/dist-packages/apipeline/pipeline/task.py:145> wait_for=<Future cancelled created at /usr/lib/python3.10/asyncio/base_events.py:429> cb=[gather.<locals>._done_callback() at /usr/lib/python3.10/asyncio/tasks.py:720, Task.task_wakeup()] created at /usr/lib/python3.10/asyncio/tasks.py:337> cb=[gather.<locals>._done_callback() at /usr/lib/python3.10/asyncio/tasks.py:720] created at /usr/lib/python3.10/asyncio/tasks.py:337> took 202.831 seconds\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 265.731s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "#cpu\n",
        "!LLM_TAG=llm_transformers_manual_vision_molmo \\\n",
        "    LLM_MODEL_NAME_OR_PATH=allenai/Molmo-7B-D-0924 \\\n",
        "    LLM_CHAT_HISTORY_SIZE=0 \\\n",
        "    python -m unittest test.integration.processors.test_vision_processor.TestVisionProcessor.test_run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gpu\n",
        "!LLM_TAG=llm_transformers_manual_vision_molmo LLM_DEVICE=cuda \\\n",
        "    LLM_MODEL_NAME_OR_PATH=allenai/Molmo-7B-D-0924 \\\n",
        "    LLM_CHAT_HISTORY_SIZE=0 \\\n",
        "    python -m unittest test.integration.processors.test_vision_processor.TestVisionProcessor.test_run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGGAlwt2KARr",
        "outputId": "edff9048-28f2-44d9-8d1c-bbd9c6932596"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-28 05:53:58.897116: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-28 05:53:58.915621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-28 05:53:58.937180: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-28 05:53:58.943765: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-28 05:53:58.959511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-28 05:54:00.280416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-09-28 05:54:01,305 - numexpr.utils - INFO - /usr/local/lib/python3.10/dist-packages/numexpr/utils.py:161 - _init_num_threads - NumExpr defaulting to 12 threads.\n",
            "2024-09-28 05:54:01,535 - chat-bot - INFO - /content/achatbot/src/common/factory.py:68 - get_engine_by_tag - use llm_transformers_manual_vision_molmo engine\n",
            "2024-09-28 05:54:01,536 - chat-bot - INFO - /content/achatbot/src/common/factory.py:33 - get_instance - class: <class 'src.core.llm.transformers.manual_vision_molmo.TransformersManualVisionMolmoLLM'> args: {'lm_model_name_or_path': 'allenai/Molmo-7B-D-0924', 'lm_device_map': None, 'lm_device': 'cuda', 'lm_torch_dtype': 'auto', 'lm_attn_impl': None, 'user_role': 'user', 'warnup_prompt': \"Repeat the word 'weedge niu bi'.\", 'warnup_steps': 2, 'init_chat_role': 'system', 'init_chat_prompt': '', 'lm_gen_max_new_tokens': 1024, 'lm_gen_min_new_tokens': 0, 'lm_gen_do_sample': False, 'lm_gen_temperature': 0.0, 'lm_gen_top_k': 20, 'lm_gen_top_p': 0.8, 'lm_gen_repetition_penalty': 1.0, 'chat_history_size': 0, 'lm_stream': True, 'model_type': 'chat_completion'}\n",
            "Loading checkpoint shards: 100% 7/7 [00:01<00:00,  4.13it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "2024-09-28 05:54:15,295 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:124 - _warmup - Warming up TransformersManualVisionMolmoLLM device: cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "2024-09-28 05:54:19,582 - chat-bot - INFO - /content/achatbot/src/core/llm/transformers/base.py:147 - _warmup - TransformersManualVisionMolmoLLM:  warmed up! time: 4.286 s\n",
            "2024-09-28 05:54:19,582 - chat-bot - INFO - /content/achatbot/src/core/llm/__init__.py:46 - initLLMEngine - initLLMEngine: llm_transformers_manual_vision_molmo, TAG:llm_transformers_manual_vision_molmo | TransformersManualVisionMolmoLLM\n",
            "2024-09-28 05:54:19,583 - asyncio - WARNING - /usr/lib/python3.10/asyncio/base_events.py:1904 - _run_once - Executing <Task pending name='Task-1' coro=<IsolatedAsyncioTestCase._asyncioLoopRunner() running at /usr/lib/python3.10/unittest/async_case.py:95> wait_for=<Future pending cb=[Task.task_wakeup()] created at /usr/lib/python3.10/asyncio/base_events.py:429> created at /usr/lib/python3.10/unittest/async_case.py:117> took 23.452 seconds\n",
            "2024-09-28 05:54:19,587 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - PipelineSource#0 ---> FrameLogger#0 Frame: StartFrame#0\n",
            "2024-09-28 05:54:19,587 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: StartFrame#0\n",
            "2024-09-28 05:54:19,587 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - PipelineSource#0 ---> FrameLogger#0 Frame: VisionImageRawFrame#0(text: 请描述下图片, size: (570, 380), format: JPEG, bytes_len:649800, mode:RGB\n",
            "2024-09-28 05:54:19,587 - chat-bot - INFO - /content/achatbot/src/processors/vision/vision_processor.py:45 - run_vision - Analyzing image: VisionImageRawFrame#0(text: 请描述下图片, size: (570, 380), format: JPEG, bytes_len:649800, mode:RGB\n",
            "2024-09-28 05:54:20,921 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#0(text:  The image captures a surreal and humorous scene on a New York City street.)\n",
            "2024-09-28 05:54:21,379 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#1(text:  A man is sitting on the back of a yellow SUV,)\n",
            "2024-09-28 05:54:21,766 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#2(text:  which is parked in the middle of the road.)\n",
            "2024-09-28 05:54:22,321 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#3(text:  He's using an ironing board and iron to press a blue shirt,)\n",
            "2024-09-28 05:54:22,805 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#4(text:  seemingly oblivious to the fact that he's in the middle of traffic.)\n",
            "2024-09-28 05:54:23,213 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#5(text: \n",
            "\n",
            "The SUV has a silver bumper and black wheels.)\n",
            "2024-09-28 05:54:23,796 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#6(text:  The man is dressed in a long-sleeved yellow shirt with black writing,)\n",
            "2024-09-28 05:54:24,051 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#7(text:  and he's wearing brown shoes.)\n",
            "2024-09-28 05:54:24,233 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#8(text:  He's leaning forward,)\n",
            "2024-09-28 05:54:24,560 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#9(text:  focused on ironing the shirt on the board.)\n",
            "2024-09-28 05:54:24,739 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#10(text: \n",
            "\n",
            "In the background,)\n",
            "2024-09-28 05:54:25,028 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#11(text:  there's a yellow taxi passing by,)\n",
            "2024-09-28 05:54:25,465 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#12(text:  and a tall gray building with pink banners hanging from it.)\n",
            "2024-09-28 05:54:26,045 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#13(text:  The street is paved with black asphalt and has a white line running down the middle.)\n",
            "2024-09-28 05:54:26,804 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#14(text: \n",
            "\n",
            "This unusual situation creates a striking contrast between the everyday act of ironing and the busy urban environment.)\n",
            "2024-09-28 05:54:27,494 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#15(text:  It's a perfect example of the unexpected and quirky moments that can be found in city life,)\n",
            "2024-09-28 05:54:27,868 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#16(text:  blending the mundane with the extraordinary in a single,)\n",
            "2024-09-28 05:54:27,983 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: TextFrame#17(text:  captivating image.)\n",
            "2024-09-28 05:54:27,983 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - PipelineSource#0 ---> FrameLogger#0 Frame: StopTaskFrame#0\n",
            "2024-09-28 05:54:27,983 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - VisionProcessor#0 ---> FrameLogger#1 Frame: StopTaskFrame#0\n",
            "2024-09-28 05:54:27,984 - asyncio - WARNING - /usr/lib/python3.10/asyncio/base_events.py:1904 - _run_once - Executing <Task pending name='Task-3' coro=<PipelineTask._process_down_queue() running at /usr/local/lib/python3.10/dist-packages/apipeline/pipeline/task.py:140> wait_for=<Task pending name='Task-2' coro=<PipelineTask._process_up_queue() running at /usr/local/lib/python3.10/dist-packages/apipeline/pipeline/task.py:145> wait_for=<Future cancelled created at /usr/lib/python3.10/asyncio/base_events.py:429> cb=[gather.<locals>._done_callback() at /usr/lib/python3.10/asyncio/tasks.py:720, Task.task_wakeup()] created at /usr/lib/python3.10/asyncio/tasks.py:337> cb=[gather.<locals>._done_callback() at /usr/lib/python3.10/asyncio/tasks.py:720] created at /usr/lib/python3.10/asyncio/tasks.py:337> took 8.397 seconds\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 31.864s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9FP2ecdF5M"
      },
      "source": [
        "> [!NOTE]\n",
        ">\n",
        "> need add DAILY_API_KEY on colab Secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg2ApmcbZ3Vc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "daily_api_key=userdata.get('DAILY_API_KEY')\n",
        "#print(daily_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPOa8RBb0dS"
      },
      "source": [
        "run bot task woker with bot.json, e.g.: daily_describe_transformers_vision_bot.json\n",
        "\n",
        "use daily room stream, u can click bot joined the room url e.g.: https://weedge.daily.co/chat-bot to start chat with bot with audio and camera stream\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKB146E-b9s_"
      },
      "outputs": [],
      "source": [
        "!DAILY_API_KEY=$daily_api_key python -m src.cmd.bots.main -f /content/daily_describe_transformers_vision_bot.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6EiEKxtbqVT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "REDIS_PASSWORD=userdata.get('REDIS_PASSWORD')\n",
        "#print(REDIS_PASSWORD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FzqIIMQ9boPX"
      },
      "outputs": [],
      "source": [
        "!LOG_LEVEL=debug REDIS_PASSWORD=$REDIS_PASSWORD python -m src.cmd.bots.main -f /content/task_bot.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4CjzTHSHQwRK"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO+8dHyp9auQFQhl6No8lvp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}