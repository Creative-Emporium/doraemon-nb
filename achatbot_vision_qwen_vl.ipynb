{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/achatbot_vision_qwen_vl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CjzTHSHQwRK"
      },
      "source": [
        "# transformers qwen2-vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6p2eLzV1O0o"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi && lscpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5hIFzPcCeu4"
      },
      "outputs": [],
      "source": [
        "!pip show torch torchvision torchaudio transformers autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2xPMMcm-313"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U autoawq torch torchvision torchaudio git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCSBF0uY_uAZ"
      },
      "outputs": [],
      "source": [
        "!pip show torch torchvision torchaudio transformers autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kstQzvzkr5Jb"
      },
      "outputs": [],
      "source": [
        "!pip install -q qwen-vl-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ydqSihTr38d"
      },
      "outputs": [],
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from transformers import TextIteratorStreamer\n",
        "import torch\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from time import perf_counter\n",
        "\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-Int8\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-Int4\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-7B-Instruct-AWQ\"\n",
        "\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-Int8\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-Int4\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct-AWQ\"\n",
        "model_ckpt_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "\n",
        "# default: Load the model on the available device(s)\n",
        "#model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "#    model_ckpt_name, torch_dtype=torch.float16, device_map=\"cuda\"\n",
        "#)\n",
        "\n",
        "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
        "# see detail: https://huggingface.co/docs/transformers/perf_infer_gpu_one\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "     model_ckpt_name,\n",
        "     torch_dtype=\"auto\",\n",
        "     #attn_implementation=\"sdpa\", #sdpa or flash_attention_2, no eager\n",
        "     #attn_implementation=\"flash_attention_2\", #sdpa or flash_attention_2, no eager\n",
        "     device_map=\"cuda\",\n",
        ")\n",
        "\n",
        "# default processer\n",
        "#processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
        "\n",
        "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
        "# You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
        "min_pixels = 256*28*28\n",
        "max_pixels = 1280*28*28\n",
        "processor = AutoProcessor.from_pretrained(model_ckpt_name, min_pixels=min_pixels, max_pixels=max_pixels)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"描述下图片内容\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "times=[]\n",
        "start_time = perf_counter()\n",
        "# Preparation for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "inputs = inputs.to(model.device)\n",
        "\n",
        "\n",
        "# Inference: Generation of the output with TextIteratorStreamer\n",
        "streamer = TextIteratorStreamer(processor, skip_prompt=True, skip_special_tokens=True)\n",
        "# Use Thread to run generation in background\n",
        "# Otherwise, the process is blocked until generation is complete\n",
        "# and no streaming effect can be observed.\n",
        "from threading import Thread\n",
        "generation_kwargs = dict(**inputs, streamer=streamer, max_new_tokens=128)\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "generated_text = \"\"\n",
        "for new_text in streamer:\n",
        "    #print(new_text)\n",
        "    times.append(perf_counter() - start_time)\n",
        "    generated_text += new_text\n",
        "    start_time = perf_counter()\n",
        "print(generated_text)\n",
        "\n",
        "print(\"times\",times)\n",
        "print(\"total cost:\",sum(times))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1V1ORJVRTWK"
      },
      "source": [
        "# achatbot vision lm qwen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyZ_1-dhRR-_"
      },
      "outputs": [],
      "source": [
        "!cd /content && rm -rf achatbot && git clone --recursive https://github.com/ai-bot-pro/achatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFvgRFMqRjlY"
      },
      "outputs": [],
      "source": [
        "%cd /content/achatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEpbiR_SRnwl"
      },
      "outputs": [],
      "source": [
        "!bash scripts/pypi_achatbot.sh dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV7eALxLR1kT"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"dist/achatbot-0.0.7.1-py3-none-any.whl[daily_room_audio_stream,sense_voice_asr,deepgram_asr_processor,llm_transformers_manual_vision,tts_edge,openai,queue]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# funasr > 1.1.6\n",
        "!pip show funasr"
      ],
      "metadata": {
        "id": "JKdU3WPptz7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2U1dZCWDEHc"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download FunAudioLLM/SenseVoiceSmall  --local-dir ./models/FunAudioLLM/SenseVoiceSmall --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSneSkUoWF8Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download Qwen/Qwen2-VL-7B-Instruct --local-dir ./models/Qwen/Qwen2-VL-7B-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzNZFynaWVMy"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download Qwen/Qwen2-VL-2B-Instruct --local-dir ./models/Qwen/Qwen2-VL-2B-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download unsloth/Llama-3.2-11B-Vision-Instruct --local-dir ./models/unsloth/Llama-3.2-11B-Vision-Instruct --local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "0lK8Af_Vs_Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT3Fd_HZTEku"
      },
      "outputs": [],
      "source": [
        "# cpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVP11g3FzCiF"
      },
      "outputs": [],
      "source": [
        "# gpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" LLM_DEVICE=cuda \\\n",
        "    VIDEO_FILE=\"/content/capture3_2024-09-15_20-10-27.mp4\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf4D2_cv7XK_"
      },
      "outputs": [],
      "source": [
        "# gpu\n",
        "!LLM_MODEL_NAME_OR_PATH=\"./models/Qwen/Qwen2-VL-2B-Instruct\" LLM_DEVICE=cuda LLM_CHAT_HISTORY_SIZE=0 \\\n",
        "    VIDEO_FILE=\"/content/capture3_2024-09-15_20-10-27.mp4\" \\\n",
        "    python -m unittest test.core.llm.test_transformers_v_qwen.TestTransformersVQwen.test_chat_completion_prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9FP2ecdF5M"
      },
      "source": [
        "> [!NOTE]\n",
        ">\n",
        "> need add DAILY_API_KEY on colab Secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg2ApmcbZ3Vc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "daily_api_key=userdata.get('DAILY_API_KEY')\n",
        "#print(daily_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPOa8RBb0dS"
      },
      "source": [
        "run bot task woker with bot.json, e.g.: daily_describe_transformers_vision_bot.json\n",
        "\n",
        "use daily room stream, u can click bot joined the room url e.g.: https://weedge.daily.co/chat-bot to start chat with bot with audio and camera stream\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKB146E-b9s_"
      },
      "outputs": [],
      "source": [
        "!DAILY_API_KEY=$daily_api_key python -m src.cmd.bots.main -f /content/daily_describe_transformers_vision_bot.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6EiEKxtbqVT"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "REDIS_PASSWORD=userdata.get('REDIS_PASSWORD')\n",
        "#print(REDIS_PASSWORD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FzqIIMQ9boPX"
      },
      "outputs": [],
      "source": [
        "!LOG_LEVEL=debug REDIS_PASSWORD=$REDIS_PASSWORD python -m src.cmd.bots.main -f /content/task_bot.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4CjzTHSHQwRK"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNnXfIwA7ynSIqu53CJYfic",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}