{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/achatbot_freeze_omni_voice_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9hXsk51gsUa"
      },
      "source": [
        "# install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGsomgs1KFCD"
      },
      "outputs": [],
      "source": [
        "!cd /content && rm -rf achatbot && git clone --recursive https://github.com/ai-bot-pro/achatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kn5eFEj2NuJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72038138-2da8-46fc-f184-22cea2c46b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/achatbot\n"
          ]
        }
      ],
      "source": [
        "%cd /content/achatbot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin feat/voice-freeze-omni"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-Al92y7xCOj",
        "outputId": "f850d844-b29d-41b6-ed6d-8e3ed457a97a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From https://github.com/ai-bot-pro/achatbot\n",
            " * branch            feat/voice-freeze-omni -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9dyrdw-Nx5y"
      },
      "outputs": [],
      "source": [
        "!bash scripts/pypi_achatbot.sh dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIIQ6ULyN3z_"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"dist/achatbot-0.0.8.3-py3-none-any.whl[fastapi_bot_server,daily_room_audio_stream,livekit_room_audio_stream,agora_channel_audio_stream,silero_vad_analyzer,freeze_omni_voice_processor,queue]\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4jLuMQhgvO7"
      },
      "source": [
        "# download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bz6H_bMmjRFr"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "#NGROK_TOKEN=userdata.get('NGROK_TOKEN')\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCGnjWUvitof"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token $HF_TOKEN --add-to-git-credential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eEuMHNfjWL5"
      },
      "outputs": [],
      "source": [
        "# FreezeOmni ckpt\n",
        "# - audiollm: encoder and adapter\n",
        "# - decoder: NAR decoder, AR decoder and codec decoder\n",
        "!huggingface-cli download VITA-MLLM/Freeze-Omni --local-dir /content/ckpts/VITA-MLLM/Freeze-Omni --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHy8iA09-Tox"
      },
      "outputs": [],
      "source": [
        "# textual llm Qwen2-7B-Instruct ckpt\n",
        "!huggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir /content/ckpts/Qwen/Qwen2-7B-Instruct --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCq_XlOhgytH"
      },
      "source": [
        "# run webrtc bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sPhYUWKpk5dD"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "DAILY_API_KEY=userdata.get('DAILY_API_KEY')\n",
        "\n",
        "LIVEKIT_URL=userdata.get('LIVEKIT_URL')\n",
        "LIVEKIT_API_KEY=userdata.get('LIVEKIT_API_KEY')\n",
        "LIVEKIT_API_SECRET=userdata.get('LIVEKIT_API_SECRET')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/achatbot"
      ],
      "metadata": {
        "id": "lINDQs9m5uRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073cca77-aaa1-4d67-b66e-5c1059b129e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/achatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh5T_mbBk0Qa",
        "outputId": "b18507ec-4333-4736-d57b-27de13f4dc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /content/daily_freeze_omni_voice_bot.json: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cat /content/daily_freeze_omni_voice_bot.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wkHdGFOzlAV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c69d12f-d831-44ac-fbd0-4f33189eef3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"chat_bot_name\": \"DailyFreezeOmniVoiceBot\",\n",
            "    \"config\": {\n",
            "        \"vad\": {\n",
            "            \"args\": {\n",
            "                \"stop_secs\": 0.7\n",
            "            },\n",
            "            \"tag\": \"silero_vad_analyzer\"\n",
            "        },\n",
            "        \"voice_llm\": {\n",
            "            \"args\": {\n",
            "                \"args\": {\n",
            "                    \"llm_path\": \"/content/ckpts/Qwen/Qwen2-7B-Instruct\",\n",
            "                    \"model_path\": \"/content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints\"\n",
            "                }\n",
            "            },\n",
            "            \"tag\": \"glm_voice_processor\"\n",
            "        }\n",
            "    },\n",
            "    \"config_list\": [],\n",
            "    \"room_manager\": {\n",
            "        \"args\": {\n",
            "            \"privacy\": \"public\"\n",
            "        },\n",
            "        \"tag\": \"daily_room\"\n",
            "    },\n",
            "    \"room_name\": \"chat-room\",\n",
            "    \"room_url\": \"\",\n",
            "    \"services\": {\n",
            "        \"pipeline\": \"achatbot\",\n",
            "        \"vad\": \"silero\",\n",
            "        \"voice_llm\": \"freeze_omni\"\n",
            "    },\n",
            "    \"token\": \"\"\n",
            "}\n",
            "2024-12-19 08:11:12,506 - chat-bot - INFO - /content/achatbot/src/cmd/bots/main.py:63 - <module> - bot_config:{'chat_bot_name': 'DailyFreezeOmniVoiceBot', 'room_name': 'chat-room', 'room_url': '', 'token': '', 'room_manager': {'tag': 'daily_room', 'args': {'privacy': 'public'}}, 'services': {'pipeline': 'achatbot', 'vad': 'silero', 'voice_llm': 'freeze_omni'}, 'config': {'vad': {'tag': 'silero_vad_analyzer', 'args': {'stop_secs': 0.7}}, 'voice_llm': {'tag': 'glm_voice_processor', 'args': {'args': {'llm_path': '/content/ckpts/Qwen/Qwen2-7B-Instruct', 'model_path': '/content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints'}}}}, 'config_list': []}\n",
            "2024-12-19 08:11:12,506 - chat-bot - INFO - /content/achatbot/src/cmd/bots/run.py:33 - __init__ - run_bot_info: is_agent=False chat_bot_name='DailyFreezeOmniVoiceBot' config={'vad': {'tag': 'silero_vad_analyzer', 'args': {'stop_secs': 0.7}}, 'voice_llm': {'tag': 'glm_voice_processor', 'args': {'args': {'llm_path': '/content/ckpts/Qwen/Qwen2-7B-Instruct', 'model_path': '/content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints'}}}} room_name='chat-room' room_url='' token='' config_list=[] services={'pipeline': 'achatbot', 'vad': 'silero', 'voice_llm': 'freeze_omni'} websocket_server_host='localhost' websocket_server_port=8765 transport_type='room' handle_sigint=True task_connector=None room_manager=EngineClassInfo(tag='daily_room', args={'privacy': 'public'})\n",
            "2024-12-19 08:11:12,631 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use daily_room engine\n",
            "2024-12-19 08:11:12,631 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.services.help.daily_room.DailyRoom'> args: {'privacy': 'public'}\n",
            "2024-12-19 08:11:12,631 - chat-bot - INFO - /content/achatbot/src/services/help/__init__.py:37 - initEngine - initEngine: daily_room, TAG:daily_room | DailyRoom\n",
            "2024-12-19 08:11:14,325 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base.py:76 - init_bot_config - ai bot_config: vad=VADConfig(tag='silero_vad_analyzer', args={'stop_secs': 0.7}) asr=None llm=LLMConfig(base_url=None, model=None, language=None, messages=None, tools=None, tag=None, args=None) nlp_task_llm=None voice_llm=LLMConfig(base_url=None, model=None, language=None, messages=None, tools=None, tag='glm_voice_processor', args={'args': {'llm_path': '/content/ckpts/Qwen/Qwen2-7B-Instruct', 'model_path': '/content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints'}}) vision_llm=None vision_detector=None vision_ocr=None tts=None img_gen=None extends=None\n",
            "2024-12-19 08:11:15,657 - chat-bot - INFO - /content/achatbot/src/common/factory.py:69 - get_engine_by_tag - use silero_vad_analyzer engine\n",
            "2024-12-19 08:11:15,658 - chat-bot - INFO - /content/achatbot/src/common/factory.py:34 - get_instance - class: <class 'src.modules.speech.vad_analyzer.silero.SileroVADAnalyzer'> args: {'sample_rate': 16000, 'num_channels': 1, 'confidence': 0.7, 'start_secs': 0.2, 'stop_secs': 0.8, 'min_volume': 0.6, 'repo_or_dir': 'snakers4/silero-vad', 'model': 'silero_vad', 'source': 'github', 'force_reload': False, 'trust_repo': True, 'verbose': True, 'onnx': False, 'silero_sensitivity': 0.4, 'is_pad_tensor': True, 'check_frames_mode': 1}\n",
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "2024-12-19 08:11:16,269 - chat-bot - INFO - /content/achatbot/src/modules/speech/vad_analyzer/__init__.py:59 - initVADAnalyzerEngine - initVADEngine: silero_vad_analyzer, TAG:silero_vad_analyzer | SileroVADAnalyzer\n",
            "2024-12-19 08:11:16,766 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:124 - load_models - loading model weights\n",
            "the number of speech encoder params: 341.3681640625M\n",
            "Loading checkpoint shards: 100% 4/4 [00:00<00:00, 12.65it/s]\n",
            "the number of audio llm params: 7621.362796783447M\n",
            "Checkpoint: loading from checkpoint /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/audiollm/final.pt for GPU\n",
            "reading a config file from /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/decoder/model.json\n",
            "the number of LLM2TTSCodecAR(NAR decoder and AR decoder(llama transformer blocks)) params: 157.08826065063477M\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "the number of vq-vae(llm2tts) params: 60.44925117492676M\n",
            "Removing weight norm...\n",
            "Removing weight norm...\n",
            "after remove_weight_norm, the number of llm2TTS(vq-vae codec decoder model) params: 60.422181129455566M\n",
            "2024-12-19 08:11:30,006 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:137 - load_models - model weights loaded\n",
            "2024-12-19 08:11:30,006 - chat-bot - INFO - /content/achatbot/src/transports/daily.py:52 - __init__ - DailyTransport register event names: dict_keys(['on_joined', 'on_left', 'on_app_message', 'on_call_state_updated', 'on_dialin_ready', 'on_dialout_answered', 'on_dialout_connected', 'on_dialout_stopped', 'on_dialout_error', 'on_dialout_warning', 'on_first_participant_joined', 'on_participant_joined', 'on_participant_left', 'on_participant_updated'])\n",
            "2024-12-19 08:11:30,011 - chat-bot - INFO - /content/achatbot/src/services/daily_client.py:174 - join - Joining https://weedge.daily.co/chat-room\n",
            "/content/achatbot/src/cmd/bots/../../../deps/FreezeOmni/models/audioLLM.py:431: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  prob = F.softmax(state_logits[:, :-1])\n",
            "State 1 prob: 0.6225, State 2 prob: 0.0000\n",
            "ClassObject <class 'deps.FreezeOmni.models.decoder.llm2tts.llm2TTS'> 0 use count: 1\n",
            "ClassObject <class 'deps.FreezeOmni.models.pipeline.inferencePipeline'> 0 use count: 1\n",
            "2024-12-19 08:11:31,305 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:161 - start - start done\n",
            "2024-12-19 08:11:31,305 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base_daily.py:34 - on_call_state_updated - Call state joining \n",
            "2024-12-19 08:11:31,306 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base_daily.py:34 - on_call_state_updated - Call state joined \n",
            "2024-12-19 08:11:31,461 - chat-bot - INFO - /content/achatbot/src/services/daily_client.py:193 - join - Joined https://weedge.daily.co/chat-room\n",
            "2024-12-19 08:11:45,186 - chat-bot - INFO - /content/achatbot/src/services/daily_client.py:386 - on_participant_joined - Participant joined 0a4f2b8f-46b9-463a-8068-038a593f067f\n",
            "2024-12-19 08:11:45,186 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base_daily.py:17 - on_first_participant_joined - First participant 0a4f2b8f-46b9-463a-8068-038a593f067f joined\n",
            "2024-12-19 08:11:50,504 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - UserAudioResponseAggregator#2 ---> FrameLogger#5 user audio aggr ==>: AudioRawFrame#250(size: 38400, frames: 320, sample_rate: 16000,sample_width: 2, channels: 1)\n",
            "/content/achatbot/deps/FreezeOmni/bin/inference.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample_data = torch.tensor(audio).reshape(1, -1, 1)[:, :, :1] * 32768\n",
            "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
            "/content/achatbot/src/cmd/bots/../../../deps/FreezeOmni/models/audioLLM.py:431: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  prob = F.softmax(state_logits[:, :-1])\n",
            "State 1 prob: 0.0009, State 2 prob: 0.0006\n",
            "2024-12-19 08:11:50,750 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.0029, State 2 prob: 0.0000\n",
            "2024-12-19 08:11:50,846 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.0953, State 2 prob: 0.0004\n",
            "2024-12-19 08:11:50,940 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.2125, State 2 prob: 0.0027\n",
            "2024-12-19 08:11:51,035 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.6383, State 2 prob: 0.0008\n",
            "2024-12-19 08:11:51,131 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: ss\n",
            "2024-12-19 08:11:51,131 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:230 - run_voice - outputs keys: dict_keys(['past_key_values', 'stat', 'last_id', 'adapter_cache', 'encoder_cache', 'pe_index'])\n",
            "2024-12-19 08:11:51,360 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 你好！\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "2024-12-19 08:11:51,977 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 28800]),push audio len:57600\n",
            "2024-12-19 08:11:51,977 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#252(size: 57600, frames: 28800, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:11:51,978 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#0(text: 你好！)\n",
            "2024-12-19 08:11:52,365 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 有什么我可以帮助你的吗？\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:11:53,253 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 47076]),push audio len:94152\n",
            "2024-12-19 08:11:53,253 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#253(size: 94152, frames: 47076, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "2024-12-19 08:11:53,410 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 8124]),push audio len:16248\n",
            "2024-12-19 08:11:53,410 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#254(size: 16248, frames: 8124, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:11:53,411 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#1(text: 有什么我可以帮助你的吗？)\n",
            "2024-12-19 08:12:00,581 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - UserAudioResponseAggregator#2 ---> FrameLogger#5 user audio aggr ==>: AudioRawFrame#606(size: 55680, frames: 320, sample_rate: 16000,sample_width: 2, channels: 1)\n",
            "/content/achatbot/deps/FreezeOmni/bin/inference.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample_data = torch.tensor(audio).reshape(1, -1, 1)[:, :, :1] * 32768\n",
            "/content/achatbot/src/cmd/bots/../../../deps/FreezeOmni/models/audioLLM.py:431: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  prob = F.softmax(state_logits[:, :-1])\n",
            "State 1 prob: 0.0001, State 2 prob: 0.0000\n",
            "2024-12-19 08:12:00,678 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.0068, State 2 prob: 0.0000\n",
            "2024-12-19 08:12:00,771 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.0135, State 2 prob: 0.0000\n",
            "2024-12-19 08:12:00,865 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.0059, State 2 prob: 0.0000\n",
            "2024-12-19 08:12:00,958 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.0261, State 2 prob: 0.0000\n",
            "2024-12-19 08:12:01,052 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.1582, State 2 prob: 0.0000\n",
            "2024-12-19 08:12:01,146 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: cl\n",
            "State 1 prob: 0.8777, State 2 prob: 0.0000\n",
            "2024-12-19 08:12:01,240 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:222 - run_voice - speech_dialogue self._outputs stat: ss\n",
            "2024-12-19 08:12:01,241 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:230 - run_voice - outputs keys: dict_keys(['past_key_values', 'stat', 'last_id', 'adapter_cache', 'encoder_cache', 'pe_index'])\n",
            "2024-12-19 08:12:01,961 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 当然，我来给您讲一个有趣的故事吧。\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "2024-12-19 08:12:02,308 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 22578]),push audio len:45156\n",
            "2024-12-19 08:12:02,308 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#608(size: 45156, frames: 22578, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:02,959 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 51822]),push audio len:103644\n",
            "2024-12-19 08:12:02,959 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#609(size: 103644, frames: 51822, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:12:02,959 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#2(text: 当然，我来给您讲一个有趣的故事吧。)\n",
            "2024-12-19 08:12:03,545 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 这个故事叫做《乌鸦喝水》。\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:04,349 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 58200]),push audio len:116400\n",
            "2024-12-19 08:12:04,349 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#610(size: 116400, frames: 58200, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:12:04,350 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#3(text: 这个故事叫做《乌鸦喝水》。\n",
            "\n",
            ")\n",
            "2024-12-19 08:12:05,804 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 从前，有一只乌鸦，它口渴了，飞到一个装有水的瓶子旁边。\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "2024-12-19 08:12:06,143 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 23855]),push audio len:47710\n",
            "2024-12-19 08:12:06,144 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#611(size: 47710, frames: 23855, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "2024-12-19 08:12:06,446 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 21782]),push audio len:43564\n",
            "2024-12-19 08:12:06,446 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#612(size: 43564, frames: 21782, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:07,259 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 63512]),push audio len:127024\n",
            "2024-12-19 08:12:07,259 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#613(size: 127024, frames: 63512, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "2024-12-19 08:12:07,513 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 27651]),push audio len:55302\n",
            "2024-12-19 08:12:07,514 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#614(size: 55302, frames: 27651, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:12:07,514 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#4(text: 从前，有一只乌鸦，它口渴了，飞到一个装有水的瓶子旁边。)\n",
            "2024-12-19 08:12:08,568 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 但是，瓶子的水位很高，乌鸦的嘴够不到水。\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "2024-12-19 08:12:08,907 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 20733]),push audio len:41466\n",
            "2024-12-19 08:12:08,907 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#615(size: 41466, frames: 20733, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:09,444 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 41769]),push audio len:83538\n",
            "2024-12-19 08:12:09,444 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#616(size: 83538, frames: 41769, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:09,913 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 44298]),push audio len:88596\n",
            "2024-12-19 08:12:09,913 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#617(size: 88596, frames: 44298, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:12:09,914 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#5(text: 但是，瓶子的水位很高，乌鸦的嘴够不到水。)\n",
            "2024-12-19 08:12:11,388 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 乌鸦想了想，然后捡起地上的小石子，一颗接一颗地丢进瓶子里。\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:13,166 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 135000]),push audio len:270000\n",
            "2024-12-19 08:12:13,166 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#618(size: 270000, frames: 135000, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:12:13,166 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#6(text: 乌鸦想了想，然后捡起地上的小石子，一颗接一颗地丢进瓶子里。)\n",
            "2024-12-19 08:12:14,520 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 随着石子的加入，水位逐渐上升，最终乌鸦成功地喝到了水。\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:15,134 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 37154]),push audio len:74308\n",
            "2024-12-19 08:12:15,134 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#619(size: 74308, frames: 37154, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "2024-12-19 08:12:15,444 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 32823]),push audio len:65646\n",
            "2024-12-19 08:12:15,444 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#620(size: 65646, frames: 32823, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:16,226 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 62023]),push audio len:124046\n",
            "2024-12-19 08:12:16,226 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#621(size: 124046, frames: 62023, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:12:16,226 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#7(text: 随着石子的加入，水位逐渐上升，最终乌鸦成功地喝到了水。\n",
            "\n",
            ")\n",
            "2024-12-19 08:12:17,858 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:321 - decoder - Synthesis: 这个故事告诉我们，面对困难时，要善于动脑筋，寻找解决问题的方法，而不仅仅是抱怨或放弃。\n",
            "Starting TTS...\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:18,491 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 40897]),push audio len:81794\n",
            "2024-12-19 08:12:18,491 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#622(size: 81794, frames: 40897, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "2024-12-19 08:12:18,763 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 28926]),push audio len:57852\n",
            "2024-12-19 08:12:18,763 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#623(size: 57852, frames: 28926, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:19,602 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 73697]),push audio len:147394\n",
            "2024-12-19 08:12:19,602 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#624(size: 147394, frames: 73697, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "Codec Done\n",
            "Codec Done\n",
            "Codec Done\n",
            "2024-12-19 08:12:20,309 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:282 - run_voice - seg tensor:torch.Size([1, 1, 56880]),push audio len:113760\n",
            "2024-12-19 08:12:20,309 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: AudioRawFrame#625(size: 113760, frames: 56880, sample_rate: 24000,sample_width: 2, channels: 1)\n",
            "2024-12-19 08:12:20,309 - chat-bot - INFO - /usr/local/lib/python3.10/dist-packages/apipeline/processors/logger.py:34 - process_frame - FreezeOmniVoiceProcessor#0 ---> FrameLogger#6 bot audio speak ==>: TextFrame#8(text: 这个故事告诉我们，面对困难时，要善于动脑筋，寻找解决问题的方法，而不仅仅是抱怨或放弃。)\n",
            "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
            "KeyboardInterrupt\n",
            "2024-12-19 08:13:01,353 - chat-bot - WARNING - /usr/local/lib/python3.10/dist-packages/apipeline/pipeline/runner.py:53 - _sig_handler - Interruption detected. Canceling runner PipelineRunner#0\n",
            "ClassObject <class 'deps.FreezeOmni.models.decoder.llm2tts.llm2TTS'> 0 use count: 0\n",
            "ClassObject <class 'deps.FreezeOmni.models.pipeline.inferencePipeline'> 0 use count: 0\n",
            "2024-12-19 08:13:01,353 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:181 - cancel - cancel done\n",
            "2024-12-19 08:13:01,353 - chat-bot - WARNING - /usr/local/lib/python3.10/dist-packages/apipeline/pipeline/runner.py:53 - _sig_handler - Interruption detected. Canceling runner PipelineRunner#0\n",
            "2024-12-19 08:13:01,353 - chat-bot - INFO - /content/achatbot/src/services/daily_client.py:270 - leave - Leaving https://weedge.daily.co/chat-room\n",
            "ClassObject <class 'deps.FreezeOmni.models.decoder.llm2tts.llm2TTS'> 0 use count: 0\n",
            "ClassObject <class 'deps.FreezeOmni.models.pipeline.inferencePipeline'> 0 use count: 0\n",
            "2024-12-19 08:13:01,354 - chat-bot - INFO - /content/achatbot/src/processors/voice/freeze_omni_voice_processor.py:181 - cancel - cancel done\n",
            "2024-12-19 08:13:01,355 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base_daily.py:34 - on_call_state_updated - Call state leaving \n",
            "2024-12-19 08:13:01,372 - chat-bot - INFO - /content/achatbot/src/cmd/bots/base_daily.py:34 - on_call_state_updated - Call state left \n",
            "2024-12-19 08:13:06,360 - chat-bot - ERROR - /content/achatbot/src/common/task_manager/multiprocessing_task_manager.py:34 - cleanup - Error while cleaning up process 11464: Cannot close a process while it is still running. You should first call join() or terminate().\n",
            "2024-12-19 08:13:06,360 - chat-bot - WARNING - /content/achatbot/src/common/task_manager/multiprocessing_task_manager.py:37 - cleanup - pid:11464 tag:chat-room proc: <Process name='DailyFreezeOmniVoiceBot' pid=11464 parent=11432 started> killed\n"
          ]
        }
      ],
      "source": [
        "!DAILY_API_KEY=$DAILY_API_KEY \\\n",
        "  python -m src.cmd.bots.main -f /content/daily_freeze_omni_voice_bot.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio(filename=\"/content/achatbot/records/bot_speak_2024-12-19_04-51-40.318.wav\")\n"
      ],
      "metadata": {
        "id": "Kx0KubHzKvpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# inference"
      ],
      "metadata": {
        "id": "QGuG_6G3fVFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/achatbot/deps/FreezeOmni/ \\\n",
        "  && PYTHONPATH=./:$PYTHONPATH CUDA_VISIBLE_DEVICES=0 python bin/inference.py \\\n",
        "    --model_path /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints \\\n",
        "    --input_wav /content/hi.wav \\\n",
        "    --output_wav /content/answer_hi.wav \\\n",
        "    --llm_path /content/ckpts/Qwen/Qwen2-7B-Instruct \\\n",
        "    --top_p 0.8 \\\n",
        "    --top_k 20 \\\n",
        "    --temperature 0.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEB9Nmmve4e4",
        "outputId": "5ae4036a-09e2-409b-eba4-5cfc291e710e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-12-19 04:06:06.420] Namespace(model_path='/content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints', llm_path='/content/ckpts/Qwen/Qwen2-7B-Instruct', top_k=20, top_p=0.8, temperature=0.8, input_wav='/content/hi.wav', output_wav='/content/answer_hi.wav')\n",
            "[2024-12-19 04:06:09.289] the number of speech encoder params: 341.3681640625M\n",
            "Loading checkpoint shards: 100% 4/4 [00:00<00:00, 14.88it/s]\n",
            "[2024-12-19 04:06:10.170] the number of audio llm params: 7621.362796783447M\n",
            "[2024-12-19 04:06:10.197] Checkpoint: loading from checkpoint /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/audiollm/final.pt for GPU\n",
            "[2024-12-19 04:06:15.638] reading a config file from /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/decoder/model.json\n",
            "[2024-12-19 04:06:17.692] the number of LLM2TTSCodecAR(NAR decoder and AR decoder(llama transformer blocks)) params: 157.08826065063477M\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "[2024-12-19 04:06:18.967] the number of vq-vae(llm2tts) params: 60.44925117492676M\n",
            "[2024-12-19 04:06:19.045] Removing weight norm...\n",
            "[2024-12-19 04:06:19.049] Removing weight norm...\n",
            "[2024-12-19 04:06:19.054] after remove_weight_norm, the number of llm2TTS(vq-vae codec decoder model) params: 60.422181129455566M\n",
            "[2024-12-19 04:06:19.061] ---> torch.Size([19200])\n",
            "/content/achatbot/deps/FreezeOmni/models/audioLLM.py:431: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  prob = F.softmax(state_logits[:, :-1])\n",
            "[2024-12-19 04:06:20.412] State 1 prob: 0.9872, State 2 prob: 0.0000\n",
            "[2024-12-19 04:06:20.416] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "/content/achatbot/deps/FreezeOmni/bin/inference.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample_data = torch.tensor(audio).reshape(1, -1, 1)[:, :, :1] * 32768\n",
            "[2024-12-19 04:06:20.417] torch.Size([1, 2560, 1])\n",
            "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
            "[2024-12-19 04:06:20.644] State 1 prob: 0.0002, State 2 prob: 0.0001\n",
            "[2024-12-19 04:06:20.645] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:20.645] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "[2024-12-19 04:06:20.645] torch.Size([1, 2560, 1])\n",
            "[2024-12-19 04:06:20.738] State 1 prob: 0.0009, State 2 prob: 0.0001\n",
            "[2024-12-19 04:06:20.739] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:20.739] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "[2024-12-19 04:06:20.739] torch.Size([1, 2560, 1])\n",
            "[2024-12-19 04:06:20.831] State 1 prob: 0.0255, State 2 prob: 0.0404\n",
            "[2024-12-19 04:06:20.831] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:20.831] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "[2024-12-19 04:06:20.831] torch.Size([1, 2560, 1])\n",
            "[2024-12-19 04:06:20.924] State 1 prob: 0.0038, State 2 prob: 0.0181\n",
            "[2024-12-19 04:06:20.924] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:20.925] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "[2024-12-19 04:06:20.925] torch.Size([1, 2560, 1])\n",
            "[2024-12-19 04:06:21.017] State 1 prob: 0.0102, State 2 prob: 0.0007\n",
            "[2024-12-19 04:06:21.017] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:21.017] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "[2024-12-19 04:06:21.017] torch.Size([1, 2560, 1])\n",
            "[2024-12-19 04:06:21.110] State 1 prob: 0.1418, State 2 prob: 0.0530\n",
            "[2024-12-19 04:06:21.110] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:21.110] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "[2024-12-19 04:06:21.110] torch.Size([1, 2560, 1])\n",
            "[2024-12-19 04:06:21.202] State 1 prob: 0.0101, State 2 prob: 0.0067\n",
            "[2024-12-19 04:06:21.202] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:21.202] ---> torch.Size([20480]) torch.Size([19200]) torch.Size([2560])\n",
            "[2024-12-19 04:06:21.203] torch.Size([1, 2560, 1])\n",
            "[2024-12-19 04:06:21.294] State 1 prob: 0.3104, State 2 prob: 0.3153\n",
            "[2024-12-19 04:06:21.295] speech_dialogue outputs stat: cl\n",
            "[2024-12-19 04:06:21.295] listen dict_keys(['past_key_values', 'stat', 'last_id', 'adapter_cache', 'encoder_cache', 'pe_index'])\n",
            "[2024-12-19 04:06:21.519] Synthesis:  ['你好！']\n",
            "[2024-12-19 04:06:21.519] Starting TTS...\n",
            "[2024-12-19 04:06:22.100] Codec Done\n",
            "[2024-12-19 04:06:22.181] Codec Done\n",
            "[2024-12-19 04:06:22.313] cur_text:你好！\n",
            "[2024-12-19 04:06:22.699] Synthesis:  ['有什么我可以帮助你的吗？']\n",
            "[2024-12-19 04:06:22.700] Starting TTS...\n",
            "[2024-12-19 04:06:22.992] Codec Done\n",
            "[2024-12-19 04:06:23.332] Codec Done\n",
            "[2024-12-19 04:06:23.349] Codec Done\n",
            "[2024-12-19 04:06:23.480] cur_text:有什么我可以帮助你的吗？\n",
            "[2024-12-19 04:06:23.552] 你好！有什么我可以帮助你的吗？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# server"
      ],
      "metadata": {
        "id": "wTJ0fODZfLpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "NGROK_TOKEN=userdata.get('NGROK_TOKEN')\n",
        "#HF_TOKEN=userdata.get('HF_TOKEN')\n"
      ],
      "metadata": {
        "id": "XhaLHOvIiLKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flask_socketio pyngrok"
      ],
      "metadata": {
        "id": "bEMzUEAPwl8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken $NGROK_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x8tRIZkiU9-",
        "outputId": "e0a0d54e-062e-440b-c60d-e43c1426f507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/achatbot/deps/FreezeOmni/ \\\n",
        "  && PYTHONPATH=./:$PYTHONPATH PYTHONDONTWRITEBYTECODE=1 CUDA_VISIBLE_DEVICES=0 python bin/server.py \\\n",
        "    --ip 127.0.0.1 \\\n",
        "    --port 8000 \\\n",
        "    --max_users 3 \\\n",
        "    --llm_exec_nums 1 \\\n",
        "    --timeout 180 \\\n",
        "    --model_path /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints \\\n",
        "    --llm_path /content/ckpts/Qwen/Qwen2-7B-Instruct \\\n",
        "    --top_p 0.8 \\\n",
        "    --top_k 20 \\\n",
        "    --temperature 0.8 \\\n",
        "    --ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqnnjWcvgsnD",
        "outputId": "9d221820-599c-447f-d183-06384f27971d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(model_path='/content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints', llm_path='/content/ckpts/Qwen/Qwen2-7B-Instruct', top_k=20, top_p=0.8, temperature=0.8, ip='127.0.0.1', port='8000', max_users=3, llm_exec_nums=1, timeout=180, ngrok=True, ssl=False)\n",
            "[2024-12-19 03:06:10.872] the number of speech encoder params: 341.3681640625M\n",
            "Loading checkpoint shards: 100% 4/4 [00:00<00:00, 11.21it/s]\n",
            "[2024-12-19 03:06:11.954] the number of audio llm params: 7621.362796783447M\n",
            "[2024-12-19 03:06:11.981] Checkpoint: loading from checkpoint /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/audiollm/final.pt for GPU\n",
            "[2024-12-19 03:06:17.588] reading a config file from /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/decoder/model.json\n",
            "[2024-12-19 03:06:17.589] reading a config file from /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/decoder/model.json\n",
            "[2024-12-19 03:06:17.590] reading a config file from /content/ckpts/VITA-MLLM/Freeze-Omni/checkpoints/decoder/model.json\n",
            "[2024-12-19 03:06:23.026] the number of LLM2TTSCodecAR(NAR decoder and AR decoder(llama transformer blocks)) params: 157.08826065063477M\n",
            "[2024-12-19 03:06:23.030] the number of LLM2TTSCodecAR(NAR decoder and AR decoder(llama transformer blocks)) params: 157.08826065063477M\n",
            "[2024-12-19 03:06:23.035] the number of LLM2TTSCodecAR(NAR decoder and AR decoder(llama transformer blocks)) params: 157.08826065063477M\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "[2024-12-19 03:06:26.222] the number of vq-vae(llm2tts) params: 60.44925117492676M\n",
            "[2024-12-19 03:06:26.312] Removing weight norm...\n",
            "[2024-12-19 03:06:26.347] Removing weight norm...\n",
            "[2024-12-19 03:06:26.408] the number of vq-vae(llm2tts) params: 60.44925117492676M\n",
            "[2024-12-19 03:06:26.413] the number of vq-vae(llm2tts) params: 60.44925117492676M\n",
            "[2024-12-19 03:06:26.429] after remove_weight_norm, the number of llm2TTS(vq-vae codec decoder model) params: 60.422181129455566M\n",
            "[2024-12-19 03:06:26.558] Removing weight norm...\n",
            "[2024-12-19 03:06:26.558] Removing weight norm...\n",
            "[2024-12-19 03:06:26.569] Removing weight norm...\n",
            "[2024-12-19 03:06:26.569] Removing weight norm...\n",
            "[2024-12-19 03:06:26.581] after remove_weight_norm, the number of llm2TTS(vq-vae codec decoder model) params: 60.422181129455566M\n",
            "[2024-12-19 03:06:26.583] after remove_weight_norm, the number of llm2TTS(vq-vae codec decoder model) params: 60.422181129455566M\n",
            "[2024-12-19 03:06:26.624] Start Freeze-Omni sever\n",
            "[2024-12-19 03:06:27.564] Public URL: https://a113-34-126-116-155.ngrok-free.app\n",
            " * Serving Flask app 'server'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:8000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "127.0.0.1 - - [19/Dec/2024 03:06:37] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [19/Dec/2024 03:06:38] \"GET /socket.io/?EIO=4&transport=polling&t=qq5zwi84 HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [19/Dec/2024 03:06:38] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [19/Dec/2024 03:06:40] \"GET /socket.io/?EIO=4&transport=polling&t=qq6eqf1b&sid=8k1AmEMkHwHsBgcQAAAA HTTP/1.1\" 200 -\n",
            "/content/achatbot/deps/FreezeOmni/models/audioLLM.py:431: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  prob = F.softmax(state_logits[:, :-1])\n",
            "[2024-12-19 03:06:40.724] State 1 prob: 0.6365, State 2 prob: 0.0009\n",
            "[2024-12-19 03:06:40.837] Sid:  TXtbIyUqp2e1KcvUAAAB  Start listening\n",
            "[2024-12-19 03:06:40.837] TTS Object 0 is in use: True\n",
            "[2024-12-19 03:06:40.838] TTS Object 1 is in use: False\n",
            "[2024-12-19 03:06:40.838] TTS Object 2 is in use: False\n",
            "[2024-12-19 03:06:40.838] Pipeline Object 0 user count: 1\n",
            "[2024-12-19 03:06:40.838] User TXtbIyUqp2e1KcvUAAAB connected\n",
            "127.0.0.1 - - [19/Dec/2024 03:06:40] \"POST /socket.io/?EIO=4&transport=polling&t=qq6epd5c&sid=8k1AmEMkHwHsBgcQAAAA HTTP/1.1\" 200 -\n",
            "[2024-12-19 03:06:47.709] Recording started\n",
            "[2024-12-19 03:06:47.897] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.033] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:48.043] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.048] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:48.190] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.195] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:48.357] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.362] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:48.513] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.518] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:48.680] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.685] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:48.827] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.833] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:48.994] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:48.999] wakeup_and_vad.predict -> res status: sl\n",
            "[2024-12-19 03:06:48.999] Sid:  TXtbIyUqp2e1KcvUAAAB  Vad start\n",
            "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
            "[2024-12-19 03:06:49.303] State 1 prob: 0.0000, State 2 prob: 0.0000\n",
            "[2024-12-19 03:06:49.303] sl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:49.398] State 1 prob: 0.0000, State 2 prob: 0.0001\n",
            "[2024-12-19 03:06:49.399] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:49.496] State 1 prob: 0.0001, State 2 prob: 0.0001\n",
            "[2024-12-19 03:06:49.496] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:49.592] State 1 prob: 0.0005, State 2 prob: 0.0001\n",
            "[2024-12-19 03:06:49.592] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:49.688] State 1 prob: 0.0001, State 2 prob: 0.0002\n",
            "[2024-12-19 03:06:49.688] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:49.783] State 1 prob: 0.0061, State 2 prob: 0.0006\n",
            "[2024-12-19 03:06:49.784] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:49.882] State 1 prob: 0.0004, State 2 prob: 0.0001\n",
            "[2024-12-19 03:06:49.882] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:49.893] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:49.898] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:49.995] State 1 prob: 0.0111, State 2 prob: 0.0013\n",
            "[2024-12-19 03:06:49.995] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.005] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.011] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.105] State 1 prob: 0.0073, State 2 prob: 0.0001\n",
            "[2024-12-19 03:06:50.105] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.116] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.120] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.216] State 1 prob: 0.0015, State 2 prob: 0.0011\n",
            "[2024-12-19 03:06:50.217] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.227] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.232] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.327] State 1 prob: 0.0027, State 2 prob: 0.0014\n",
            "[2024-12-19 03:06:50.328] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.338] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.343] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.438] State 1 prob: 0.0356, State 2 prob: 0.0032\n",
            "[2024-12-19 03:06:50.439] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.449] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.454] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.550] State 1 prob: 0.1118, State 2 prob: 0.0018\n",
            "[2024-12-19 03:06:50.550] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.560] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.567] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.661] State 1 prob: 0.0044, State 2 prob: 0.0007\n",
            "[2024-12-19 03:06:50.661] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.671] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.676] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.771] State 1 prob: 0.0596, State 2 prob: 0.0012\n",
            "[2024-12-19 03:06:50.771] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.782] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.787] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.880] State 1 prob: 0.0038, State 2 prob: 0.0013\n",
            "[2024-12-19 03:06:50.880] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:50.891] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:50.895] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:50.991] State 1 prob: 0.0771, State 2 prob: 0.0002\n",
            "[2024-12-19 03:06:50.991] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:51.001] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.006] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:51.099] State 1 prob: 0.0060, State 2 prob: 0.0010\n",
            "[2024-12-19 03:06:51.099] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:51.110] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.114] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:51.209] State 1 prob: 0.0464, State 2 prob: 0.0002\n",
            "[2024-12-19 03:06:51.210] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:51.220] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.225] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:51.325] State 1 prob: 0.2789, State 2 prob: 0.0146\n",
            "[2024-12-19 03:06:51.325] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:51.335] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.341] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:51.438] State 1 prob: 0.2575, State 2 prob: 0.0091\n",
            "[2024-12-19 03:06:51.438] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:51.449] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.454] wakeup_and_vad.predict -> res status: el\n",
            "[2024-12-19 03:06:51.454] Sid:  TXtbIyUqp2e1KcvUAAAB  Detect vad time out\n",
            "[2024-12-19 03:06:51.565] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.571] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:51.723] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.728] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:51.889] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:51.895] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:52.067] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:52.072] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:52.233] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:52.239] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:52.381] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:52.386] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:52.547] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:52.553] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:52.724] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:52.729] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:52.870] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:52.876] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:53.017] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:53.022] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:53.184] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:53.188] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:53.340] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:53.345] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:53.507] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:53.512] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:53.673] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:53.679] wakeup_and_vad.predict -> res status: sl\n",
            "[2024-12-19 03:06:53.679] Sid:  TXtbIyUqp2e1KcvUAAAB  Vad start\n",
            "[2024-12-19 03:06:53.777] State 1 prob: 0.0000, State 2 prob: 0.0000\n",
            "[2024-12-19 03:06:53.778] sl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:53.871] State 1 prob: 0.0000, State 2 prob: 0.0002\n",
            "[2024-12-19 03:06:53.871] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:53.967] State 1 prob: 0.0001, State 2 prob: 0.0000\n",
            "[2024-12-19 03:06:53.967] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.060] State 1 prob: 0.0005, State 2 prob: 0.0011\n",
            "[2024-12-19 03:06:54.061] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.156] State 1 prob: 0.0062, State 2 prob: 0.0048\n",
            "[2024-12-19 03:06:54.157] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.251] State 1 prob: 0.0003, State 2 prob: 0.0003\n",
            "[2024-12-19 03:06:54.252] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.348] State 1 prob: 0.0005, State 2 prob: 0.0001\n",
            "[2024-12-19 03:06:54.348] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.358] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:54.363] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:54.458] State 1 prob: 0.0127, State 2 prob: 0.0005\n",
            "[2024-12-19 03:06:54.458] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.469] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:54.473] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:54.571] State 1 prob: 0.0312, State 2 prob: 0.0008\n",
            "[2024-12-19 03:06:54.571] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.582] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:54.588] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:54.683] State 1 prob: 0.0012, State 2 prob: 0.0006\n",
            "[2024-12-19 03:06:54.684] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.694] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:54.699] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:54.792] State 1 prob: 0.0055, State 2 prob: 0.0010\n",
            "[2024-12-19 03:06:54.793] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.803] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:54.808] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:54.903] State 1 prob: 0.0377, State 2 prob: 0.0009\n",
            "[2024-12-19 03:06:54.904] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:54.914] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:54.920] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:55.015] State 1 prob: 0.3664, State 2 prob: 0.0005\n",
            "[2024-12-19 03:06:55.016] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:55.026] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.031] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:55.129] State 1 prob: 0.0973, State 2 prob: 0.0028\n",
            "[2024-12-19 03:06:55.129] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:55.139] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.144] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:55.239] State 1 prob: 0.0376, State 2 prob: 0.0005\n",
            "[2024-12-19 03:06:55.240] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:55.250] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.255] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:55.350] State 1 prob: 0.0227, State 2 prob: 0.0016\n",
            "[2024-12-19 03:06:55.351] cl -> speech_dialogue outputs stat: cl\n",
            "[2024-12-19 03:06:55.361] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.366] wakeup_and_vad.predict -> res status: cl\n",
            "[2024-12-19 03:06:55.462] State 1 prob: 0.6005, State 2 prob: 0.0005\n",
            "[2024-12-19 03:06:55.463] cl -> speech_dialogue outputs stat: ss\n",
            "[2024-12-19 03:06:55.483] Sid:  TXtbIyUqp2e1KcvUAAAB  Detect break\n",
            "[2024-12-19 03:06:55.504] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.511] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:55.581] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.586] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:55.749] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.759] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:55.901] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:55.910] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:56.062] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:56.071] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:56.188] Synthesis:  ['你好！有什么我可以帮助你的吗？']\n",
            "[2024-12-19 03:06:56.190] Starting TTS...\n",
            "[2024-12-19 03:06:56.224] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:56.234] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:56.397] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:56.407] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:56.549] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:56.556] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:56.698] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:56.704] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:56.713] Codec Done\n",
            "[2024-12-19 03:06:56.741] Do not need to split\n",
            "[2024-12-19 03:06:56.757] Sid:  TXtbIyUqp2e1KcvUAAAB Send TTS data\n",
            "[2024-12-19 03:06:56.858] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:56.864] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:57.015] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:57.017] Codec Done\n",
            "[2024-12-19 03:06:57.021] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:57.039] Sid:  TXtbIyUqp2e1KcvUAAAB Send TTS data\n",
            "[2024-12-19 03:06:57.152] Codec Done\n",
            "[2024-12-19 03:06:57.155] Sid:  TXtbIyUqp2e1KcvUAAAB Send TTS data\n",
            "[2024-12-19 03:06:57.184] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:57.190] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:57.290] Codec Done\n",
            "[2024-12-19 03:06:57.307] Sid:  TXtbIyUqp2e1KcvUAAAB Send TTS data\n",
            "[2024-12-19 03:06:57.352] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:57.359] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:57.431] Codec Done\n",
            "[2024-12-19 03:06:57.434] Sid:  TXtbIyUqp2e1KcvUAAAB Send TTS data\n",
            "[2024-12-19 03:06:57.500] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:57.507] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:57.574] Codec Done\n",
            "[2024-12-19 03:06:57.578] Sid:  TXtbIyUqp2e1KcvUAAAB Send TTS data\n",
            "[2024-12-19 03:06:57.660] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:57.666] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:57.687] Codec Done\n",
            "[2024-12-19 03:06:57.828] Sid:  TXtbIyUqp2e1KcvUAAAB Send TTS data\n",
            "[2024-12-19 03:06:57.839] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:57.845] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:57.986] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:57.991] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:58.143] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:58.148] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:58.299] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:58.304] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:58.456] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:58.461] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:58.632] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:58.638] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:58.789] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:58.794] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:58.946] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:58.951] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:59.102] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:59.108] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:59.260] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:59.265] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:59.457] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:59.462] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:59.594] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:59.599] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:59.761] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:59.767] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:06:59.919] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:06:59.924] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:00.066] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:00.071] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:00.232] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:00.237] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:00.388] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:00.393] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:00.555] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:00.560] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:00.722] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:00.728] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:00.870] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:00.875] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:01.027] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:01.032] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:01.185] Sid:  TXtbIyUqp2e1KcvUAAAB  Received PCM data:  2560 (2560,)\n",
            "[2024-12-19 03:07:01.190] wakeup_and_vad.predict -> res status: None\n",
            "[2024-12-19 03:07:01.363] Recording stopped\n",
            "t=2024-12-19T03:09:34+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-f1eed709-d1ee-49ae-8d23-d83e4657561a acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNLXljRKIHlOKtOecKIPBEE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}