{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weedge/doraemon-nb/blob/main/Langchain_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604",
      "metadata": {
        "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604"
      },
      "source": [
        "# Retrieval-augmented generation (RAG)\n",
        "\n",
        "https://python.langchain.com/docs/use_cases/question_answering/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5151afed",
      "metadata": {
        "id": "5151afed"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### 什么是RAG?\n",
        "RAG是一种使用额外的(通常是私有的或实时的)数据来扩充LMMs的技术。\n",
        "\n",
        "LLMs可以对广泛的主题进行推理，但他们的知识仅限于他们接受训练的特定时间点的公共数据。如果您想要构建能够推断私有数据或在模型截止日期之后引入的数据的AI应用程序，则需要使用模型所需的特定信息来增强模型的知识。引入适当的信息并将其插入模型提示符的过程称为检索增强生成(RAG)。\n",
        "\n",
        "### 这本指南有什么内容?\n",
        "LangChain有许多专门设计用来帮助构建RAG应用程序的组件。为了熟悉它们，我们将在文本数据源上构建一个简单的问答应用程序。具体来说，我们将在Lilian Weng的LLM Powered Autonomous Agents博客文章上构建一个QA机器人。在此过程中，我们将介绍一个典型的QA体系结构，讨论相关的LangChain组件，并重点介绍用于更高级QA技术的额外资源。我们还将看到LangSmith如何帮助我们跟踪和理解我们的应用程序。随着我们的应用程序变得越来越复杂，LangSmith将变得越来越有帮助。\n",
        "\n",
        "注意:这里我们关注的是非结构化数据的RAG。我们在其他地方介绍的两个RAG用例是:\n",
        "- [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL)\n",
        "- [QA over code](/docs/use_cases/question_answering/code_understanding) (e.g., Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f25cbbd-0938-4e3d-87e4-17a204a03ffb",
      "metadata": {
        "id": "2f25cbbd-0938-4e3d-87e4-17a204a03ffb"
      },
      "source": [
        "## 架构\n",
        "典型的RAG应用程序有两个主要组件:\n",
        "\n",
        "**索引**:从源获取数据并为其建立索引的管道。这通常发生在离线状态下\n",
        "\n",
        "**检索和生成**:实际的RAG链，它在运行时接受用户查询并从索引中检索相关数据，然后将其传递给模型。\n",
        "\n",
        "从原始数据到答案最常见的完整序列如下:\n",
        "\n",
        "### 索引\n",
        "1. **加载**:首先我们需要加载数据。我们将使用[DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)。\n",
        "2. **Split**: [Text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/)将大的“Documents”分解成小块。这对于索引数据和将数据传递给模型都很有用，因为大块很难搜索，并且不适合模型有限的上下文窗口。\n",
        "3. **Store**:我们需要一个地方来存储和索引拆分，以便以后可以搜索它们。这通常使用[VectorStore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)和[Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/)模型来完成。\n",
        "\n",
        "![index_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_indexing.png?raw=1)\n",
        "\n",
        "### 检索与生成\n",
        "4. **检索**: 给定用户输入，使用[retriver](https://python.langchain.com/docs/modules/data_connection/retrivers/)从存储中检索相关拆分。\n",
        "5. **生成**:[ChatModel](https://python.langchain.com/docs/modules/model_io/chat_models) / [LLM](/docs/modules/model_io/llms/)使用包含问题和检索数据的提示生成答案\n",
        "\n",
        "![retrieval_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_retrieval_generation.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487d8d79-5ee9-4aa4-9fdf-cd5f4303e099",
      "metadata": {
        "id": "487d8d79-5ee9-4aa4-9fdf-cd5f4303e099"
      },
      "source": [
        "## 设置\n",
        "\n",
        "### 安装依赖\n",
        "\n",
        "我们将使用OpenAI聊天模型和嵌入和一个Chroma矢量存储在这个walkthrough中，但这里显示的一切都可以与任何[ChatModel](https://python.langchain.com/docs/integrations/chat/)或[LLM](https://python.langchain.com/docs/integrations/llms/)， [embeddings](https://python.langchain.com/docs/integrations/text_embedding/)和[VectorStore](https://python.langchain.com/docs/integrations/vectorstores/)或[retriver](https://python.langchain.com/docs/integrations/retrivers)一起工作。\n",
        "\n",
        "我们将使用以下包:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXiip0HhReO5",
        "outputId": "7052e1b2-1329-4b04-8495-0062d122f733"
      },
      "id": "YXiip0HhReO5",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
      "metadata": {
        "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b05e5d9-74ab-4af5-817c-9b22f01d6ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.351-py3-none-any.whl (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.3/794.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.5.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.20-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.7/507.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchainhub\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.4-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain)\n",
            "  Downloading langsmith-0.0.72-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<5,>=4.5 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
            "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.41b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.41b0-py3-none-any.whl (13 kB)\n",
            "Collecting opentelemetry-instrumentation==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.41b0-py3-none-any.whl (25 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.41b0-py3-none-any.whl (26 kB)\n",
            "Collecting opentelemetry-util-http==0.41b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.41b0-py3-none-any.whl (6.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.9-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Collecting types-urllib3 (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub)\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: bs4, pypika\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=fbc9da6c2aeb2b0763c78a2c58597e510d98cf683aae6c7082aa5f735c998b51\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=9aad55f78a8aa74a64242abb24b88ac1a0565c6840bba553f8a3681c3216bf58\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built bs4 pypika\n",
            "Installing collected packages: types-urllib3, pypika, monotonic, mmh3, websockets, uvloop, urllib3, typing-extensions, types-requests, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, coloredlogs, bs4, asgiref, posthog, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, langchainhub, httpx, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, langchain-core, kubernetes, opentelemetry-instrumentation-fastapi, langchain-community, langchain, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.20 coloredlogs-15.0.1 dataclasses-json-0.6.3 deprecated-1.2.14 fastapi-0.105.0 h11-0.14.0 httpcore-1.0.2 httptools-0.6.1 httpx-0.25.2 humanfriendly-10.0 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-28.1.0 langchain-0.0.351 langchain-community-0.0.4 langchain-core-0.1.1 langchainhub-0.1.14 langsmith-0.0.72 marshmallow-3.20.1 mmh3-4.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.16.3 openai-1.5.0 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.1.0 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 types-requests-2.31.0.6 types-urllib3-1.26.25.14 typing-extensions-4.9.0 typing-inspect-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain openai chromadb langchainhub bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ef48de-70b6-4f43-8e0b-ab9b84c9c02a",
      "metadata": {
        "id": "51ef48de-70b6-4f43-8e0b-ab9b84c9c02a"
      },
      "source": [
        "从colab设置的OPENAI_API_KEY秘钥中获取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143787ca-d8e6-4dc9-8281-4374f4d71720",
      "metadata": {
        "id": "143787ca-d8e6-4dc9-8281-4374f4d71720"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "print(userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# import dotenv\n",
        "\n",
        "# dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1665e740-ce01-4f09-b9ed-516db0bd326f",
      "metadata": {
        "id": "1665e740-ce01-4f09-b9ed-516db0bd326f"
      },
      "source": [
        "### LangSmith\n",
        "\n",
        "使用LangChain构建的许多应用程序将包含多个步骤，并调用多个LLM调用。随着这些应用程序变得越来越复杂，能够检查链或代理内部究竟发生了什么变得至关重要。最好的方法是使用[LangSmith](https://smith.langchain.com)。\n",
        "\n",
        "请注意，LangSmith不是必需的，但它很有帮助。如果你确实想使用LangSmith，在你注册了上面的链接后，确保设置你的环境变量来开始记录跟踪:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07411adb-3722-4f65-ab7f-8f6f57663d11",
      "metadata": {
        "id": "07411adb-3722-4f65-ab7f-8f6f57663d11"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "from google.colab import userdata\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6ba684-26cf-4860-904e-a4d51380c134",
      "metadata": {
        "id": "fa6ba684-26cf-4860-904e-a4d51380c134"
      },
      "source": [
        "##快速入门\n",
        "\n",
        "- https://docs.smith.langchain.com/cookbook/hub-examples\n",
        "\n",
        "假设我们想在Lilian Weng的博客文章[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)上构建一个QA应用程序。我们可以用20行代码创建一个简单的管道:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
      "metadata": {
        "id": "d8a913b1-0eea-442a-8a64-ec73333f104b"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "820244ae-74b4-4593-b392-822979dd91b8",
      "metadata": {
        "id": "820244ae-74b4-4593-b392-822979dd91b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc39049-791c-4818-d9fb-be5ce775c0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
          ]
        }
      ],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=37c38d2b-ad01-59a2-a7ab-48f1fcc0cf57\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "print(prompt)\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24",
      "metadata": {
        "id": "0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24",
        "outputId": "dc478e68-6c86-4952-ffbb-83a0a9983aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through various methods such as using prompting techniques, task-specific instructions, or human inputs. The goal is to make the task more manageable and facilitate the interpretation of the model's thinking process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7cb344e0-c423-400c-a079-964c08e07e32",
      "metadata": {
        "id": "7cb344e0-c423-400c-a079-964c08e07e32"
      },
      "outputs": [],
      "source": [
        "# cleanup\n",
        "vectorstore.delete_collection()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "639dc31a-7f16-40f6-ba2a-20e7c2ecfe60",
      "metadata": {
        "id": "639dc31a-7f16-40f6-ba2a-20e7c2ecfe60"
      },
      "source": [
        ":::tip\n",
        "\n",
        "Check out the [LangSmith trace](https://smith.langchain.com/public/1c6ca97e-445b-4d00-84b4-c7befcbc59fe/r)\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842cf72d-abbc-468e-a2eb-022470347727",
      "metadata": {
        "id": "842cf72d-abbc-468e-a2eb-022470347727"
      },
      "source": [
        "##详细演练\n",
        "\n",
        "让我们一步一步地检查上面的代码，以真正理解发生了什么。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5daed6",
      "metadata": {
        "id": "ba5daed6"
      },
      "source": [
        "##步骤1. 加载\n",
        "\n",
        "需要首先加载博客文章的内容。可以使用'DocumentLoader's，它是从一个源中加载数据为'Documents'的对象。'Document'是一个具有'page_content'(str)和'metadata'(dict)属性的对象。\n",
        "\n",
        "在这种情况下，将使用'WebBaseLoader'，它使用'urllib'和'BeautifulSoup'来加载和解析传入的web url，每个url返回一个'Document'。我们可以自定义html -> 通过'bs_kwargs'将参数传递给'BeautifulSoup'解析器进行文本解析(参见[BeautifulSoup文档](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup))。在这种情况下，只有带有“post-content”、“post-title”或“post-header”类的HTML标签是相关的，所以删除所有其他标签。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cf4d5c72",
      "metadata": {
        "id": "cf4d5c72"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\n",
        "        \"parse_only\": bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    },\n",
        ")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "207f87a3-effa-4457-b013-6d233bc7a088",
      "metadata": {
        "id": "207f87a3-effa-4457-b013-6d233bc7a088",
        "outputId": "f1601a66-49b7-40f8-c4de-a23de16b0afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42824"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "52469796-5ce4-4c12-bd2a-a903872dac33",
      "metadata": {
        "id": "52469796-5ce4-4c12-bd2a-a903872dac33",
        "outputId": "8c2bc0da-5b48-4c30-c0f3-2cdf6b167959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee5c6556-56be-4067-adbc-98b5aa19ef6e",
      "metadata": {
        "id": "ee5c6556-56be-4067-adbc-98b5aa19ef6e"
      },
      "source": [
        "### 进一步了解\n",
        "'DocumentLoader':从一个源中以'Documents'加载数据的对象。\n",
        "- [Docs](https://python.langchain.com/docs/modules/data_connection/document_loaders/):关于如何使用DocumentLoader的进一步文档。\n",
        "- [Integrations](https://python.langchain.com/docs/Integrations/document_loaders/):查找相关的“DocumentLoader”集成(其中160个)用于用例。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2cc9a7",
      "metadata": {
        "id": "fd2cc9a7"
      },
      "source": [
        "##步骤2. 切分\n",
        "\n",
        "我们加载的文档长度超过42k个字符。这对于许多模型的上下文窗口来说太长了。即使对于那些可以在上下文窗口中匹配整个帖子的模型，经验模型也很难在很长的提示中找到相关的上下文。\n",
        "\n",
        "因此，我们将把“Document”分割成块用于embeddings矢量存储。这将帮助我们在运行时只检索博客文章中最相关的部分。\n",
        "\n",
        "在本例中，我们将文档分成1000个字符的块，块之间有200个字符重叠。重叠有助于减少将语句从与其相关的重要上下文中分离出来的可能性。我们使用“RecursiveCharacterTextSplitter”，它将使用通用分隔符(如新行)(递归地)拆分文档，直到每个块都是适当的大小。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4b11c01d",
      "metadata": {
        "id": "4b11c01d"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3741eb67-9caf-40f2-a001-62f49349bff5",
      "metadata": {
        "id": "3741eb67-9caf-40f2-a001-62f49349bff5",
        "outputId": "539f8f1f-a36e-44a1-ea0c-44fa84a49972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(all_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f868d0e5-5670-4d54-b562-f50265e907f4",
      "metadata": {
        "id": "f868d0e5-5670-4d54-b562-f50265e907f4",
        "outputId": "1bc8177b-7ac3-47ac-ed54-68628a2d0fd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "969"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(all_splits[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5c9e5f27-c8e3-4ca7-8a8e-45c5de2901cc",
      "metadata": {
        "id": "5c9e5f27-c8e3-4ca7-8a8e-45c5de2901cc",
        "outputId": "10c7ab89-25a2-4ded-9119-c6a05b938e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              " 'start_index': 7056}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "all_splits[10].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a33bd4d",
      "metadata": {
        "id": "0a33bd4d"
      },
      "source": [
        "### 深入阅读\n",
        "\n",
        "'DocumentSplitter': 将'Document'列表分割成更小块的对象。DocumentTransformer的子类。\n",
        "- 探索“Context-aware splitters 上下文感知分配器”，它保留原始“Document”中每个分配器的位置(“Context”):\n",
        "- [Markdown文件](https://python.langchain.com/docs/use_cases/question_answer/document-context-aware-QA)\n",
        "- [代码(py或js)](https://python.langchain.com/docs/integrations/document_loaders/source_code)\n",
        "- [科学论文](https://python.langchain.com/docs/integrations/document_loaders/grobid)\n",
        "\n",
        "'DocumentTransformer': 对'Document'的列表执行转换的对象。\n",
        "- [Docs](https://python.langchain.com/docs/modules/data_connection/document_transformers/):关于如何使用DocumentTransformer的进一步文档\n",
        "- [Integration](https://python.langchain.com/docs/integrations/document_transformers/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46547031-2352-4321-9970-d6ea27285c2e",
      "metadata": {
        "id": "46547031-2352-4321-9970-d6ea27285c2e"
      },
      "source": [
        "##步骤3. 存储\n",
        "\n",
        "现在我们在内存中有66个文本块，我们需要存储和索引它们，以便稍后在我们的RAG应用程序中搜索它们。最常见的方法是嵌入每个文档拆分的内容并将这些嵌入上传到矢量存储。\n",
        "\n",
        "然后，当我们想要搜索分割时，我们将搜索查询也embedding其中，并执行某种“相似性”搜索，以识别与我们的查询embedding最相似的embedding的存储分割。最简单的相似度度量是余弦相似度——我们测量每一对嵌入(它们只是非常高维的向量)之间角度的余弦。\n",
        "\n",
        "我们可以使用“Chroma”矢量存储和“OpenAIEmbeddings”模型在单个命令中embedding和存储所有文档分割。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e9c302c8",
      "metadata": {
        "id": "e9c302c8"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6f22b0",
      "metadata": {
        "id": "dc6f22b0"
      },
      "source": [
        "###进一步了解\n",
        "`Embeddings`:围绕文本embedding模型的包装器，用于将文本转换为embeddings。\n",
        "- [Docs](https://python.langchain.com/Docs/modules/data_connection/text_embedding):关于接口的进一步文档。\n",
        "- [Integration](https://python.langchain.com/docs/integration/text_embedding/):浏览30个文本嵌入集成\n",
        "\n",
        "`VectorStore`:围绕向量数据库的包装器，用于存储和查询嵌入。\n",
        "- [Docs](https://python.langchain.com/Docs/modules/data_connection/vectorstores/):关于接口的进一步文档。\n",
        "- [Integration](https://python.langchain.com/docs/integration/vectorstores/):浏览40个“VectorStore”集成。\n",
        "\n",
        "这完成了管道的**Indexing**部分。此时，我们有一个可查询的矢量存储，其中包含博客文章的分块内容。给定一个用户问题，理想情况下，我们应该能够返回回答这个问题的博客文章的片段:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d64d40-e475-43d9-b64c-925922bb5ef7",
      "metadata": {
        "id": "70d64d40-e475-43d9-b64c-925922bb5ef7"
      },
      "source": [
        "##步骤4. 检索\n",
        "\n",
        "现在让我们编写实际的应用程序逻辑。我们希望创建一个简单的应用程序，让用户提出问题，搜索与该问题相关的文档，将检索到的文档和初始问题传递给模型，最后返回答案。\n",
        "\n",
        "LangChain定义了一个`retriver`接口，该接口包装了一个索引，该索引可以返回给定字符串查询的相关文档。所有检索器都实现一个通用方法`get_relevance_documents()`(及其异步变体`aget_relevance_documents()`)。\n",
        "\n",
        "最常见的“检索器”类型是“向量存储检索器”，它使用向量存储的相似性搜索功能来方便检索。任何“VectorStore”都可以很容易地变成“Retriever”:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4414df0d-5d43-46d0-85a9-5f47be0dd099",
      "metadata": {
        "id": "4414df0d-5d43-46d0-85a9-5f47be0dd099"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e2c26b7d",
      "metadata": {
        "id": "e2c26b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20e0c68-7dc9-4bea-be0a-d6fc935358c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}),\n",
              " Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}),\n",
              " Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}),\n",
              " Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414}),\n",
              " Document(page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}),\n",
              " Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2837})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(\n",
        "    \"What are the approaches to Task Decomposition?\"\n",
        ")\n",
        "\n",
        "retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8684291d-0f5e-453a-8d3e-ff9feea765d0",
      "metadata": {
        "id": "8684291d-0f5e-453a-8d3e-ff9feea765d0",
        "outputId": "0c036214-5e85-4b0b-b9ff-bd988f17141f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9a5dc074-816d-409a-b005-ab4eddfd76af",
      "metadata": {
        "id": "9a5dc074-816d-409a-b005-ab4eddfd76af",
        "outputId": "0546a11b-0ca6-4a6b-ce1c-b818e85aa17c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
          ]
        }
      ],
      "source": [
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d5a113b",
      "metadata": {
        "id": "5d5a113b"
      },
      "source": [
        "### 进一步了解\n",
        "向量存储通常用于检索，但是还有很多其他的方法来进行检索。\n",
        "\n",
        "一个对象，它返回给定文档的文本查询\n",
        "-  [Docs](https://python.langchain.com/docs/modules/data_connection/retrievers/):关于接口和内置检索技术的进一步文档。其中包括:\n",
        "- `MultiQueryRetriever`[generates variants of the input question](https://python.langchain.com/docs/modules/data_connection/retrievers/multiqueryretriver)提高检索命中率。\n",
        "- `MultiVectorRetriever`[generates variants of embeddings](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)，也是为了提高检索命中率。\n",
        "- `Max marginal relevance`(最大边际相关性)在检索的文档中选择[相关性和多样性](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf)，以避免传递重复的上下文。\n",
        "-文档可以在矢量存储检索期间使用[`metadata`过滤器](https://python.langchain.com/docs/use_cases/question_answer/document-context-aware-QA)进行过滤。\n",
        "- [Integrations](https://python.langchain.com/docs/integrations/retrievers/):与检索服务的集成。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415d6824",
      "metadata": {
        "id": "415d6824"
      },
      "source": [
        "##步骤5. 生成\n",
        "\n",
        "让我们将所有这些放到一个链中，该链接受一个问题、检索相关文档、构造提示、将提示传递给模型并解析输出。\n",
        "\n",
        "我们将使用gpt-3.5 turbo OpenAI聊天模型，但任何LangChain `LLM`或`ChatModel`都可以替换。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d34d998c-9abf-4e01-a4ad-06dadfcf131c",
      "metadata": {
        "id": "d34d998c-9abf-4e01-a4ad-06dadfcf131c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d43623f-8c69-498d-f570-f9c391734a77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x78b03147e4d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x78b0314e56c0>, temperature=0.0, openai_api_key='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', openai_proxy='')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc826723-36fc-45d1-a3ef-df8c2c8471a8",
      "metadata": {
        "id": "bc826723-36fc-45d1-a3ef-df8c2c8471a8"
      },
      "source": [
        "我们将为RAG使用一个签入到LangChain提示中心的提示符([这里](https://smith.langchain.com/hub/rlm/rag-prompt))。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bede955b-9aeb-4fd3-964d-8e43f214ce70",
      "metadata": {
        "id": "bede955b-9aeb-4fd3-964d-8e43f214ce70"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "11c35354-f275-47ec-9f72-ebd5c23731eb",
      "metadata": {
        "id": "11c35354-f275-47ec-9f72-ebd5c23731eb",
        "outputId": "6a60f93a-2c33-4e9a-e287-704c8c0881a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: filler question \n",
            "Context: filler context \n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    prompt.invoke(\n",
        "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        "    ).to_string()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51f9a210-1eee-4054-99d7-9d9ddf7e3593",
      "metadata": {
        "id": "51f9a210-1eee-4054-99d7-9d9ddf7e3593"
      },
      "source": [
        "我们将使用[**LCEL Runnable**](https://python.langchain.com/docs/expression_language/)协议来定义链，允许我们\n",
        "- 以透明的方式将组件和功能连接在一起\n",
        "- 自动追踪我们在LangSmith的链条\n",
        "- 获得流，异步和批处理调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "99fa1aec",
      "metadata": {
        "id": "99fa1aec"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8655a152-d7cf-466f-b1bc-fbff9ae2b889",
      "metadata": {
        "id": "8655a152-d7cf-466f-b1bc-fbff9ae2b889",
        "outputId": "74f2482e-5da6-4bab-af98-ec722c3bc744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs."
          ]
        }
      ],
      "source": [
        "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c000e5f-2b7f-4eb9-8876-9f4b186b4a08",
      "metadata": {
        "id": "2c000e5f-2b7f-4eb9-8876-9f4b186b4a08"
      },
      "source": [
        ":::tip\n",
        "\n",
        "Check out the [LangSmith trace](https://smith.langchain.com/public/1799e8db-8a6d-4eb2-84d5-46e8d7d5a99b/r)\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d52c84",
      "metadata": {
        "id": "f7d52c84"
      },
      "source": [
        "### 进一步了解\n",
        "\n",
        "#### 选择LLMs\n",
        "`ChatModel`: llm支持的聊天模型包装器。接受消息序列并返回消息。\n",
        "- [Docs](https://python.langchain.com/docs/modules/model_io/chat/)\n",
        "- [Integrations](https://python.langchain.com/docs/integrations/chat/):探索超过25个“ChatModel”集成。\n",
        "\n",
        "`LLM`: 文本输入文本输出LLM。接受一个字符串并返回一个字符串。\n",
        "- [Docs](https://python.langchain.com/docs/modules/model_io/llms)\n",
        "- [Integrations](https://python.langchain.com/docs/integrations/llms): 浏览超过75个LLM集成。\n",
        "\n",
        "参见本地运行模型的RAG指南在[这里](https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa82f437",
      "metadata": {
        "id": "fa82f437"
      },
      "source": [
        "#### 自定义提示符\n",
        "\n",
        "如上所示，我们可以从提示中心加载提示(例如，[这个RAG提示](https://smith.langchain.com/hub/rlm/rag-prompt))。提示符也可以很容易地定制:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e4fee704",
      "metadata": {
        "id": "e4fee704",
        "outputId": "cebfd651-c9dc-42fc-9ced-7140bbaec539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "rag_prompt_custom = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt_custom\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b952e6-dc4b-415b-9cf3-1ad333e48366",
      "metadata": {
        "id": "94b952e6-dc4b-415b-9cf3-1ad333e48366"
      },
      "source": [
        ":::tip\n",
        "\n",
        "Check out the [LangSmith trace](https://smith.langchain.com/public/da23c4d8-3b33-47fd-84df-a3a582eedf84/r)\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2f99b5-80b4-4178-bf30-c1c0a152638f",
      "metadata": {
        "id": "1c2f99b5-80b4-4178-bf30-c1c0a152638f"
      },
      "source": [
        "### 添加源\n",
        "\n",
        "使用LCEL很容易从文档中返回检索到的文档或某些源元数据:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ded41680-b749-4e2a-9daa-b1165d74783b",
      "metadata": {
        "id": "ded41680-b749-4e2a-9daa-b1165d74783b",
        "outputId": "71d573fe-efa4-4991-d7b9-b0dfa0fde78e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              "   'start_index': 1585},\n",
              "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              "   'start_index': 2192},\n",
              "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              "   'start_index': 17804},\n",
              "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              "   'start_index': 17414},\n",
              "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              "   'start_index': 29630},\n",
              "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              "   'start_index': 19373}],\n",
              " 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "rag_chain_from_docs = (\n",
        "    {\n",
        "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt_custom\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
        ") | {\n",
        "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
        "    \"answer\": rag_chain_from_docs,\n",
        "}\n",
        "\n",
        "rag_chain_with_source.invoke(\"What is Task Decomposition\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b437da5d-ca09-4d15-9be2-c35e5a1ace77",
      "metadata": {
        "id": "b437da5d-ca09-4d15-9be2-c35e5a1ace77"
      },
      "source": [
        ":::tip\n",
        "\n",
        "Check out the [LangSmith trace](https://smith.langchain.com/public/007d7e01-cb62-4a84-8b71-b24767f953ee/r)\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776ae958-cbdc-4471-8669-c6087436f0b5",
      "metadata": {
        "id": "776ae958-cbdc-4471-8669-c6087436f0b5"
      },
      "source": [
        "### 添加记忆(memory)\n",
        "\n",
        "假设我们想要创建一个有状态的应用程序来记住过去的用户输入。为了支持这一点，我们需要做两件主要的事情。\n",
        "1. 在我们的链中添加一个消息占位符，它允许我们传入历史消息\n",
        "2. 添加一条链，接收最新的用户查询，并在聊天历史的上下文中将其重新表述为可以传递给检索器的独立问题。\n",
        "\n",
        "从2开始。我们可以构建一个“浓缩问题”链，看起来像这样:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2b685428-8b82-4af1-be4f-7232c5d55b73",
      "metadata": {
        "id": "2b685428-8b82-4af1-be4f-7232c5d55b73"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "which might reference the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", condense_q_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "condense_q_chain = condense_q_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "46ee9aa1-16f1-4509-8dae-f8c71f4ad47d",
      "metadata": {
        "id": "46ee9aa1-16f1-4509-8dae-f8c71f4ad47d",
        "outputId": "63a3757c-d095-48a3-f90a-f37f4b1393d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the definition of \"large\" in the context of a language model?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "condense_q_chain.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"What does LLM stand for?\"),\n",
        "            AIMessage(content=\"Large language model\"),\n",
        "        ],\n",
        "        \"question\": \"What is meant by large\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "31ee8481-ce37-41ae-8ca5-62196619d4b3",
      "metadata": {
        "id": "31ee8481-ce37-41ae-8ca5-62196619d4b3",
        "outputId": "bd65006c-c774-472d-ce8d-c5a7646c969a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How do transformer models function?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "condense_q_chain.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"What does LLM stand for?\"),\n",
        "            AIMessage(content=\"Large language model\"),\n",
        "        ],\n",
        "        \"question\": \"How do transformers work\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42a47168-4a1f-4e39-bd2d-d5b03609a243",
      "metadata": {
        "id": "42a47168-4a1f-4e39-bd2d-d5b03609a243"
      },
      "source": [
        "现在我们可以构建完整的QA链了。注意，我们添加了一些路由功能，只在聊天记录不是空的情况下运行“压缩问题链(condense question chain)”。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "66f275f3-ddef-4678-b90d-ee64576878f9",
      "metadata": {
        "id": "66f275f3-ddef-4678-b90d-ee64576878f9"
      },
      "outputs": [],
      "source": [
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "{context}\"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def condense_question(input: dict):\n",
        "    if input.get(\"chat_history\"):\n",
        "        return condense_q_chain\n",
        "    else:\n",
        "        return input[\"question\"]\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
        "    | qa_prompt\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "51fd0e54-5bb4-4a9a-b012-87a18ebe2bef",
      "metadata": {
        "id": "51fd0e54-5bb4-4a9a-b012-87a18ebe2bef",
        "outputId": "4ffd7423-c947-4089-977d-4a1c31ab79a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs a model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. This approach utilizes more computation at test-time and helps in interpreting the model\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" This allows the model to generate a sequence of subtasks or steps for completing the main task.\\n\\n3. Task-specific instructions: For certain tasks, specific instructions can be provided to guide the model in decomposing the task. For example, in the context of writing a novel, an instruction like \"Write a story outline\" can help in breaking down the task into manageable components.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide insights, expertise, or domain knowledge to help identify and decompose complex tasks into smaller subtasks.')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "chat_history = []\n",
        "\n",
        "question = \"What is Task Decomposition?\"\n",
        "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
        "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
        "\n",
        "second_question = \"What are common ways of doing it?\"\n",
        "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53263a65-4de2-4dd8-9291-6a8169ab6f1d",
      "metadata": {
        "id": "53263a65-4de2-4dd8-9291-6a8169ab6f1d"
      },
      "source": [
        ":::tip\n",
        "\n",
        "Check out the [LangSmith trace](https://smith.langchain.com/public/b3001782-bb30-476a-886b-12da17ec258f/r)\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf6c7e0-84f8-4747-b2ae-e84315152bd9",
      "metadata": {
        "id": "fdf6c7e0-84f8-4747-b2ae-e84315152bd9"
      },
      "source": [
        "在这里，我们讨论了如何添加链逻辑来合并历史输出。但是我们如何存储和检索不同会话的历史输出呢?为此，请查看LCEL[如何添加消息历史(记忆)](https://python.langchain.com/docs/expression_language/how_to/message_history)页面。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580e18de-132d-4009-ba67-4aaf2c7717a2",
      "metadata": {
        "id": "580e18de-132d-4009-ba67-4aaf2c7717a2"
      },
      "source": [
        "## 下一步\n",
        "\n",
        "我们在很短的时间内涵盖了很多内容。在上面的每一节中都有许多细微差别、特性、集成等值得探索。除了上面提到的来源，接下来的步骤包括:\n",
        "\n",
        "- 在[retrievers](https://python.langchain.com/docs/modules/data_connection/ retrievers/)部分中阅读更高级的检索技术。\n",
        "- 学习LangChain [Indexing API](https://python.langchain.com/docs/modules/data_connection/indexing)，它可以帮助重复同步数据源和矢量存储，而无需冗余计算或存储。\n",
        "- 探索RAG [LangChain Templates](https://python.langchain.com/docs/templates/#-advanced-retrieval)，这些参考应用程序可以很容易地与[LangServe](https://python.langchain.com/docs/langserve)一起部署。\n",
        "- 学习[使用LangSmith评估RAG应用程序](https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/qa-correctness/qa-correctness.ipynb)。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 总结\n",
        "\n",
        "- 使用这个CoT的RAG方式, 可以整合不同的LLMs, 如果有自己的内部垂直领域好的大模型，对于短板领域，可以`套壳`借助另外好的该领域大模型进行使用（模型服务，还是上层应用都可以`套壳`大模型）。 补齐短板，未来机器人交互应该可以这样，个人助理 - 超级马里奥\n",
        "- 主要还是以大模型Transform为主的神经网络的思维去思考上层应用，进行串联，仿生人类大脑工作思维进行交互。"
      ],
      "metadata": {
        "id": "baKUmzedjtYo"
      },
      "id": "baKUmzedjtYo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "poetry-venv",
      "language": "python",
      "name": "poetry-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}